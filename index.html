<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<title>He Wang</title>
<link rel="stylesheet" type="text/css" href="./He_Wang_files/main.css">
</head>

<body>

<table>
<tbody><tr>
<td><img src="./He_Wang_files/he_protrait_2023.JPG" width="160"></td>
<td>
<div style="font-size:24; font-weight:bold">He Wang (王鹤)</div>
<div>
Tenure-track Assistant Professor at <a href="https://english.pku.edu.cn/">Peking University</a><br>
Founder and Leader of <b>Embodied Perception and InteraCtion (EPIC) Lab</b> <br>
助理教授 博士生导师 北京大学前沿计算研究中心（CFCS）<br>
<b>Email:</b> <tt>hewang at pku dot edu dot cn</tt><br>

<br>
<a href="https://scholar.google.com/citations?user=roCAWkoAAAAJ&hl=en">[Google Scholar]</a><a href="https://github.com/hughw19">[Github]</a><a href="https://twitter.com/HughWang19">[Twitter]</a>

<br>

</div>
</td>
</tr>
</tbody></table>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>




<div class="section">
<h3>About Me</h3>
<ul>
I am a tenure-track Assistant Professor in the <a href="https://cfcs.pku.edu.cn/english">Center on Frontiers of Computing Studies (CFCS)</a> at <a href="https://english.pku.edu.cn/">Peking University</a>, where I lead <b>Embodied Perception and InteraCtion (EPIC) Lab</b>.
<!--and a research scientist at <a href="https://www.bigai.ai/">Beijing Institute for General Artificial Intelligence</a> -->
My research interests span 3D vision, robotics, and machine learning. My research objective is to endow embodied agents working in complex real-world scenes with generalizable 3D vision and interaction policies.
Prior to joining Peking University, I received my PhD degree from <a href="https://www.stanford.edu">Stanford University</a> advised by Prof. <a href="http://geometry.stanford.edu/member/guibas/index.html">Leonidas J. Guibas</a> in 2021 and my bachelor's degree from <a href="http://www.tsinghua.edu.cn/">Tsinghua University</a> in 2014.<br> 
</ul>

<h3>Recruting Information （课题组招生/智源合作招聘信息）</h3>
<ul>	
We are actively looking for interns, Masters, PhDs, and postdocs. Feel free to contact me if you are interested in my research or potential collaborations. 
<li> For graduate school applicants, we have two openings for PhD students (in addition to master/PhD openings for foreigners) each year,  and please contact me at least one year prior to the applicaiton deadline. </b>
<li> For visiting students or research interns, we welcome undergradute and graduate students from top univerisities all world wide to apply for >6 months research internship. Our interns have published many top-tier conference/journal papers and have been admitted to PhD/MS programs in Stanford, CMU, UCLA, UCSD, etc.</b>
<li> In collaboration with the Embodied AI center of <a href="https://baai.ac.cn">BAAI</a>, we also hire full-time research scientists, engineers and interns there.</b>
</div>
<br>


<div class="section">
<h3>News</h3>
<ul>

<li>  <b style="color: green; background-color: #ffff42">NEW</b> Three papers get accepted to ICCV 2023 with UniDexGrasp++ receiving final reviews of <b><font color='red'>all accepts</font></b> (the highest ratings). 
<li> <b style="color: green; background-color: #ffff42">NEW</b> I am invited to be a speaker in <a href="https://sites.google.com/view/hands2023/home">HANDS workshop</a> at ICCV 2023. 
<li> <b style="color: green; background-color: #ffff42">NEW</b> I am invited to be a speaker in <a href="https://generalist-robots.github.io/speakers/">Towards Generalist Robots: 
Learning Paradigms for Scalable Skill Acquisition Workshop</a> at CoRL 2023.
<li> <b style="color: green; background-color: #ffff42">NEW</b> I am invited to be a speaker in <a href="https://sites.google.com/view/iros23-policy-learning">Policy Learning for Geometrical Spaces Workshop</a> at IROS 2023.
<li> <b style="color: green; background-color: #ffff42">NEW</b> I am invited to be a speaker in <a href="https://sites.google.com/view/rss23-sym">Workshop on Symmetries in Robot Learning at RSS 2023</a> at RSS 2023.
<li>  <b style="color: green; background-color: #ffff42">NEW</b> Our dexterous grasping synthesis and dataset paper, DexGraspNet, is selected as a <b><font color='red'>finalist of ICRA 2023 outstanding paper in manipulation </font></b> (top 1% of submissions). 
<li>  <b style="color: green; background-color: #ffff42">NEW</b> Seven papers get accepted to CVPR 2023 with GAPartNet receiving <b><font color='red'>highlight</font></b> (top 2.5% of submissions) with final reviews of <b><font color='red'>all accepts</font></b> (the highest ratings). 
<li>  <b style="color: green; background-color: #ffff42">NEW</b> I am invited to be an associate editor of <a href="https://www.sciencedirect.com/journal/image-and-vision-computing">Image and Vision Computing</a>, which ranks top 20 in computer vision journals/conferences on Google Scholar Metrics. 
<li>  Two papers get accepted to ICLR 2023 with one as <b><font color='red'>spotlight</font></b>. 
<li>  Two papers get accepted to ICRA 2023. 
<li>  One paper gets accepted to AAAI 2023 as <b><font color='red'>oral presentation</font></b>. 
<li>  My first-author CVPR 2019 oral paper, <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_CVPR_2019_paper.pdf">NOCS</a>, received <a href="http://www.waicyop.cn/index">2022 World Artificial Intelligence Conference Youth Outstanding Paper Award</a>. Top 10 from 155 papers published in top-tier AI conferences and top journals (Science, Nature Computational Method, etc.) in the past 3 years.
<li>  One paper gets accepted to T-RO. 
<li>  One paper gets accepted to ECCV 2022. 
<li>  One paper gets accepted to Robotics and Automation Letters (RA-L) and IROS 2022. 
<li>  My students and I won the <b><font color='red'>first place</font></b> in the no external annotation track of <a href="https://sapien.ucsd.edu/challenges/maniskill2021/">SAPIEN Manipulation Skill Challenge 2021</a> and will receive $3000 prize and give a winner presentation in <a href="https://ai-workshops.github.io/generalizable-policy-learning-in-the-physical-world/">ICLR 2022 Generalizable Policy Learning in the Physical World Workshop</a>. </b>
<li>  Two papers are selected to <b><font color='red'>oral presentations</font></b> in CVPR 2022. </b>
<li>  Seven papers get accepted to CVPR 2022. </b>
<li>  One paper accepted to NeurIPS 2021. </b>
<li>  ...

</ul>
</div>
<br>

<div class="mainsection">

<a name="publications"></a>
<div class="mainsection">
<h3>Selected Publications</h3>
<ul>
*: equivalent contribution, <span>&#8224;</span>: corresponding author<br><br>
	
	
<h3>Published Works</h3>	
<ul>
<table width="100%">
	
	
<!-- GApartNet -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/gapartnet.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts</a ></b><br><br>Haoran Geng*, Helin Xu*, Chengyang Zhao*, Chao Xu, Li Yi, Siyuan Huang, <b>He Wang<span>&#8224;</span></b>
    <br><br> CVPR 2023 (<b><font color='red'>highlight, final reviews of all accepts</font></b>) <br><br>
    <a href=" ">arXiv</a>/<a href="https://pku-epic.github.io/GAPartNet/">Project</a>/<a href="javascript:hideshow(document.getElementById('GApartNet'))">bibtex</a >
</p ><pre><p id="GApartNet" style="font:18px; display: none">
@article{geng2022gapartnet,
    title={GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts},
    author={Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He},
    journal={arXiv preprint arXiv:2211.05272},
    year={2022}
}
      
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>	
	

<!-- 3DNav -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/3DNav.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href=" ">3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification</a ></b><br><br>Jiazhao Zhang*, Liu Dai*, Fanpeng Meng, Qingnan Fan, Xuelin Chen, Kai Xu, <b>He Wang<span>&#8224;</span></b> 
 <br><br> CVPR 2023 <br><br>
<a href="https://arxiv.org/pdf/2212.00338.pdf">arXiv</a >/<a href="https://pku-epic.github.io/3D-Aware-ObjectNav/">Project</a>/<a href="javascript:hideshow(document.getElementById('3DNav'))">bibtex</a>
</p ><pre><p id="3DNav" style="font:18px; display: none">
@article{zhang20223d,
  title={3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification},
  author={Zhang, Jiazhao and Dai, Liu and Meng, Fanpeng and Fan, Qingnan and Chen, Xuelin and Xu, Kai and Wang, He},
  journal={arXiv preprint arXiv:2212.00338},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
	
<!-- UniDexGrasp -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/unidexgrasp.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href=" ">UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</a ></b><br><br>Yinzhen Xu*, Weikang Wan*, Jialiang Zhang*, Haoran Liu*, Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi Chen, Tengyu Liu, Li Yi, <b>He Wang<span>&#8224;</span></b> 
 <br><br> CVPR 2023 <br><br>
<a href="https://arxiv.org/pdf/2303.00938.pdf">arXiv</a>/<a href="https://pku-epic.github.io/UniDexGrasp/">Project</a>/<a href="javascript:hideshow(document.getElementById('UniDexGrasp'))">bibtex</a>
</p ><pre><p id="UniDexGrasp" style="font:18px; display: none">
@article{xu2023unidexgrasp,
  title={UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy},
  author={Xu, Yinzhen and Wan, Weikang and Zhang, Jialiang and Liu, Haoran and Shan, Zikang and Shen, Hao and Wang, Ruicheng and Geng, Haoran and Weng, Yijia and Chen, Jiayi and others},
  journal={arXiv preprint arXiv:2303.00938},
  year={2023}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>

<!-- GAPartManip -->
<tbody><tr>
    <td width="25%" valign="top"><p><img src="./He_Wang_files/gapartmanip.png" width="250" alt="" style="border-style: none" align="top"></p ></td>
    <td width="75%" valign="top"><p>
        <b><a href="">PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations</a ></b><br><br>Haoran Geng*, Ziming Li*, Yiran Geng, Jiayi Chen, Hao Dong, <b>He Wang<span>&#8224;</span></b>
        <br><br> CVPR 2023 <br><br>
        <a href=" ">arXiv</a >/<a href="https://pku-epic.github.io/PartManip/">Project</a >/<a href="javascript:hideshow(document.getElementById('PartManip'))">bibtex</a >
    </p ><pre><p id="PartManip" style="font:18px; display: none">
        @article{geng2023partmanip,
            title={PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations},
            author={Geng, Haoran and Li, Ziming and Geng, Yiran and Chen, Jiayi and Dong, Hao and Wang, He},
            journal={arXiv preprint arXiv:2303.16958},
            year={2023}
        }
    </p ><p></p ></pre>
    <p></p ></td>
    </tr>
    <tr><td><br></td></tr> 	
	
	
<!-- SO3NF -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/so3nf.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Delving into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling</a ></b><br><br>Yulin Liu*, Haoran Liu*, Yingda Yin*, Yang Wang, Baoquan Chen<span>&#8224;</span>, <b>He Wang<span>&#8224;</span></b>
    <br><br> CVPR 2023 <br><br>
    <a href="https://arxiv.org/pdf/2304.03937.pdf">arXiv</a>/<a href="javascript:hideshow(document.getElementById('DiGA'))">SO3NF</a>
</p><pre><p id="SO3NF" style="font:18px; display: none">
@article{liu2023delving,
  title={Delving into Discrete Normalizing Flows on SO (3) Manifold for Probabilistic Rotation Modeling},
  author={Liu, Yulin and Liu, Haoran and Yin, Yingda and Wang, Yang and Chen, Baoquan and Wang, He},
  journal={arXiv preprint arXiv:2304.03937},
  year={2023}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>	

	

	
	
<!-- DiGA -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/diga.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">DiGA: Distil to Generalize and then Adapt for Domain Adaptive Semantic Segmentation</a ></b><br><br>Fengyi Shen, Akhil Gurram, Ziyuan Liu, <b>He Wang<span>&#8224;</span></b>, Alois Knoll<span>&#8224;</span>
    <br><br> CVPR 2023 <br><br>
    <a href="https://arxiv.org/pdf/2304.02222.pdf">arXiv</a>/<a href="javascript:hideshow(document.getElementById('DiGA'))">bibtex</a>
</p><pre><p id="DiGA" style="font:18px; display: none">
@article{shen2023diga,
  title={DiGA: Distil to Generalize and then Adapt for Domain Adaptive Semantic Segmentation},
  author={Shen, Fengyi and Gurram, Akhil and Liu, Ziyuan and Wang, He and Knoll, Alois},
  journal={arXiv preprint arXiv:2304.02222},
  year={2023}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>	

	

	
<!-- AdaZone -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/adazone.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation</a ></b><br><br>Chen Gao, Xingyu Peng, Mi Yan, <b>He Wang</b>, Lirong Yang, Haibing Ren, Hongsheng Li, Si Liu 
    <br><br> CVPR 2023 <br><br>
    arXiv (soon)
      
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>	

	
<!-- DexGraspNet -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/dexgraspnet.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation</a></b><br><br>Ruicheng Wang*, Jialiang Zhang*, Jiayi Chen, Yinzhen Xu, Puhao Li, Tengyu Liu, <b>He Wang<span>&#8224;</span></b>
	<br><br> ICRA 2023 (<b><font color='red'>a finalist of outstanding manipulation paper</font></b>)<br><br>
	<a href="https://arxiv.org/pdf/2210.02697.pdf">arXiv</a>/<a href="https://pku-epic.github.io/DexGraspNet/">Project</a>/<a href="javascript:hideshow(document.getElementById('dexgraspnet'))">bibtex</a>
</p><pre><p id="dexgraspnet" style="font:18px; display: none">
@article{2210.02697,
    title = {DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation},
    author = {Ruicheng Wang and Jialiang Zhang and Jiayi Chen and Yinzhen Xu and Puhao Li and Tengyu Liu and He Wang},
    journal={arXiv preprint arXiv:2210.02697},
    year = {2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
<!-- GraspNeRF -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/graspnerf.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF</a></b><br><br>Qiyu Dai*, Yan Zhu*, Yiran Geng, Ciyu Ruan, Jiazhao Zhang, <b>He Wang<span>&#8224;</span></b> 
	<br><br> ICRA 2023 <br><br>
<a href="https://arxiv.org/pdf/2210.06575.pdf">arXiv</a>/<a href="https://pku-epic.github.io/GraspNeRF/">Project</a>/<a href="javascript:hideshow(document.getElementById('GraspNeRF'))">bibtex</a>
</p><pre><p id="GraspNeRF" style="font:18px; display: none">
@article{dai2022graspnerf,
  title={GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF},
  author={Dai, Qiyu and Zhu, Yan and Geng, Yiran and Ruan, Ciyu and Zhang, Jiazhao and Wang, He},
  journal={arXiv preprint arXiv:2210.06575},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>		
	



<!-- RotLaplace -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/rotlaplace.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">A Laplace-inspired Distribution on SO(3) for Probabilistic Rotation Estimation</a></b><br><br>Yingda Yin, Yang Wang, <b>He Wang<span>&#8224;</span></b>, Baoquan Chen<span>&#8224;</span>
	<br><br> ICLR 2023 (<b><font color='red'>notable top 25%</font></b>) <br><br>
<a href="https://openreview.net/pdf?id=Mvetq8DO05O">paper</a>/<a href="javascript:hideshow(document.getElementById('RotLaplace'))">bibtex</a>
</p><pre><p id="RotLaplace" style="font:18px; display: none">
@inproceedings{
yin2023a,
title={A Laplace-inspired Distribution on {SO}(3) for Probabilistic Rotation Estimation},
author={Yingda Yin and Yang Wang and He Wang and Baoquan Chen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Mvetq8DO05O}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	

<!-- se3art -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/se3art.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance</a></b><br><br>Xueyi Liu, Ji Zhang, Ruizhen Hu, Haibin Huang, <b>He Wang</b>, Li Yi
	<br><br> ICLR 2023 <br><br>
<a href="https://openreview.net/forum?id=20GtJ6hIaPA">paper</a>/<a href="javascript:hideshow(document.getElementById('SE3Art'))">bibtex</a>
</p><pre><p id="SE3Art" style="font:18px; display: none">
@inproceedings{
liu2023selfsupervised,
title={Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level {SE}(3) Equivariance},
author={Xueyi Liu and Ji Zhang and Ruizhen Hu and Haibin Huang and He Wang and Li Yi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=20GtJ6hIaPA}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>


<!-- TRHOI -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/trhoi.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild</a></b><br><br>Jiayi Chen*, Mi Yan*, Jiazhao Zhang, Yenzhen Xu, Xiaolong Li, Yijia Weng, Li Yi, Shuran Song, <b>He Wang<span>&#8224;</span></b> 
	<br><br> AAAI 2023 (<font color="red">Oral Presentation</font>) <br><br>
<a href="https://arxiv.org/pdf/2210.06575.pdf">arXiv</a>/<a href="https://github.com/PKU-EPIC/HOTrack">Project</a>/<a href="javascript:hideshow(document.getElementById('TRHOI'))">bibtex</a>
</p><pre><p id="TRHOI" style="font:18px; display: none">
@article{chen2022tracking,
  title={Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild},
  author={Chen, Jiayi and Yan, Mi and Zhang, Jiazhao and Xu, Yinzhen and Li, Xiaolong and Weng, Yijia and Yi, Li and Song, Shuran and Wang, He},
  journal={arXiv preprint arXiv:2209.12009},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>	
		
	
	
<!-- TRO -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/tro.jpeg" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">ASRO-DIO: Active Subspace Random Optimization Based Depth Inertial Odometry</a ></b><br><br> Jiazhao Zhang, Yijie Tang, <b>He Wang</b>, Kai Xu
 <br><br> IEEE Transactions on Robotics (T-RO)<br><br>
<a href=" ">Paper</a>
<p></p ></td>
</tr>
<tr><td><br></td></tr> 
	
	
	
<!-- DREDS -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/dreds.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects</a></b><br><br>Qiyu Dai*, Jiyao Zhang*, Qiwei Li, Tianhao Wu, Hao Dong, Ziyuan Liu, Ping Tan, <b>He Wang<span>&#8224;</span></b> 
	<br><br> ECCV 2022 <br><br>
<a href="https://arxiv.org/pdf/2208.03792.pdf">arXiv</a>/<a href="https://pku-epic.github.io/DREDS/">Project</a>/<a href="https://github.com/PKU-EPIC/DREDS">Code</a>/<a href="javascript:hideshow(document.getElementById('dreds'))">bibtex</a>
</p><pre><p id="dreds" style="font:18px; display: none">
@article{dai2022domain,
  title={Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects},
  author={Dai, Qiyu and Zhang, Jiyao and Li, Qiwei and Wu, Tianhao and Dong, Hao and Liu, Ziyuan and Tan, Ping and Wang, He},
  journal={arXiv preprint arXiv:2208.03792},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>	
	
	

	
<!-- ManiSkill -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/maniskill.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Learning Category-Level Generalizable Object Manipulation Policy via Generative Adversarial Self-Imitation Learning from Demonstrations</a></b><br><br>Hao Shen*, Weikang Wan*, <b>He Wang<span>&#8224;</span></b> 
	<br><br> Robotics and Automation Letters (RA-L) and IROS 2022
	<br><br> <font color="red">1st prize winner</font> of <a href="https://sapien.ucsd.edu/challenges/maniskill2021/">SAPIEN ManiSkill Challenge 2021</a> (no external annotation track) <br><br>
<a href="https://arxiv.org/pdf/2203.02107.pdf">arXiv</a>/<a href="https://shen-hhao.github.io/Category_Level_Manipulation/">Project</a>/<a href="https://shen-hhao.github.io/Category_Level_Manipulation/">Code</a>/<a href="javascript:hideshow(document.getElementById('maniskill'))">bibtex</a>
</p><pre><p id="maniskill" style="font:18px; display: none">
@article{shen2022learning,
    title={Learning Category-Level Generalizable Object Manipulation Policy via Generative Adversarial Self-Imitation Learning from Demonstrations},
    author={Shen, Hao and Wan, Weikang and Wang, He},
    journal={arXiv preprint arXiv:2203.02107},
    year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
	
<!-- FisherMatch -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/fishermatch.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">FisherMatch: Semi-Supervised Rotation Regression via Entropy-based Filtering</a></b><br><br>Yingda Yin, Yingcheng Cai, <b>He Wang<span>&#8224;</span></b>, Baoquan Chen<span>&#8224;</span> <br><br> CVPR 2022 (<font color="red">Oral Presentation</font>) <br><br>
<a href="https://arxiv.org/pdf/2203.15765.pdf">arXiv</a>/<a href="https://yd-yin.github.io/FisherMatch/">Project</a>/<a href="https://github.com/yd-yin/FisherMatch">Code</a>/<a href="javascript:hideshow(document.getElementById('fishermatch'))">bibtex</a>
</p><pre><p id="fishermatch" style="font:18px; display: none">
@article{yin2022fishermatch,
  title={FisherMatch: Semi-Supervised Rotation Regression via Entropy-based Filtering},
  author={Yin, Yingda and Cai, Yingcheng and Wang, He and Chen, Baoquan},
  journal={arXiv preprint arXiv:2203.15765},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
	
<!-- RPMG -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/rpmg.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Projective Manifold Gradient Layer for Deep Rotation Regression</a></b><br><br>Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen, Leonidas Guibas, <b>He Wang<span>&#8224;</span></b> <br><br> CVPR 2022 <br><br>
<a href="https://arxiv.org/pdf/2110.11657.pdf">arXiv</a>/<a href="https://jychen18.github.io/RPMG/">Project</a>/<a href="https://github.com/jychen18/RPMG">Code</a>/<a href="javascript:hideshow(document.getElementById('rpmg'))">bibtex</a>
</p><pre><p id="rpmg" style="font:18px; display: none">
@article{chen2021projective,
  title={Projective Manifold Gradient Layer for Deep Rotation Regression},
  author={Chen, Jiayi and Yin, Yingda and Birdal, Tolga and Chen, Baoquan and Guibas, Leonidas and Wang, He},
  journal={arXiv preprint arXiv:2110.11657},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	

<!-- ADELA -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/adela.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
	<b><a href="">ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation</a></b><br><br>Yanchao Yang*, Hanxiang Ren*, <b>He Wang</b>, Bokui Shen, Qingnan Fan, Youyi Zheng, C. Karen Liu, Leonidas J. Guibas<br><br> CVPR 2022 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/2107.14285">arXiv</a>/<a href="javascript:hideshow(document.getElementById('adela'))">bibtex</a>
</p><pre><p id="adela" style="font:18px; display: none">
@article{yang2021adela,
  title={ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation},
  author={Yang, Yanchao and Ren, Hanxiang and Wang, He and Shen, Bokui and Fan, Qingnan and Zheng, Youyi and Liu, C Karen and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2107.14285},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
<!-- HOI4D -->
 <tbody><tr>
 <td width="25%" valign="top"><p><img src="./He_Wang_files/hoi4d.png" width="250" alt="" style="border-style: none" align="top"></p></td>
 <td width="75%" valign="top"><p>
 	<b><a href="">HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction</a></b><br><br>Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, <b>He Wang</b>, Li Yi <br><br> CVPR 2022 <br><br>
 <a href="https://arxiv.org/pdf/2203.01577.pdf">arXiv</a>/<a href="javascript:hideshow(document.getElementById('HOI4D'))">bibtex</a>
 </p><pre><p id="HOI4D" style="font:18px; display: none">
 @article{liu2022hoi4d,
   title={HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction},
   author={Liu, Yunze and Liu, Yun and Jiang, Che and Fu, Zhoujie and Lyu, Kangbo and Wan, Weikang and Shen, Hao and Liang, Boqiang and Wang, He and Yi, Li},
   journal={arXiv preprint arXiv:2203.01577},
   year={2022}
 }
 </p><p></p></pre>
 <p></p></td>
 </tr>
 <tr><td><br></td></tr>


<!-- MultiRobotMap -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/multirobotmap.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
	<b><a href="">Multi-Robot Active Mapping via Neural Bipartite Graph Matching</a></b><br><br>Kai Ye*, Siyan Dong*, Qingnan Fan, <b>He Wang</b>, Li Yi, Fei Xia, Jue Wang, Baoquan Chen <br><br> CVPR 2022 <br><br>
<a href="https://arxiv.org/pdf/2203.16319.pdf">arXiv</a>/<a href="javascript:hideshow(document.getElementById('MultiRobotMap'))">bibtex</a>
</p><pre><p id="MultiRobotMap" style="font:18px; display: none">
@article{ye2022multi,
  title={Multi-Robot Active Mapping via Neural Bipartite Graph Matching},
  author={Ye, Kai and Dong, Siyan and Fan, Qingnan and Wang, He and Yi, Li and Xia, Fei and Wang, Jue and Chen, Baoquan},
  journal={arXiv preprint arXiv:2203.16319},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>

	
<!-- CodedVTR -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/codedvtr.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
	<b><a href="">CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance</a></b><br><br>Tianchen Zhao, Niansong Zhang, Xuefei Ning, <b>He Wang</b>, Li Yi, Yu Wang <br><br> CVPR 2022 <br><br>
<a href="https://arxiv.org/pdf/2203.09887.pdf">arXiv</a>/<a href="javascript:hideshow(document.getElementById('CodedVTR'))">bibtex</a>
</p><pre><p id="CodedVTR" style="font:18px; display: none">
@article{zhao2022codedvtr,
  title={CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance},
  author={Zhao, Tianchen and Zhang, Niansong and Ning, Xuefei and Wang, He and Yi, Li and Wang, Yu},
  journal={arXiv preprint arXiv:2203.09887},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>

<!-- DAGAI -->
<td width="25%" valign="top"><p><img src="./He_Wang_files/dagai.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
	 <b><a href="">Domain Adaptation on Point Clouds via Geometry-Aware Implicits</a></b><br><br>Yuefan Shen*, Yanchao Yang*, Mi Yan, <b>He Wang</b>, Youyi Zheng, Leonidas J. Guibas<br><br> CVPR 2022 <br><br>
<a href="https://arxiv.org/abs/2112.09343">arXiv</a>/<a href="javascript:hideshow(document.getElementById('DAGAI'))">bibtex</a>
</p><pre><p id="DAGAI" style="font:18px; display: none">
@article{shen2021domain,
  title={Domain Adaptation on Point Clouds via Geometry-Aware Implicits},
  author={Shen, Yuefan and Yang, Yanchao and Yan, Mi and Wang, He and Zheng, Youyi and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2112.09343},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
	
	
<!-- ESSCOP -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/esscop.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/2111.00190.pdf">Leveraging SE(3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds</a></b><br><br>Xiaolong Li, Yijia Weng, Li Yi, Leonidas J. Guibas, A. Lynn Abbott, Shuran Song, <b>He Wang<span>&#8224;</span></b> <br><br> NeurIPS 21 <br><br>
<a href="https://openreview.net/forum?id=wGRNAqVBQT2">Paper</a>/<a href="https://arxiv.org/pdf/2111.00190.pdf">arXiv</a>/<a href="https://dragonlong.github.io/equi-pose/">Project</a>/<a href="https://github.com/dragonlong/equi-pose">Code</a>/<a href="javascript:hideshow(document.getElementById('esscop'))">bibtex</a>
</p><pre><p id="esscop" style="font:18px; display: none">
@article{li2021leveraging,
  title={Leveraging SE (3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds},
  author={Li, Xiaolong and Weng, Yijia and Yi, Li and Guibas, Leonidas J and Abbott, A and Song, Shuran and Wang, He},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
	
<!-- CAPTRA -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/captra.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds</a></b><br><br>Yijia Weng*, <b>He Wang*<span>&#8224;</span></b>, Qiang Zhou, Yuzhe Qin, Yueqi Duan, Qingnan Fan, Baoquan Chen, Hao Su, Leonidas J. Guibas<br><br> ICCV 2021 (<font color="red">Oral Presentation</font>) <br><br>
<a href="https://arxiv.org/abs/2104.03437">arXiv</a>/<a href="https://yijiaweng.github.io/CAPTRA/">Project</a>/<a href="https://github.com/halfsummer11/CAPTRA">Code</a>/<a href="https://youtu.be/JFPcOHCH2O0">Video</a>/<a href="javascript:hideshow(document.getElementById('captra'))">bibtex</a>
</p><pre><p id="captra" style="font:18px; display: none">
@article{weng2021captra,
  title={CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds},
  author={Weng, Yijia and Wang, He and Zhou, Qiang and Qin, Yuzhe and Duan, Yueqi and Fan, Qingnan and 
          Chen, Baoquan and Su, Hao and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:2104.03437},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	

<!-- Retrieval -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/retrieval.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Single Image 3D Shape Retrieval via Cross-Modal Instance and Category Contrastive Learning</a></b><br><br> Mingxian Lin, Jie Yang, <b>He Wang</b>, Yu-Kun Lai, Rongfei Jia, Binqiang Zhao, Lin Gao <br><br> ICCV 2021 <br><br>
<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Single_Image_3D_Shape_Retrieval_via_Cross-Modal_Instance_and_Category_ICCV_2021_paper.pdf">Paper</a>/<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Lin_Single_Image_3D_ICCV_2021_supplemental.pdf">Supp.</a>/<a href="javascript:hideshow(document.getElementById('retrievel'))">bibtex</a>
</p><pre><p id="retrievel" style="font:18px; display: none">
@inproceedings{lin2021single,
  title={Single Image 3D Shape Retrieval via Cross-Modal Instance and Category Contrastive Learning},
  author={Lin, Ming-Xian and Yang, Jie and Wang, He and Lai, Yu-Kun and Jia, Rongfei and Zhao, Binqiang and Gao, Lin},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11405--11415},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
	
</tr>
<tr><td><br></td></tr>
	
	


<!-- 3DIoUMatch -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/3dioumatch.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection</a></b><br><br> <b>He Wang*</b>, Yezhen Cong*, Or Litany, Yue Gao and Leonidas J. Guibas<br><br>CVPR 2021<br><br>
<a href="https://arxiv.org/pdf/2012.04355.pdf">Paper</a>/<a href="https://thu17cyz.github.io/3DIoUMatch/">Project</a>/<a href="https://github.com/THU17cyz/3DIoUMatch">Code</a>/<a href="https://youtu.be/nuARjhkQN2U">Video</a>/<a href="javascript:hideshow(document.getElementById('3dioumatch'))">bibtex</a>
</p><pre><p id="3dioumatch" style="font:18px; display: none">
@inproceedings{wang20213dioumatch,
  title={3DIoUMatch: Leveraging iou prediction for semi-supervised 3d object detection},
  author={Wang, He and Cong, Yezhen and Litany, Or and Gao, Yue and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14615--14624},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- MultiBodySync -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/multibodysync.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization</a></b><br><br> Jiahui Huang, <b>He Wang</b>, Tolga Birdal, Minkyuk Sung, Federica Arrigoni, Shi-Min Hu, and Leonidas J. Guibas<br><br>CVPR 2021 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/pdf/2101.06605.pdf">Paper</a>/<a href="https://github.com/huangjh-pub/multibody-sync">Code</a>/<a href="https://www.youtube.com/watch?v=BuIBXL2UNvI">Video</a>/<a href="javascript:hideshow(document.getElementById('multibodysync'))">bibtex</a>
</p><pre><p id="multibodysync" style="font:18px; display: none">
@inproceedings{huang2021multibodysync,
  title={Multibodysync: Multi-body segmentation and motion estimation via 3d scan synchronization},
  author={Huang, Jiahui and Wang, He and Birdal, Tolga and Sung, Minhyuk and Arrigoni, Federica and Hu, Shi-Min and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7108--7118},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>

<!-- NeuralRouting -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/neuralrouting.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments</a></b><br><br>Siyan Dong*, Qingnan Fan*, <b>He Wang</b>,  Ji Shi, Li Yi, Thomas Funkhouser, Baoquan Chen, Leonidas J. Guibas<br><br>CVPR 2021 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/2010.05272">Paper</a>/<a href="https://github.com/siyandong/NeuralRouting">Code</a>/<a href="javascript:hideshow(document.getElementById('neuralrouting'))">bibtex</a>
</p><pre><p id="neuralrouting" style="font:18px; display: none">
@InProceedings{Dong_2021_CVPR,
    author    = {Dong, Siyan and Fan, Qingnan and Wang, He and Shi, Ji and Yi, Li and Funkhouser, Thomas and Chen, Baoquan and Guibas, Leonidas J.},
    title     = {Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8544-8554}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>

	
<!-- Rethink -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/rethink.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks</a></b><br><br><b>He Wang*</b>, Zetian Jiang*, Li Yi, Kaichun Mo, Hao Su, Leonidas J. Guibas<br><br>CVPR 2021 Workshop on <i>Learning to Generate 3D Shapes and Scenes</i> <br><br>
<a href="https://arxiv.org/abs/2006.07029">Paper</a>/<a href="https://drive.google.com/file/d/1RF77Zp6cQgoU0tf33Wjthx0D6tr3IYAJ/view?usp=sharing">Poster</a>/<a href="https://www.youtube.com/watch?v=Ejzj0hnKW4Y">Video</a>/<a href="javascript:hideshow(document.getElementById('rethink'))">bibtex</a>
</p><pre><p id="rethink" style="font:18px; display: none">
@article{wang2020rethinking,
  title={Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks},
  author={Wang, He and Jiang, Zetian and Yi, Li and Mo, Kaichun and Su, Hao and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:2006.07029},
  year={2020}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>

<!-- PT2PC -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/pt2pc.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions</a></b><br><br>Kaichun Mo, <b>He Wang</b>, Li Yi, Xinchen Yan and Leonidas J. Guibas<br><br>ECCV 2020<br><br>
<a href="https://arxiv.org/abs/2003.08624">Paper</a>/<a href="https://cs.stanford.edu/~kaichun/pt2pc/">Project</a>/<a href="https://github.com/daerduoCarey/pt2pc">Code&Data</a>/<a href="javascript:hideshow(document.getElementById('pt2pc'))">bibtex</a>
</p><pre><p id="pt2pc" style="font:18px; display: none">
@article{mo2020pt2pc,
    title={{PT2PC}: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions},
    author={Mo, Kaichun and Wang, He and Yan, Xinchen and Guibas, Leonidas},
    journal={arXiv preprint arXiv:2003.08624},
    year={2020}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- Curriculm -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/csdf.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Curriculum DeepSDF</a></b><br><br>Yueqi Duan*, Haidong Zhu*, <b>He Wang</b>, Li Yi, Ram Nevatia, Leonidas J. Guibas<br><br>ECCV 2020<br><br>
<a href="https://arxiv.org/abs/2003.08593">Paper</a>/<a href="https://github.com/haidongz-usc/Curriculum-DeepSDF">Code</a>/<a href="javascript:hideshow(document.getElementById('csdf'))">bibtex</a>
</p><pre><p id="csdf" style="font:18px; display: none">
@misc{duan2020curriculum,
    title={Curriculum DeepSDF},
    author={Yueqi Duan and Haidong Zhu and He Wang and Li Yi and Ram Nevatia and Leonidas J. Guibas},
    year={2020},
    eprint={2003.08593},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- ANCSH -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/ancsh.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Category-level Articulated Object Pose Estimation</a></b><br><br><b>He Wang*</b>, Xiaolong Li*, Li Yi, Leonidas Guibas, A. Lynn Abbott, Shuran Song<br><br>CVPR 2020 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/1912.11913">Paper</a>/<a href="https://articulated-pose.github.io/">Project</a>/<a href="https://github.com/dragonlong/articulated-pose">Code&Data</a>/<a href="javascript:hideshow(document.getElementById('ancsh'))">bibtex</a>
</p><pre><p id="ancsh" style="font:18px; display: none">
@article{li2019category,
  title={Category-Level Articulated Object Pose Estimation},
  author={Li, Xiaolong and Wang, He and Yi, Li and Guibas, Leonidas and Abbott, A Lynn and Song, Shuran},
  journal={arXiv preprint arXiv:1912.11913},
  year={2019}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SAPIEN -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/sapien.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">SAPIEN: A SimulAted Part-based Interactive ENvironment</a></b><br><br>Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan,<br><b>He Wang</b>, Li Yi, Angel X.Chang, Leonidas J. Guibas and Hao Su<br><br>CVPR 2020 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/2003.08515">Paper</a>/<a href="https://sapien.ucsd.edu/">Project</a>/<a href="https://github.com/haosulab/SAPIEN-Release">Code&Data</a>/<a href="https://youtu.be/K2yOeJhJXzM">Demo</a>/<a href="javascript:hideshow(document.getElementById('sapien'))">bibtex</a>
</p><pre><p id="sapien" style="font:18px; display: none">
@InProceedings{Xiang_2020_SAPIEN,
    author = {Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao 
                  and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu 
                  and Wang, He and Yi, Li and Chang, Angel X. and Guibas, Leonidas J. and Su, Hao},
    title = {{SAPIEN}: A SimulAted Part-based Interactive ENvironment},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- Pose RCNN -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/posercnn.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation</a></b><br><br><b>He Wang</b>, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, Leonidas J. Guibas<br><br>CVPR 2019 (<font color="red">Oral Presentation</font>), <a href='https://mp.weixin.qq.com/s/LAVMLCsi3-XHWLbklomxkg'>WAICYOP Award 2022</a> <br><br>
<a href="https://arxiv.org/abs/1901.02970">Paper</a>/<a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/">Project</a>/<a href="https://github.com/hughw19/NOCS_CVPR2019">Code&Data</a>/<a href="javascript:hideshow(document.getElementById('posercnn'))">bibtex</a>
</p><pre><p id="posercnn" style="font:18px; display: none">
@inproceedings{wang2019normalized,
  title={Normalized object coordinate space for category-level 6d object pose and size estimation},
  author={Wang, He and Sridhar, Srinath and Huang, Jingwei and Valentin, Julien and Song, Shuran and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2642--2651},
  year={2019}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
    
<!-- GSPN -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/gspn.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud</a></b><br><br>Li Yi, Wang Zhao, <b>He Wang</b>, Minhyuk Sung, Leonidas Guibas<br><br>CVPR 2019<br><br>
<a href="https://arxiv.org/abs/1812.03320">Paper</a>/<a href="https://github.com/ericyi/GSPN">Code</a>/<a href="javascript:hideshow(document.getElementById('gspn'))">bibtex</a>
</p><pre><p id="gspn" style="font:18px; display: none">
@article{yi2018gspn,
  title={GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud},
  author={Yi, Li and Zhao, Wang and Wang, He and Sung, Minhyuk and Guibas, Leonidas},
  journal={arXiv preprint arXiv:1812.03320},
  year={2018}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Genertive Model for Interaction -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/interaction.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Learning a Generative Model for Multi-Step Human-Object Interactions from Videos</a></b><br><br><b>He Wang*</b>, Soeren Pirk*, Ersin Yumer, Vladimir Kim, Ozan Sener, Srinath Sridhar, Leonidas J. Guibas<br><br>Eurographics 2019 (<font color="red">Best Paper Honorable Mention</font>)<br><br>
<a href="http://www.pirk.info/projects/learning_interactions/index.html">Project</a>/<a href="./He_Wang_files/19_EG_FnInteract.pdf">Paper</a>/<a href="https://github.com/hughw19/ActionPlotGeneration.git">Code</a>/<a href="http://ai.stanford.edu/blog/generate-human-object/">Blog</a>/<a href="https://www.youtube.com/watch?v=WgpPalA2RzA">Video</a>
</p><pre><p id="interaction" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

</tbody></table>

</ul>
</div>
<br>






<a name="education"></a>
<div class="section">
<h3>Education</h3>
<ul>
<li>2015.9 - 2021.9: Ph.D. in Electrical Engineering, Stanford University</li>
<li>2014.9 - 2017.6: M.S. in Electrical Engineering, Stanford University</li>
<li>2010.9 - 2014.7: B.Eng. in Microelectronics and Nanoelectronics, Tsinghua University</li>
</ul>
</div>
<br>

<!-- <a name="experiences"></a>
<div class="section">
<h3>Experiences</h3>
<ul>
<li>2019.6 - 2019.9: Research intern at the robotics team of Facebook AI Research (FAIR).</li>
<li>2019.2 - 2019.5: Student researcher at Google Daydream team.</li>
<li>2018.6 - 2018.9: Intern at Google Daydream team.</li>
</ul>
</div>
<br> -->

<a name="service"></a>
<div class="section">
<h3>Professional service</h3>
<ul>
    <li> Associate Editor: <a href="https://www.sciencedirect.com/journal/image-and-vision-computing">Image and Vision Computing</a>
	    
    <li> Area chair (AC):
        <ul>
            <li> Conferences: CVPR 2022, WACV 2022</li>
	    <li> Seminars: <a href="http://valser.org">VALSE</a></li>
        </ul>
    
    <li> Program committee/reviewer:
    <ul>
        <li> Conferences: CVPR, ICCV, ECCV, NeurIPS, ICLR, AAAI, SIGGRAPH, RSS, IROS, ICRA </li>
        <li> Journals: IEEE TPAMI, IEEE RAL</li>
    </ul>
</div>



<hr>
<div id="footer" style="font-size:10">He Wang <i>Last updated: Apr. 22, 2023 </i></div>

</body></html>
