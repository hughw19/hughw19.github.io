<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1, minimum-scale=1"
    />
    <title>He Wang</title>
    <link rel="stylesheet" type="text/css" href="./css/home10.css" />
  </head>
  <body>
    <div class="main">
      <div class="main-head">
        <div class="head-pc">
          <div class="name">He Wang</div>
          <div class="list">
            <div class="list-item"><a href="#news">News</a></div>
            <div class="list-item">
              <a href="#publications">Publications</a>
            </div>
            <div class="list-item"><a href="#awards">Awards</a></div>
            <div class="list-item"><a href="#teaching">Teaching</a></div>
            <div class="list-item">
              <a href="#professional">Professional Service</a>
            </div>
            <div class="list-item">
              <a href="#opportunities">Opportunities</a>
            </div>
          </div>

          <div class="menu">
            <div class="menu-item active">
              <a href="https://hughw19.github.io">Home</a>
            </div>
            <span>/</span>
            <div class="menu-item">
              <a href="https://PKU-EPIC.github.io">Lab</a>
            </div>
          </div>
        </div>
        <div class="head-phone">
          <div class="name">He Wang</div>
          <svg
            class="menu-phone"
            xmlns="http://www.w3.org/2000/svg"
            width="21"
            height="19"
            viewBox="0 0 21 19"
            fill="none"
          >
            <path
              fill-rule="evenodd"
              clip-rule="evenodd"
              d="M0 1.5C0 0.671573 0.671573 0 1.5 0H19.5C20.3284 0 21 0.671573 21 1.5C21 2.32843 20.3284 3 19.5 3H1.5C0.671573 3 0 2.32843 0 1.5ZM0 9.5C0 8.67157 0.671573 8 1.5 8H19.5C20.3284 8 21 8.67157 21 9.5C21 10.3284 20.3284 11 19.5 11H1.5C0.671573 11 0 10.3284 0 9.5ZM1.5 16C0.671573 16 0 16.6716 0 17.5C0 18.3284 0.671573 19 1.5 19H19.5C20.3284 19 21 18.3284 21 17.5C21 16.6716 20.3284 16 19.5 16H1.5Z"
              fill="#808080"
            />
          </svg>
        </div>
      </div>
      <div class="main-content">
        <div class="main-userinfo">
          <div class="info">
            <div>
              <img class="lazy-load" src="./assets/photo1.jpg" alt="" />
              <div class="icon-phone">
                <a
                  href="https://scholar.google.com/citations?user=roCAWkoAAAAJ&hl=en"
                  ><img
                    style="width: 18px; height: 18px"
                    src="./assets/ic_1.svg"
                    alt=""
                /></a>
                <a href="mailto:hewang@pku.edu.cn"
                  ><img
                    style="width: 19px; height: 15px"
                    src="./assets/ic_4.svg"
                    alt=""
                /></a>
                <a href="https://twitter.com/HughWang19"
                  ><img
                    style="width: 19px; height: 16px"
                    src="./assets/ic_3.svg"
                    alt=""
                /></a>
              </div>
            </div>
            <div class="info-right">
              <div class="info-title">Prof. He Wang</div>
              <p>
                Tenure-track Assistant Professor at
                <a href="https://english.pku.edu.cn/">Peking University</a>
              </p>
              <p>
                Director of
                <i>Embodied Perception and InteraCtion (EPIC) Lab</i>
              </p>
              <p>Director of <i> PKU-Galbot Joint Lab of Embodied AI</i></p>
              <div class="icon">
                <a
                  href="https://scholar.google.com/citations?user=roCAWkoAAAAJ&hl=en"
                  ><img
                    style="width: 18px; height: 18px"
                    src="./assets/ic_1.svg"
                    alt=""
                /></a>
                <a href="mailto:hewang@pku.edu.cn"
                  ><img
                    style="width: 19px; height: 15px"
                    src="./assets/ic_4.svg"
                    alt=""
                /></a>
                <a href="https://twitter.com/HughWang19"
                  ><img
                    style="width: 19px; height: 16px"
                    src="./assets/ic_3.svg"
                    alt=""
                /></a>
              </div>
            </div>
          </div>
          <div class="decs">
            <p>
              I am a tenure-track assistant professor in the
              <a href="https://cfcs.pku.edu.cn/english"
                >Center on Frontiers of Computing Studies (CFCS)
              </a>
              at <a href="https://english.pku.edu.cn/">Peking University.</a> I
              founded and lead the
              <i>Embodied Perception and InteraCtion (EPIC) Lab</i>, with the
              mission of developing generalizable skills and embodied multimodal
              embodied multimodal large models for robots to facilitate embodied
              AGI.
            </p>
            <p>
              I am also the founder and CTO of Beijing Galbot Co., Ltd., and the
              director of the BAAI Center of Embodied AI.
            </p>
            <!-- <p>
              I serve as an associate editor of Image and Vision Computing and
              serve as an area chair in CVPR 2022 and WACV 2022. Prior to
              joining Peking University, I received my Ph.D. degree from
              <a href="https://www.stanford.edu">Stanford University</a> in 2021
              under the advisory of Prof.
              <a href="http://geometry.stanford.edu/member/guibas/index.html"
                >Leonidas J.Guibas</a
              >
              and my Bachelor's degree from
              <a href="http://www.tsinghua.edu.cn/">Tsinghua University</a> in
              2014.
            </p> -->
          </div>
        </div>
      </div>
      <div class="main-content" style="padding-bottom: 38px">
        <div class="main-swiper" id="videos">
          <div class="swiper-content">
            <div class="slide-source">
              <a>
                <video id="video01" autoplay loop muted>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/trackvla.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p01">
                  TrackVLA is a product-level navigation large model introduced
                  by Galbot, capable of pure visual environmental perception and
                  driven by natural language instructions.
                </div>
              </a>
            </div>
            <div class="slide-source">
              <a>
                <video id="video02" autoplay loop muted>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/grasp.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p02">
                  GraspVLA is the world's first end-to-end embodied grasping
                  foundation model. Its pre-training is entirely based on
                  billion-scale "vision-language-action" synthetic data.
                </div>
              </a>
            </div>
            <div class="slide-source">
              <a>
                <video style="width: 100%" id="video03" autoplay loop muted>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/grocery.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p03">
                  GroceryVLA is the world's first end-to-end embodied VLA large
                  model designed for the retail industry.
                </div>
              </a>
            </div>
            <div class="slide-source">
              <a href="https://pku-epic.github.io/ASGrasp/">
                <video id="video1" autoplay loop muted>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p1">
                  Material-agnostic generalized grasping technology, capable of
                  handling scenes with completely transparent objects with high
                  success.
                </div>
              </a>
            </div>
            <div class="slide-source">
              <a href="  https://pku-epic.github.io/DexGraspNet/">
                <video id="video2" autoplay loop muted>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech7-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p2">
                  《DexGraspNet》ICRA 2023 Best Manipulation Paper
                  Nominee，Million-level dexterous hands Dataset.
                </div>
              </a>
            </div>
            <script>
              var video = document.getElementById('video2');
              var container = document.getElementById('p2');
              container.style.width = video.clientWidth - 56 + 'px';
            </script>
            <div class="slide-source">
              <a href="https://pku-epic.github.io/Open6DOR/">
                <video id="video3" autoplay loop muted>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech2-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p3">
                  The world's first open command large model system for object
                  pick-and-place, capable of controlling object orientation:
                  Open6DOR.
                </div>
              </a>
            </div>
            <script>
              var video = document.getElementById('video3');
              var container = document.getElementById('p3');
              container.style.width = video.clientWidth - 56 + 'px';
            </script>
            <div class="slide-source">
              <a href="https://pku-epic.github.io/NaVid/">
                <video autoplay muted loop>
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech3-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div>
                  The world's first generalized embodied navigation large model:
                  NaVid.
                </div>
              </a>
            </div>
          </div>
          <div class="swiper-content-phone" id="videos01">
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/trackvla.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video01"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/trackvla.jpg"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/trackvla.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p01">
                <a
                  >TrackVLA is a product-level navigation large model introduced
                  by Galbot, capable of pure visual environmental perception and
                  driven by natural language instructions.
                </a>
              </div>
            </div>
            <div class="slide-source">
              <a href="https://oss-cn-beijing.galbot.com/online/blog/grasp.mp4">
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video02"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/grasp.jpg"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/grasp.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p02">
                <a
                  >GraspVLA is the world's first end-to-end embodied grasping
                  foundation model. Its pre-training is entirely based on
                  billion-scale "vision-language-action" synthetic data.
                </a>
              </div>
            </div>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/grocery.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video03"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/grocery.jpg"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/grocery.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p03">
                <a
                  >GroceryVLA is the world's first end-to-end embodied VLA large
                  model designed for the retail industry.
                </a>
              </div>
            </div>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video1"
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech6.png"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p1">
                <a href="https://pku-epic.github.io/ASGrasp/"
                  >Material-agnostic generalized grasping technology, capable of
                  handling scenes with completely transparent objects with high
                  success.
                </a>
              </div>
            </div>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech7-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  id="video2"
                  x5-video-player-type="h5-page"
                  loop
                  muted
                  controlsList="nodownload noPictureInPicture"
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech7.png"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech7-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p2">
                <a href="  https://pku-epic.github.io/DexGraspNet/"
                  >《DexGraspNet》ICRA 2023 Best Manipulation Paper
                  Nominee，Million-level dexterous hands Dataset.
                </a>
              </div>
            </div>
            <script>
              var video = document.getElementById('video2');
              var container = document.getElementById('p2');
              container.style.width = video.clientWidth - 56 + 'px';
            </script>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech2-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />

                <video
                  id="video3"
                  loop
                  controlsList="nodownload noPictureInPicture"
                  muted
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech2.png"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech2-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p3">
                <a href="https://pku-epic.github.io/Open6DOR/">
                  The world's first open command large model system for object
                  pick-and-place, capable of controlling object orientation:
                  Open6DOR.
                </a>
              </div>
            </div>
            <script>
              var video = document.getElementById('video3');
              var container = document.getElementById('p3');
              container.style.width = video.clientWidth - 56 + 'px';
            </script>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech3-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  muted
                  controlsList="nodownload noPictureInPicture"
                  loop
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech3.png"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech3-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div>
                <a href="https://pku-epic.github.io/NaVid/">
                  The world's first generalized embodied navigation large model:
                  NaVid.
                </a>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="main-content">
        <div id="opportunities" class="main-news" style="padding-bottom: 50px">
          <div class="title">
            <span>OPPORTUNITIES</span>
            <img
              id="lang-cn"
              class="opportunities-language cn"
              src="./assets/cn.png"
              alt=""
            />
            <img
              id="lang-en"
              class="opportunities-language en"
              src="./assets/en.png"
              alt=""
            />
          </div>
          <div style="padding-bottom: 18px; border-bottom: 1px dashed #d8d8d8">
            <div class="sub-title" data-i18n="opportunities.subTitle1"></div>
            <p data-i18n="opportunities.tips"></p>
            <ul style="padding-bottom: 0px">
              <li>
                <b data-i18n="opportunities.li1"></b>
              </li>
              <li>
                <b data-i18n="opportunities.li2"></b>
              </li>
              <li>
                <b data-i18n="opportunities.li3"></b>
              </li>
              <li>
                <b data-i18n="opportunities.li4"></b>
              </li>
            </ul>
            <p data-i18n="opportunities.p1"></p>
            <p data-i18n="opportunities.p2"></p>
          </div>
          <div style="padding-bottom: 18px; border-bottom: 1px dashed #d8d8d8">
            <div class="sub-title" data-i18n="opportunities.subTitle2"></div>
            <p data-i18n="opportunities.p3"></p>
            <p data-i18n="opportunities.p4"></p>
            <p data-i18n="opportunities.p5"></p>
          </div>
          <div class="sub-title" data-i18n="opportunities.subTitle3"></div>
          <p data-i18n="opportunities.p6"></p>
          <ul style="padding-bottom: 0px">
            <li data-i18n="opportunities.li5"></li>
            <li data-i18n="opportunities.li6"></li>
          </ul>
          <p data-i18n="opportunities.p7"></p>
        </div>
      </div>
      <div class="main-content">
        <div id="news" class="main-news">
          <div class="title">NEWS</div>
          <ul>
            <li>
              I am invited to be a speaker in
              <a href="https://humanoidssummit.com/agenda" target="_blank"
                >Humanoids Summit 2025.</a
              >
            </li>
            <li>
              Our project, the "Synthetic and real data-driven VLA embodied
              intelligence large model" was honored with the
              <a
                href="https://www.cac.gov.cn/2025-11/06/c_1764156715189004.htm"
                target="_blank"
                >2025 World Internet Conference Leading Technology Award.</a
              >
            </li>
            <li>One paper gets accepted to RA-L.</li>
            <li>
              I have been selected for Fortune magazine's 2025 "China's 40 Under
              40 Business Leaders" list.
            </li>
            <li>I have been selected for the 9th "Haiying Talent" program.</li>
            <li>
              Two papers get accepted to NeurIPS,with one selected as a
              spotlight.
            </li>
            <li>
              I am invited to be a speaker in
              <a
                href="https://iccv.thecvf.com/Conferences/2025/ACWorkshop"
                target="_blank"
                >Area Chair Workshop</a
              >
              at ICCV 2025.
            </li>
            <li>
              I am invited to be a speaker in the 3rd edition of the TRICKY
              workshop(Transparent & Reflective Objects in the Wild Challenges)
              at ICCV 2025.
            </li>
            <li>
              I am invited to be a speaker in
              <a href="https://wclop.github.io" target="_blank"
                >the 1st Workshop and Challenge on Category-Level Object Pose
                Estimation in the Wild</a
              >
              at ICCV 2025.
            </li>
            <li>
              I am invited to be a speaker in
              <a href="https://heai-iros25-workshop.github.io" target="_blank"
                >the Workshop on Human-aware Embodied AI</a
              >
              at IROS 2025.
            </li>
            <!-- <li>...</li> -->
          </ul>
        </div>
      </div>
      <div class="main-content">
        <div id="publications" class="main-selected">
          <div class="title">SELECTED PUBLICATIONS</div>
          <div class="selected-item">
            <img src="./assets/navfom.gif" alt="" />
            <div class="item-right">
              <div class="item-title">Embodied Navigation Foundation Model</div>
              <p>
                Jiazhao Zhang*, Anqi Li*, Yunpeng Qi*, Minghan Li*, Jiahang Liu,
                Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li,
                Yuxin Fan, Wenjun Li, Zhibo Chen, Fei Gao, Qi Wu,
                <b>Zhizheng Zhang<span>&#8224;</span></b
                >, <b>He Wang<span>&#8224;</span></b>
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2509.12129">arXiv</a>
                <a href="https://pku-epic.github.io/NavFoM-Web">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img src="./assets/trackvla.gif" alt="" />
            <div class="item-right">
              <div class="item-title">
                TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA
                Models for Embodied Visual Tracking
              </div>
              <p>
                Jiahang Liu*, Yunpeng Qi*, Jiazhao Zhang*, Minghan Li, Shaoan
                Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong,
                <b>Zhizheng Zhang<span>&#8224;</span></b
                >, <b>He Wang<span>&#8224;</span></b>
              </p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2510.07134">arXiv</a>
                <a href="https://pku-epic.github.io/TrackVLA-plus-plus-Web"
                  >Project</a
                >
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img src="./assets/urbanvla.gif" alt="" />
            <div class="item-right">
              <div class="item-title">
                UrbanVLA: A Vision-Language-Action Model for Urban Micromobility
              </div>
              <p>
                Anqi Li*, Zhiyong Wang*, Jiazhao Zhang*, Minghan Li, Zhibo Chen,
                <b>Zhizheng Zhang<span>&#8224;</span></b
                >, <b>He Wang<span>&#8224;</span></b>
              </p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2510.23576">arXiv</a>
                <a href="https://pku-epic.github.io/UrbanVLA-Web">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img src="./assets/mm_nav.gif" alt="" />
            <div class="item-right">
              <div class="item-title">
                MM-Nav: Multi-View VLA Model for Robust Visual Navigation via
                Multi-Expert Learning
              </div>
              <p>
                Tianyu Xu*, Jiawei Chen*, Jiazhao Zhang*, Wenyao Zhang, Zekun
                Qi, Minghan Li, <b>Zhizheng Zhang<span>&#8224;</span></b
                >, <b>He Wang<span>&#8224;</span></b>
              </p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2510.03142">arXiv</a>
                <a href="https://pku-epic.github.io/MM-Nav-Web">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/73.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Robust Differentiable Collision Detection for General Objects
              </div>
              <p>
                Jiayi Chen, Wei Zhao, Liangwang Ruan, Baoquan Chen,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2511.06267">arXiv</a>
                <a href="https://github.com/JYChen18/DiffCollision">Project</a>
                <a
                  id="chen2025robustID"
                  href="javascript:hideshow(document.getElementById('chen2025robustID'),document.getElementById('chen2025robust'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="chen2025robust" style="font-size:14px; display: none">
                @article{chen2025robust,
                  title={Robust Differentiable Collision Detection for General Objects},
                  author={Chen, Jiayi and Zhao, Wei and Ruan, Liangwang and Chen, Baoquan and Wang, He},
                  journal={arXiv preprint arXiv:2511.06267},
                  year={2025}
                }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/72.jpg" alt="" />
            <div class="item-right">
              <div class="item-title">
                Unleashing Humanoid Reaching Potential via Real-world-Ready
                Skill Space
              </div>
              <p>
                Zhikai Zhang*,Chao Chen*, Han Xue*,Jilong Wang*,Sikai Liang*,Yun
                Liu*,Zongzhang Zhang*, He Wang,<b>Li Yi<span>&#8224;</span></b>
              </p>
              <p class="decs">RA-L</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2507.04447">arXiv</a>
                <a href="https://zzk273.github.io/R2S2">Project</a>
                <a
                  id="UnleashingID"
                  href="javascript:hideshow(document.getElementById('UnleashingID'),document.getElementById('article-Unleashing'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-Unleashing" style="font-size:14px; display: none">
            @article{DBLP:journals/corr/abs-2505-10918,
              author= {Zhikai Zhang and
                              Chao Chen and
                              Han Xue and
                              Jilong Wang and
                              Sikai Liang and
                              Yun Liu and
                              Zongzhang Zhang and
                              He Wang and
                              Li Yi},
              title= {Unleashing Humanoid Reaching Potential via Real-world-Ready Skill
                              Space},
              journal= {CoRR},
              volume= {abs/2505.10918},
              year= {2025},
              url= {https://doi.org/10.48550/arXiv.2505.10918},
              doi= {10.48550/ARXIV.2505.10918},
              eprinttype= {arXiv},
              eprint= {2505.10918},
              timestamp= {Sun, 29 Jun 2025 10:28:00 +0200},
              biburl= {https://dblp.org/rec/journals/corr/abs-2505-10918.bib},
              bibsource= {dblp computer science bibliography, https://dblp.org}
            }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/72.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                DreamVLA: A Vision-Language-Action Model Dreamed with
                Comprehensive World Knowledge
              </div>
              <p>
                Wenyao Zhang*, Hongsi Liu*, Zekun Qi*, Yunnan Wang*, XinQiang
                Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, He Wang, Zhizheng
                Zhang, Li Yi, Wenjun Zeng, <b>Xin Jin<span>&#8224;</span></b>
              </p>
              <p class="decs">NeurIPS 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2507.04447">arXiv</a>
                <a href="https://zhangwenyao1.github.io/DreamVLA/">Project</a>
              </div>
            </div>
          </div> -->
          <div class="selected-item">
            <img class="lazy-load" src="./assets/71.jpg" alt="" />
            <div class="item-right">
              <div class="item-title">
                SoFar: Language-Grounded Orientation Bridges Spatial Reasoning
                and Object Manipulation
              </div>
              <p>
                Zekun Qi*, Wenyao Zhang*, Yufei Ding*, Runpei Dong, XinQiang Yu,
                Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, Jiazhao
                Zhang, Jiawei He, Jiayuan Gu, Xin Jin, Kaisheng Ma,
                <b>Zhizheng Zhang<span>&#8224;</span></b
                >, <b>He Wang<span>&#8224;</span></b
                >, <b>Li Yi<span>&#8224;</span></b>
              </p>
              <p class="decs">NeurIPS 2025(<b>spotlight</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2502.13143">arXiv</a>
                <a href="https://qizekun.github.io/sofar/">Project</a>
                <a
                  id="sofar25"
                  href="javascript:hideshow(document.getElementById('sofar25'),document.getElementById('article-sofar25'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-sofar25" style="font-size:14px; display: none">
            bibtex: @article{sofar25,
              title={Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation},
              author={Qi, Zekun and Zhang, Wenyao and Ding, Yufei and Dong, Runpei and Yu, Xinqiang and Li, Jingwen and Xu, Lingyun and Li, Baoyu and He, Xialin and Fan, Guofan and others},
              journal={arXiv preprint arXiv:2502.13143},
              year={2025}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/70.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Advancing general robotic manipulation with multimodal
                foundation models: Anembodied Al paradigm
              </div>
              <p>
                Shifeng HUANG , He WANG , Xing ZHOU , Wenkai CHEN , Haibin YANG
                , Jianwei ZHANG
              </p>
              <p class="decs">SCIENCE CHINA Technological Sciences</p>
              <div class="button">
                <a
                  href="https://link.springer.com/article/10.1007/s11431-024-2910-3#citeas"
                  >arXiv</a
                >
              </div>
            </div>
          </div>
          <div class="selected-item">
            <a
              href="https://oss-cn-beijing.galbot.com/online/blog/trackvla1.mp4"
            >
              <img class="play" src="/assets/play1.png" alt="" />
              <img
                class="lazy-load"
                src="https://oss-cn-beijing.galbot.com/online/blog/trackvla1.png"
                alt=""
              />
            </a>
            <div class="item-right">
              <div class="item-title">
                TrackVLA: Embodied Visual Tracking in the Wild
              </div>
              <p>
                Shaoan Wang∗，Jiazhao Zhang∗, Minghan Li，Jiahang Liu，Anqi
                Li，Kui Wu， Fangwei Zhong，Junzhi Yu，<b
                  >Zhizheng Zhang<span>&#8224;</span></b
                >,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CoRL 2025</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2505.23189">arXiv</a>
                <a href="https://pku-epic.github.io/TrackVLA-web/">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/69.gif" alt="" />
            <div class="item-right">
              <div class="item-title">
                FetchBot: Learning Generalizable Object Fetching in Cluttered
                Scenes via Zero-Shot Sim2Real
              </div>
              <p>
                Weiheng Liu*, Yuxuan Wan*, Jilong Wang, Yuxuan Kuang, Wenbo Cui,
                Xuesong Shi, Haoran Li, Dongbin Zhao,
                <b>Zhizheng Zhang<span>&#8224;</span></b
                >, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CoRL 2025（<b>Oral</b>）</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2502.17894">arXiv</a>
                <a href="https://pku-epic.github.io/FetchBot">Project</a>
                <a
                  id="fetchbot"
                  href="javascript:hideshow(document.getElementById('fetchbot'),document.getElementById('article-fetchbot'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-fetchbot" style="font-size:14px; display: none">
            @inproceedings{liufetchbot,
              title={FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real},
              author={Liu, Weiheng and Wan, Yuxuan and Wang, Jilong and Kuang, Yuxuan and Shi, Xuesong and Li, Haoran and Zhao, Dongbin and Zhang, Zhizheng and Wang, He},
              booktitle={9th Annual Conference on Robot Learning}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/68.jpg" alt="" />
            <div class="item-right">
              <div class="item-title">
                GraspVLA: a Grasping Foundation Model Pre-trained on
                Billion-scale Synthetic Action Data
              </div>
              <p>
                Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang,
                Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui,
                Zhizheng Zhang, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CoRL 2025</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2505.03233">arXiv</a>
                <a href="https://pku-epic.github.io/GraspVLA-web">Project</a>
                <a
                  id="GraspVLA"
                  href="javascript:hideshow(document.getElementById('GraspVLA'),document.getElementById('article-grasp-vla'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-grasp-vla" style="font-size:14px; display: none">
            @article{deng2025graspvla,
                title={GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data}, 
                author={Shengliang Deng and Mi Yan and Songlin Wei and Haixin Ma and Yuxin Yang and Jiayi Chen and Zhiqi Zhang and Taoyu Yang and Xuheng Zhang and Wenhao Zhang and Heming Cui and Zhizheng Zhang and He Wang},
                year={2025},
                eprint={2505.03233},
                archivePrefix={arXiv},
                primaryClass={cs.RO},
                url={https://arxiv.org/abs/2505.03233}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/67.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                DexVLG: Dexterous Vision-Language-Grasp Model at Scale
              </div>
              <p>
                Jiawei He*, Danshi Li*, Xinqiang Yu*, Zekun Qi, Wenyao Zhang,
                Jiayi Chen, <b>Zhaoxiang Zhang<span>&#8224;</span></b
                >, <b>Zhizheng Zhang<span>&#8224;</span></b
                >, <b>Li Yi<span>&#8224;</span></b
                >, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICCV 2025（<b>highlight</b>）</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2507.02747">arXiv</a>
                <a href="https://jiaweihe.com/dexvlg">Project</a>
                <a
                  id="misc-dexvlg"
                  href="javascript:hideshow(document.getElementById('misc-dexvlg'),document.getElementById('dexvlg'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="dexvlg" style="font-size:14px; display: none">
              @misc{he2025dexvlgdexterousvisionlanguagegraspmodel,
                title={DexVLG: Dexterous Vision-Language-Grasp Model at Scale}, 
                author={Jiawei He and Danshi Li and Xinqiang Yu and Zekun Qi and Wenyao Zhang and Jiayi Chen and Zhaoxiang Zhang and Zhizheng Zhang and Li Yi and He Wang},
                year={2025},
                eprint={2507.02747},
                archivePrefix={arXiv},
                primaryClass={cs.CV},
                url={https://arxiv.org/abs/2507.02747}, 
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/66.gif" alt="" />
            <div class="item-right">
              <div class="item-title">
                DyWA: Dynamics-adaptive World Action Model for Generalizable
                Non-prehensile Manipulation
              </div>
              <p>
                Jiangran Lyu, Ziming Li, Xuesong Shi , Chaoyi Xu,
                <b>Yizhou Wang<span>&#8224;</span></b
                >, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICCV 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2503.16806">arXiv</a>
                <a href="https://pku-epic.github.io/DyWA">Project</a>
                <a
                  id="article-dywa"
                  href="javascript:hideshow(document.getElementById('article-dywa'),document.getElementById('dywa'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="dywa" style="font-size:14px; display: none">
              @article{lyu2025dywa,
                title={DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation},
                author={Lyu, Jiangran and Li, Ziming and Shi, Xuesong and Xu, Chaoyi and Wang, Yizhou and Wang, He},
                journal={arXiv preprint arXiv:2503.16806},
                year={2025}
              } 
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/65.gif" alt="" />
            <div class="item-right">
              <div class="item-title">
                RoboHanger: Learning Generalizable Robotic Hanger Insertion for
                Diverse Garments
              </div>
              <p>
                Yuxing Chen, Songlin Wei,Bowen Xiao, Jiangran Lyu, Jiayi Chen,
                Feng Zhu and <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">RA-L</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2412.01083">arXiv</a>
                <a href=" https://pku-epic.github.io/RoboHanger/">Project</a>
                <a
                  id="misc-robohanger"
                  href="javascript:hideshow(document.getElementById('misc-robohanger'),document.getElementById('robohanger'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="robohanger" style="font-size:14px; display: none">
              @misc{chen2025robohangerlearninggeneralizablerobotic,
                title={RoboHanger: Learning Generalizable Robotic Hanger Insertion for Diverse Garments}, 
                author={Yuxing Chen and Songlin Wei and Bowen Xiao and Jiangran Lyu and Jiayi Chen and Feng Zhu and He Wang},
                year={2025},
                eprint={2412.01083},
                archivePrefix={arXiv},
                primaryClass={cs.RO},
                url={https://arxiv.org/abs/2412.01083}, 
              } 
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/64.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp
                Taxonomy
              </div>
              <p>
                Jiayi Chen , Yubin Ke , Lin Peng,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">RSS 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2504.18829">arXiv</a>
                <a href="https://pku-epic.github.io/Dexonomy/">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/63.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Uni-NaVid: A Video-based Vision-Language-Action Model for
                Unifying Embodied Navigation Tasks
              </div>
              <p>
                Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu,
                Songlin Wei, Zhongyuan Wang,
                <b>Zhizheng Zhang<span>&#8224;</span></b
                >,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">RSS 2025</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2412.06224">arXiv</a>
                <a href="https://pku-epic.github.io/Uni-NaVid/">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/62.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot
                Deformable Manipulation with Tools
              </div>
              <p>
                Yang You, Bokui Shen, Congyue Deng, Haoran Geng, Songlin Wei, He
                Wang, Leonidas Guibas
              </p>
              <p class="decs">RA-L 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2311.02787">arXiv</a>
                <a href="https://qq456cvb.github.io/projects/donut/">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/61.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Code-as-Monitor: Constraint-aware Visual Programming for
                Reactive and Proactive Robotic Failure Detection
              </div>
              <p>
                Enshen Zhou*, Qi Su*,
                <b>Cheng Chi*<span>&#8224;</span></b> Zhizheng Zhang, Zhongyuan
                Wang, Tiejun Huang, <b>Lu Sheng<span>&#8224;</span></b
                >, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2412.04455">arXiv</a>
                <a href="https://zhoues.github.io/Code-as-Monitor/">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/60.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                MobileH2R: Learning Generalizable Human to Mobile Robot Handover
                Exclusively from Scalable and Diverse Synthetic Data
              </div>
              <p>
                Zifan Wang*, Ziqing Chen*, Junyu Chen*, Jilong Wang, Yuxin Yang,
                Yunze Liu, Xueyi Liu, He Wang,
                <b>Li Yi<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2501.04595">arXiv</a>
                <a href="https://mobile.github.io/">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/58.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                GAPartManip: A Large-scale Part-centric Dataset for
                Material-Agnostic Articulated Object Manipulation
              </div>
              <p>
                Wenbo Cui*, Chengyang Zhao* , Songlin Wei* , Jiazhao Zhang,
                Haoran Geng,Yaran Chen,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2411.18276">arXiv</a>
                <a href="https://cwb0106.github.io/GAPartManip.github.io/"
                  >Project</a
                >
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/57.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                BODex: Scalable and Efficient Robotic Dexterous Grasp Synthesis
                Using Bilevel Optimization
              </div>
              <p>
                Jiayi Chen, Yubin,
                <b> He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2412.16490">arXiv</a>
                <a href="https://pku-epic.github.io/BODex/">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/56.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                NaVid-4D: Unleashing Spatial Intelligence in Egocentric RGB-D
                Videos for Vision-and-Language Navigation
              </div>
              <p>
                Haoran Liu, Weikang Wan, Xiqian Yu, Minghan Li, Jiazhao Zhang,
                Bo Zhao, Zhibo Chen, Zhongyuan Wang, Zhizheng Zhang,
                <b> He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2025</p>
              <!-- <div class="button">
                <a href="https://arxiv.org/abs/2411.06782">arXiv</a>
                <a href="https://quadwbg.github.io/">Project</a>
              </div> -->
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/55.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                QuadWBG: Generalizable Quadrupedal Whole-Body Grasping
              </div>
              <p>
                Jilong Wang, Javokhirbek Rajabov, Chaoyi Xu, Yiming Zheng,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2411.06782">arXiv</a>
                <a href="https://quadwbg.github.io/">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <a
              href="https://oss-cn-beijing.galbot.com/online/video/watchLess.mp4"
              ><img class="lazy-load" src="./assets/59.jpg" alt=""
            /></a>
            <div class="item-right">
              <div class="item-title">
                Watch Less, Feel More: Direct Sim-to-real RL for Articulated
                Object Manipulation with Motion Adaptation and Impedance Control
              </div>
              <p>
                Tan-Dzung Do, Gireesh Nandiraju, Jilong Wang,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2502.14457">arXiv</a>
                <a href="https://watch-less-feel-more.github.io/">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/54.jpg" alt="" />
            <div class="item-right">
              <div class="item-title">
                W-ControlUDA: Weather-Controllable Diffusion-assisted
                Unsupervised Domain Adaptation for Semantic Segmentation
              </div>
              <p>
                Fengyi Shen, Li Zhou, Kagan Kucukaytekin, George Eskandar,
                Ziyuan Liu,
                <b>He Wang<span>&#8224;</span></b
                >,<b> Alois Knoll<span>&#8224;</span></b>
              </p>
              <p class="decs">RA-L</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2402.06446">arXiv</a>
                <!-- <a href="https://pku-epic.github.io/RotationLaplace">Project</a> -->
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/53.jpg" alt="" />
            <div class="item-right">
              <div class="item-title">
                Towards Robust Probabilistic Modeling on SO(3) via Rotation
                Laplace Distribution
              </div>
              <p>
                Yingda Yin*, Jiangran Lyu*, Yang Wang,
                <b>He Wang<span>&#8224;</span></b
                >,<b> Baoquan Chen<span>&#8224;</span></b>
              </p>
              <p class="decs">TPAMI</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2305.10465">arXiv</a>
                <a href="https://pku-epic.github.io/RotationLaplace">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/49.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                D3RoMa: Disparity Diffusion-based Depth Sensing for
                Material-Agnostic Robotic Manipulation
              </div>
              <p>
                Songlin Wei, Haoran Geng, Jiayi Chen, Congyue Deng, Wenbo Cui,
                Chengyang Zhao, Xiaomeng Fang, Leonidas Guibas,<b
                  >He Wang<span>&#8224;</span></b
                >
              </p>
              <p class="decs">CoRL 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2409.14365">arXiv</a>
                <a href="https://pku-epic.github.io/D3RoMa">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/50.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                DexGraspNet 2.0: Learning Generative Dexterous Grasping in
                Large-scale Synthetic Cluttered Scenes
              </div>
              <p>
                Jialiang Zhang*, Haoran Liu*, Danshi Li*, Xinqiang Yu*, Haoran
                Geng, Yufei Ding, Jiayi Chen, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CoRL 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2410.23004">arXiv</a>
                <a href="https://pku-epic.github.io/DexGraspNet2.0">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/51.jpg" alt="" />
            <div class="item-right">
              <div class="item-title">
                RAM: Retrieval-Based Affordance Transfer for Generalizable
                Zero-Shot Robotic Manipulation
              </div>
              <p>
                Yuxuan Kuang*, Junjie Ye*, Haoran Geng*, Jiageng Mao, Congyue
                Deng, Leonidas Guibas, <b>He Wang</b>, Yue Wang
              </p>
              <p class="decs">CoRL 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2407.04689">arXiv</a>
                <a href="https://yxkryptonite.github.io/RAM/">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <a
              href="https://oss-cn-beijing.galbot.com/online/video/scissorbot_1.mp4"
              ><img class="lazy-load" src="./assets/52.jpg" alt=""
            /></a>
            <div class="item-right">
              <div class="item-title">
                ScissorBot: Learning Generalizable Scissor Skill for Paper
                Cutting via Simulation, Imitation, and Sim2Real
              </div>
              <p>
                Jiangran Lyu, Yuxing Chen, Tao Du, Feng Zhu, Huiquan Liu,
                <b>Yizhou Wang<span>&#8224;</span></b
                >, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CoRL 2024</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2409.13966">arXiv</a>
                <a href="https://pku-epic.github.io/ScissorBot">Project</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/46.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Task-Oriented Dexterous Grasp Synthesis via Differentiable Grasp
                Wrench Boundary Estimator
              </div>
              <p>
                Jiayi Chen,Yuxing Chen,Jialiang Zhang,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">IROS 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2309.13586" class="arXiv"
                  >arXiv</a
                >
                <a
                  href="https://pku-epic.github.io/TaskDexGrasp/"
                  class="project"
                  >Project</a
                >
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/47.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Open6DOR: Benchmarking Open-instruction 6-DoF Object
                Rearrangement and A VLM-based Approach
              </div>
              <p>
                Yufei Ding,Haoran Geng , Chaoyi Xu ,Xiaomeng Fang,Jiazhao
                Zhang,Songlin Wei, Qiyu Dai, Zhizheng Zhang,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">IROS 2024 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://pku-epic.github.io/Open6DOR/" class="project"
                  >Project</a
                >
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/1.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                NaVid: Video-based VLM Plans the Next Step for
                Vision-and-Language Navigation
              </div>
              <p>
                Jiazhao Zhang, Kunyu Wang ,Rongtao Xu* ,Gengze Zhou ,Yicong Hong
                ,Xiaomeng Fang ,Qi Wu ,Zhizheng Zhang<span>&#8224;</span> ,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">RSS 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2402.15852" class="arXiv"
                  >arXiv</a
                >
                <a href="https://pku-epic.github.io/NaVid/" class="project"
                  >Project</a
                >
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/2.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                SAGE: Bridging Semantic and Actionable Parts for Generalizable
                Manipulation of Articulated Objects
              </div>
              <p>
                Haoran Geng*, Songlin Wei*, Congyue Deng, Bokui Shen,
                <b>He Wang<span>&#8224;</span></b
                >, Leonidas Guibas<span>&#8224;</span>
              </p>
              <p class="decs">RSS 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2312.01307">arXiv</a>
                <a href="https://geometry.stanford.edu/projects/sage/"
                  >Project</a
                >
                <a
                  id="misc-a"
                  href="javascript:hideshow(document.getElementById('misc-a'),document.getElementById('misc'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="misc" style="font-size:14px; display: none">
            @misc{geng2023sage,
              title={SAGE: Bridging Semantic and Actionable Parts for GEneralizable Articulated-Object Manipulation under Language Instructions},
              author={Haoran Geng and Songlin Wei and Congyue Deng and Bokui Shen and He Wang and Leonidas Guibas},
              year={2023},
              eprint={2312.01307},
              archivePrefix={arXiv},
              primaryClass={cs.RO} 
            }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/3.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Enhancing Generalizable 6D Pose Tracking of an In-Hand Object
                with Tactile Sensing
              </div>
              <p>
                Yun Liu*, Xiaomeng Xu*, Weihang Chen, Haocheng Yuan,
                <b>He Wang</b>, Jing Xu, Rui Chen, Li Yi<span>&#8224;</span>
              </p>
              <p class="decs">RA-L</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2210.04026" class="arXiv"
                  >arXiv</a
                >
                <a href="https://github.com/leolyliu/TEG-Track" class="project"
                  >Project</a
                >
              </div>
            </div>
          </div>
          <div class="selected-item">
            <a
              href="https://oss-cn-beijing.galbot.com/online/video/maskcluster.mp4"
              ><img class="lazy-load" src="./assets/4.png" alt=""
            /></a>
            <div class="item-right">
              <div class="item-title">
                MaskClustering: View Consensus based Mask Graph Clustering for
                Open-Vocabulary 3D Instance Segmentation
              </div>
              <p>
                Mi Yan, Jiazhao Zhang, Yan Zhu,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2401.07745" class="arXiv"
                  >arXiv</a
                >
                <a
                  href="https://pku-epic.github.io/MaskClustering/"
                  class="project"
                  >Project</a
                >
              </div>
            </div>
          </div>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/5.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Category-Level Multi-Part Multi-Joint 3D Shape Assembly
              </div>
              <p>
                Yichen Li<span>&#8224;</span> ,Kaichun Mo, Yueqi Duan ,
                <b>He Wang</b>, Jiequan Zhang, Lin Shao, Wojciech Matusik,
                Leonidas Guibas
              </p>
              <p class="decs">CVPR 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2303.06163" class="arXiv"
                  >arXiv</a
                >
                <a
                  href="https://people.csail.mit.edu/yichenl/projects/joint/"
                  class="project"
                  >Project</a
                >
                <a
                  id="Category-a"
                  href="javascript:hideshow(document.getElementById('Category-a'),document.getElementById('Category'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="Category" style="font-size:14px; display: none">
            @article{li2020impartass,
              title={Learning 3D Part Assembly from a Single Image},
              author={Li, Yichen and Mo, Kaichun and Shao, Lin and Sung, Minghyuk and Guibas, Leonidas},
              journal={European conference on computer vision (ECCV 2020)},
              year={2020}
            }
            </p>
          </pre> -->
          <div class="selected-item">
            <img class="lazy-load" src="./assets/6.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                STOPNet: Multiview-based 6-DoF Suction Detection for Transparent
                Objects on Production Lines
              </div>
              <p>
                Yuxuan Kuang*, Qin Han*, Danshi Li, Qiyu Dai, Lian Ding, Dong
                Sun, Hanlin Zhao,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2310.05717" class="arXiv"
                  >arXiv</a
                >
                <a href="https://pku-epic.github.io/STOPNet/" class="project"
                  >Project</a
                >
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/7.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning
                based on Online Grasping Pose Fusion
              </div>
              <p>
                Jiazhao Zhang*, Nandiraju Gireesh*, Jilong Wang, Xiaomeng Fang,
                Chaoyi Xu, Weiguang Chen, Liu Dai,
                <b>He Wang</b>
              </p>
              <p class="decs">ICRA 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2309.15459" class="arXiv"
                  >arXiv</a
                >
                <a href="https://pku-epic.github.io/GAMMA/" class="project"
                  >Project</a
                >
                <a
                  id="GAMMA-a"
                  href="javascript:hideshow(document.getElementById('GAMMA-a'),document.getElementById('GAMMA'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="GAMMA" style="font-size:14px; display: none">
            @misc{zhang2023gamma,
              title={GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion}, 
              author={Jiazhao Zhang and Nandiraju Gireesh and Jilong Wang and Xiaomeng Fang and Chaoyi Xu and Weiguang Chen and Liu Dai and He Wang},
              year={2023},
              eprint={2309.15459},
              archivePrefix={arXiv},
              primaryClass={cs.RO}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/8.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                ASGrasp: Generalizable Transparent Object Reconstruction and
                6-DoF Grasp Detection from RGB-D Active Stereo Camera
              </div>
              <p>
                Jun Shi, Yong A, Yixiang Jin, Dingzhe Li, Haoyu Niu, Zhezhu Jin,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2405.05648" class="arXiv"
                  >arXiv</a
                >
                <a href="https://pku-epic.github.io/ASGrasp/" class="project"
                  >Project</a
                >
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/9.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                UniDexGrasp++: Improving Dexterous Grasping Policy Learning via
                Geometry-aware Curriculum and Iterative Generalist-Specialist
                Learning
              </div>
              <p>
                Weikang Wan*, Haoran Geng*, Yun Liu, Zikang Shan, Yaodong Yang,
                Li Yi,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">
                ICCV 2023 (<b>Oral & Best Paper Finalist,</b> final reviews of
                all strong accepts)
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2304.00464">arXiv</a>
                <a href="https://pku-epic.github.io/UniDexGrasp++/">Project</a>
                <a
                  id="UniDexGrasp1-a"
                  href="javascript:hideshow(document.getElementById('UniDexGrasp1-a'),document.getElementById('UniDexGrasp++'))"
                  >bibtex
                </a>
              </div>
            </div>
          </div>
          <pre>
            <p id="UniDexGrasp++" style="font-size:14px; display: none">
            @article{wan2023unidexgrasp++,
              title={UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning},
              author={Wan, Weikang and Geng, Haoran and Liu, Yun and Shan, Zikang and Yang, Yaodong and Yi, Li and Wang, He},
              journal={arXiv preprint arXiv:2304.00464},
              year={2023} 
            }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/10.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                GAPartNet: Cross-Category Domain-Generalizable Object Perception
                and Manipulation via Generalizable and Actionable Parts
              </div>
              <p>
                Haoran Geng*, Helin Xu*, Chengyang Zhao*, Chao Xu, Li Yi, Siyuan
                Huang,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">
                CVPR 2023 (<b>Highlight,</b> , final reviews of all accepts)
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2211.05272">arXiv</a>
                <a href="https://pku-epic.github.io/GAPartNet/">Project</a>
                <a
                  id="GAPartNet-a"
                  href="javascript:hideshow(document.getElementById('GAPartNet-a'), document.getElementById('GAPartNet'))"
                  >bibtex
                </a>
              </div>
            </div>
          </div>
          <pre>
            <p id="GAPartNet" style="font-size:14px; display: none">
              @article{geng2022gapartnet,
                title={GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts},
                author={Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He},
                journal={arXiv preprint arXiv:2211.05272},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/11.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                3D-Aware Object Goal Navigation via Simultaneous Exploration and
                Identification
              </div>
              <p>
                Jiazhao Zhang*, Liu Dai*, Fanpeng Meng, Qingnan Fan, Xuelin
                Chen, Kai Xu,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2212.00338.pdf">arXiv</a>
                <a href="https://pku-epic.github.io/3D-Aware-ObjectNav/"
                  >Project</a
                >
                <a
                  id="3DNav-a"
                  href="javascript:hideshow(document.getElementById('3DNav-a'),document.getElementById('3DNav'))"
                  >bibtex
                </a>
              </div>
            </div>
          </div>
          <pre>
            <p id="3DNav" style="font-size:14px; display: none">
              @article{zhang20223d,
                title={3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification},
                author={Zhang, Jiazhao and Dai, Liu and Meng, Fanpeng and Fan, Qingnan and Chen, Xuelin and Xu, Kai and Wang, He},
                journal={arXiv preprint arXiv:2212.00338},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/12.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                UniDexGrasp: Universal Robotic Dexterous Grasping via Learning
                Diverse Proposal Generation and Goal-Conditioned Policy
              </div>
              <p>
                Yinzhen Xu*, Weikang Wan*, Jialiang Zhang*, Haoran Liu*, Zikang
                Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi
                Chen, Tengyu Liu, Li Yi,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2303.00938.pdf">arXiv</a>
                <a href="https://pku-epic.github.io/UniDexGrasp/">Project</a>
                <a
                  id="UniDexGrasp-a"
                  href="javascript:hideshow(document.getElementById('UniDexGrasp-a'),document.getElementById('UniDexGrasp'))"
                  >bibtex
                </a>
              </div>
            </div>
          </div>
          <pre>
            <p id="UniDexGrasp" style="font:18px; display: none">
              @article{xu2023unidexgrasp,
                title={UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy},
                author={Xu, Yinzhen and Wan, Weikang and Zhang, Jialiang and Liu, Haoran and Shan, Zikang and Shen, Hao and Wang, Ruicheng and Geng, Haoran and Weng, Yijia and Chen, Jiayi and others},
                journal={arXiv preprint arXiv:2303.00938},
                year={2023}
              }
            </p >
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/13.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                PartManip: Learning Cross-Category Generalizable Part
                Manipulation Policy from Point Cloud Observations
              </div>
              <p>
                Haoran Geng*, Ziming Li*, Yiran Geng, Jiayi Chen, Hao Dong,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2303.16958">arXiv</a>
                <a href="https://pku-epic.github.io/PartManip/">Project</a>
                <a
                  id="PartManip-a"
                  href="javascript:hideshow(document.getElementById('PartManip-a'), document.getElementById('PartManip'))"
                  >bibtex
                </a>
              </div>
            </div>
          </div>
          <pre>
            <p id="PartManip" style="font:18px; display: none">
              @article{geng2023partmanip,
                title={PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations},
                author={Geng, Haoran and Li, Ziming and Geng, Yiran and Chen, Jiayi and Dong, Hao and Wang, He},
                journal={arXiv preprint arXiv:2303.16958},
                year={2023}
              }
            </p >
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/14.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Delving into Discrete Normalizing Flows on SO(3) Manifold for
                Probabilistic Rotation Modeling
              </div>
              <p>
                Yulin Liu*, Haoran Liu*, Yingda Yin*, Yang Wang, Baoquan
                Chen<span>&#8224;</span>,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2304.03937.pdf">arXiv</a>
                <a
                  id="SO3NF-a"
                  href="javascript:hideshow(document.getElementById('SO3NF-a'), document.getElementById('SO3NF'))"
                  >SO3NF</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="SO3NF" style="font-size:14px; display: none">
            @article{liu2023delving,
              title={Delving into Discrete Normalizing Flows on SO (3) Manifold for Probabilistic Rotation Modeling},
              author={Liu, Yulin and Liu, Haoran and Yin, Yingda and Wang, Yang and Chen, Baoquan and Wang, He},
              journal={arXiv preprint arXiv:2304.03937},
              year={2023}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/15.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                DiGA: Distil to Generalize and then Adapt for Domain Adaptive
                Semantic Segmentation
              </div>
              <p>
                Fengyi Shen, Akhil Gurram, Ziyuan Liu,
                <b>He Wang<span>&#8224;</span></b
                >, Alois Knoll<span>&#8224;</span>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2304.02222.pdf">arXiv</a>
                <a
                  id="DiGA-a"
                  href="javascript:hideshow(document.getElementById('DiGA-a'), document.getElementById('DiGA'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="DiGA" style="font-size:14px; display: none">
              @article{shen2023diga,
                title={DiGA: Distil to Generalize and then Adapt for Domain Adaptive Semantic Segmentation},
                author={Shen, Fengyi and Gurram, Akhil and Liu, Ziyuan and Wang, He and Knoll, Alois},
                journal={arXiv preprint arXiv:2304.02222},
                year={2023}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/16.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Adaptive Zone-aware Hierarchical Planner for Vision-Language
                Navigation
              </div>
              <p>
                Chen Gao, Xingyu Peng, Mi Yan,
                <b>He Wang</b>, Lirong Yang, Haibing Ren, Hongsheng Li, Si Liu
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a class="disable">arXiv (soon)</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/17.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for
                General Objects Based on Simulation
              </div>
              <p>
                Ruicheng Wang*, Jialiang Zhang*, Jiayi Chen, Yinzhen Xu, Puhao
                Li, Tengyu Liu,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">
                ICRA 2023 (<b>Outstanding Manipulation Paper Award Finalist</b>)
              </p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2210.02697.pdf">arXiv</a>
                <a href="https://pku-epic.github.io/DexGraspNet/">Project</a>
                <a
                  id="dexgraspnet-a"
                  href="javascript:hideshow(document.getElementById('dexgraspnet-a'),document.getElementById('dexgraspnet'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="dexgraspnet" style="font-size:14px; display: none">
              @article{2210.02697,
                title = {DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation},
                author = {Ruicheng Wang and Jialiang Zhang and Jiayi Chen and Yinzhen Xu and Puhao Li and Tengyu Liu and He Wang},
                journal={arXiv preprint arXiv:2210.02697},
                year = {2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/18.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent
                and Specular Objects Using Generalizable NeRF
              </div>
              <p>
                Qiyu Dai*, Yan Zhu*, Yiran Geng, Ciyu Ruan, Jiazhao Zhang,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2023</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2210.06575.pdf">arXiv</a>
                <a href="https://pku-epic.github.io/GraspNeRF/">Project</a>
                <a
                  id="GraspNeRF-a"
                  href="javascript:hideshow(document.getElementById('GraspNeRF-a'), document.getElementById('GraspNeRF'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="GraspNeRF" style="font-size:14px; display: none">
              @article{dai2022graspnerf,
                title={GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF},
                author={Dai, Qiyu and Zhu, Yan and Geng, Yiran and Ruan, Ciyu and Zhang, Jiazhao and Wang, He},
                journal={arXiv preprint arXiv:2210.06575},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/19.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                A Laplace-inspired Distribution on SO(3) for Probabilistic
                Rotation Estimation
              </div>
              <p>
                Yingda Yin, Yang Wang,
                <b>He Wang<span>&#8224;</span></b
                >, Baoquan Chen<span>&#8224;</span>
              </p>
              <p class="decs">ICLR 2023 (<b>notable top 25%</b>)</p>
              <div class="button">
                <a href="https://openreview.net/pdf?id=Mvetq8DO05O">Paper</a>
                <a
                  id="RotLaplace-a"
                  href="javascript:hideshow(document.getElementById('RotLaplace-a'),document.getElementById('RotLaplace'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="RotLaplace" style="font-size:14px; display: none">
              @inproceedings{
                yin2023a,
                title={A Laplace-inspired Distribution on {SO}(3) for Probabilistic Rotation Estimation},
                author={Yingda Yin and Yang Wang and He Wang and Baoquan Chen},
                booktitle={The Eleventh International Conference on Learning Representations },
                year={2023},
                url={https://openreview.net/forum?id=Mvetq8DO05O}
              }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/20.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Self-Supervised Category-Level Articulated Object Pose
                Estimation with Part-Level SE(3) Equivariance
              </div>
              <p>
                Xueyi Liu, Ji Zhang, Ruizhen Hu, Haibin Huang,
                <b>He Wang</b>, Li Yi
              </p>
              <p class="decs">ICLR 2023</p>
              <div class="button">
                <a href="https://openreview.net/forum?id=20GtJ6hIaPA">Paper</a>
                <a
                  id="SE3Art-a"
                  href="javascript:hideshow(document.getElementById('SE3Art-a'),document.getElementById('SE3Art'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="SE3Art" style="font-size:14px; display: none">
              @inproceedings{
                liu2023selfsupervised,
                title={Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level {SE}(3) Equivariance},
                author={Xueyi Liu and Ji Zhang and Ruizhen Hu and Haibin Huang and He Wang and Li Yi},
                booktitle={The Eleventh International Conference on Learning Representations },
                year={2023},
                url={https://openreview.net/forum?id=20GtJ6hIaPA}
              }
            </p>
          </pre> -->
          <div class="selected-item">
            <img class="lazy-load" src="./assets/21.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Tracking and Reconstructing Hand Object Interactions from Point
                Cloud Sequences in the Wild
              </div>
              <p>
                Jiayi Chen*, Mi Yan*, Jiazhao Zhang, Yenzhen Xu, Xiaolong Li,
                Yijia Weng, Li Yi, Shuran Song,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">AAAI 2023 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2209.12009">arXiv</a>
                <a href="https://github.com/PKU-EPIC/HOTrack">Project</a>
                <a
                  id="TRHOI-a"
                  href="javascript:hideshow(document.getElementById('TRHOI-a'),document.getElementById('TRHOI'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="TRHOI" style="font-size:14px; display: none">
              @article{chen2022tracking,
                title={Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild},
                author={Chen, Jiayi and Yan, Mi and Zhang, Jiazhao and Xu, Yinzhen and Li, Xiaolong and Weng, Yijia and Yi, Li and Song, Shuran and Wang, He},
                journal={arXiv preprint arXiv:2209.12009},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/22.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                ASRO-DIO: Active Subspace Random Optimization Based Depth
                Inertial Odometry
              </div>
              <p>
                Jiazhao Zhang, Yijie Tang,
                <b>He Wang</b>, Kai Xu
              </p>
              <p class="decs">IEEE Transactions on Robotics (T-RO)</p>
              <div class="button">
                <a href="https://ieeexplore.ieee.org/document/9915552">Paper</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/23.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Domain Randomization-Enhanced Depth Simulation and Restoration
                for Perceiving and Grasping Specular and Transparent Objects
              </div>
              <p>
                Qiyu Dai*, Jiyao Zhang*, Qiwei Li, Tianhao Wu, Hao Dong, Ziyuan
                Liu, Ping Tan,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ECCV 2022</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2208.03792.pdf">arXiv</a>
                <a href="https://pku-epic.github.io/DREDS/">Project</a>
                <a href="https://github.com/PKU-EPIC/DREDS">Code</a>
                <a
                  id="dreds-a"
                  href="javascript:hideshow(document.getElementById('dreds-a'),document.getElementById('dreds'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="dreds" style="font-size:14px; display: none">
              @article{dai2022domain,
                title={Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects},
                author={Dai, Qiyu and Zhang, Jiyao and Li, Qiwei and Wu, Tianhao and Dong, Hao and Liu, Ziyuan and Tan, Ping and Wang, He},
                journal={arXiv preprint arXiv:2208.03792},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/24.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Learning Category-Level Generalizable Object Manipulation Policy
                via Generative Adversarial Self-Imitation Learning from
                Demonstrations
              </div>
              <p>
                Hao Shen*, Weikang Wan*,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <div>
                <p class="decs">
                  Robotics and Automation Letters (RA-L) and IROS 2022
                </p>
                <p style="opacity: 1; position: relative; top: -10px">
                  <b class="bold">1st prize winner</b> of
                  <a href="https://sapien.ucsd.edu/challenges/maniskill2021/">
                    SAPIEN ManiSkill Challenge 2021
                  </a>
                  (no external annotation track)
                </p>
              </div>

              <div class="button">
                <a href="https://arxiv.org/pdf/2203.02107.pdf">arXiv</a>
                <a
                  href="https://shen-hhao.github.io/Category_Level_Manipulation/"
                  >Project</a
                >
                <a
                  href="https://shen-hhao.github.io/Category_Level_Manipulation/"
                  >Code</a
                >
                <a
                  id="maniskill-a"
                  href="javascript:hideshow(document.getElementById('maniskill-a'),document.getElementById('maniskill'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="maniskill" style="font-size:14px; display: none">
              @article{shen2022learning,
                title={Learning Category-Level Generalizable Object Manipulation Policy via Generative Adversarial Self-Imitation Learning from Demonstrations},
                author={Shen, Hao and Wan, Weikang and Wang, He},
                journal={arXiv preprint arXiv:2203.02107},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/25.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                FisherMatch: Semi-Supervised Rotation Regression via
                Entropy-based Filtering
              </div>
              <p>
                Yingda Yin, Yingcheng Cai,
                <b>He Wang<span>&#8224;</span></b
                >, Baoquan Chen<span>&#8224;</span>
              </p>
              <p class="decs">CVPR 2022 (<b>Oral Presentation</b>)</p>

              <div class="button">
                <a href="https://arxiv.org/pdf/2203.15765.pdf">arXiv</a>
                <a href="https://yd-yin.github.io/FisherMatch/">Project</a>
                <a href="https://github.com/yd-yin/FisherMatch">Code</a>
                <a
                  id="fishermatch-a"
                  href="javascript:hideshow(document.getElementById('fishermatch-a'),document.getElementById('fishermatch'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="fishermatch" style="font-size:14px; display: none">
              @article{yin2022fishermatch,
                title={FisherMatch: Semi-Supervised Rotation Regression via Entropy-based Filtering},
                author={Yin, Yingda and Cai, Yingcheng and Wang, He and Chen, Baoquan},
                journal={arXiv preprint arXiv:2203.15765},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/26.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Projective Manifold Gradient Layer for Deep Rotation Regression
              </div>
              <p>
                Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen, Leonidas
                Guibas,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2022</p>

              <div class="button">
                <a href="https://arxiv.org/pdf/2110.11657.pdf">arXiv</a>
                <a href="https://jychen18.github.io/RPMG/">Project</a>
                <a href="https://github.com/jychen18/RPMG">Code</a>
                <a
                  id="rpmg-a"
                  href="javascript:hideshow(document.getElementById('rpmg-a'),document.getElementById('rpmg'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="rpmg" style="font-size:14px; display: none">
              @article{chen2021projective,
                title={Projective Manifold Gradient Layer for Deep Rotation Regression},
                author={Chen, Jiayi and Yin, Yingda and Birdal, Tolga and Chen, Baoquan and Guibas, Leonidas and Wang, He},
                journal={arXiv preprint arXiv:2110.11657},
                year={2021}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/27.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                ADeLA: Automatic Dense Labeling with Attention for Viewpoint
                Adaptation in Semantic Segmentation
              </div>
              <p>
                <!-- Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen, Leonidas
                Guibas, -->
                <b>Yanchao Yang<span>*&#8224;</span></b
                >, Hanxiang Ren*, He Wang, Bokui Shen, Qingnan Fan,
                <b>Youyi Zheng<span>&#8224;</span></b
                >, C Karen Liu, Leonidas Guibas
              </p>
              <p class="decs">CVPR 2022 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2107.14285">arXiv</a>
                <a
                  id="adela-a"
                  href="javascript:hideshow(document.getElementById('adela-a'),document.getElementById('adela'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="adela" style="font-size:14px; display: none">
              @article{yang2021adela,
                title={ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation},
                author={Yang, Yanchao and Ren, Hanxiang and Wang, He and Shen, Bokui and Fan, Qingnan and Zheng, Youyi and Liu, C Karen and Guibas, Leonidas},
                journal={arXiv preprint arXiv:2107.14285},
                year={2021}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/28.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object
                Interaction
              </div>
              <p>
                Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao
                Shen, Boqiang Liang, Zhoujie Fu,
                <b>He Wang</b>, Li Yi
              </p>
              <p class="decs">CVPR 2022</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2203.01577.pdf">arXiv</a>
                <a
                  id="HOI4D-a"
                  href="javascript:hideshow(document.getElementById('HOI4D-a'),document.getElementById('HOI4D'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="HOI4D" style="font-size:14px; display: none">
              @article{liu2022hoi4d,
                title={HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction},
                author={Liu, Yunze and Liu, Yun and Jiang, Che and Fu, Zhoujie and Lyu, Kangbo and Wan, Weikang and Shen, Hao and Liang, Boqiang and Wang, He and Yi, Li},
                journal={arXiv preprint arXiv:2203.01577},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/29.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Multi-Robot Active Mapping via Neural Bipartite Graph Matching
              </div>
              <p>
                Kai Ye*, Siyan Dong*, Qingnan Fan,
                <b>He Wang</b>, Li Yi, Fei Xia, Jue Wang, Baoquan Chen
              </p>
              <p class="decs">CVPR 2022</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2203.16319.pdf">arXiv</a>
                <a
                  id="MultiRobotMap-a"
                  href="javascript:hideshow(document.getElementById('MultiRobotMap-a'),document.getElementById('MultiRobotMap'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="MultiRobotMap" style="font-size:14px; display: none">
              @article{ye2022multi,
                title={Multi-Robot Active Mapping via Neural Bipartite Graph Matching},
                author={Ye, Kai and Dong, Siyan and Fan, Qingnan and Wang, He and Yi, Li and Xia, Fei and Wang, Jue and Chen, Baoquan},
                journal={arXiv preprint arXiv:2203.16319},
                year={2022}
              }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/30.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric
                Guidance
              </div>
              <p>
                Tianchen Zhao, Niansong Zhang, Xuefei Ning,
                <b>He Wang</b>, Li Yi, Yu Wang
              </p>
              <p class="decs">CVPR 2022</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2203.09887.pdf">arXiv</a>
                <a
                  id="CodedVTR-a"
                  href="javascript:hideshow(document.getElementById('CodedVTR-a'),document.getElementById('CodedVTR'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="CodedVTR" style="font-size:14px; display: none">
              @article{zhao2022codedvtr,
                title={CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance},
                author={Zhao, Tianchen and Zhang, Niansong and Ning, Xuefei and Wang, He and Yi, Li and Wang, Yu},
                journal={arXiv preprint arXiv:2203.09887},
                year={2022}
              }
            </p>
          </pre> -->
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/31.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Domain Adaptation on Point Clouds via Geometry-Aware Implicits
              </div>
              <p>
                Yuefan Shen*, Yanchao Yang*, Mi Yan,
                <b>He Wang</b>, Youyi Zheng, Leonidas J. Guibas
              </p>
              <p class="decs">CVPR 2022</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2112.09343">arXiv</a>
                <a
                  id="DAGAI-a"
                  href="javascript:hideshow(document.getElementById('DAGAI-a'),document.getElementById('DAGAI'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="DAGAI" style="font-size:14px; display: none">
              @article{shen2021domain,
                title={Domain Adaptation on Point Clouds via Geometry-Aware Implicits},
                author={Shen, Yuefan and Yang, Yanchao and Yan, Mi and Wang, He and Zheng, Youyi and Guibas, Leonidas},
                journal={arXiv preprint arXiv:2112.09343},
                year={2021}
              }
            </p>
          </pre> -->
          <div class="selected-item">
            <img class="lazy-load" src="./assets/32.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Leveraging SE(3) Equivariance for Self-supervised Category-Level
                Object Pose Estimation from Point Clouds
              </div>
              <p>
                Xiaolong Li, Yijia Weng, Li Yi, Leonidas J. Guibas, A. Lynn
                Abbott, Shuran Song,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">NeurIPS 21</p>
              <div class="button">
                <a href="https://openreview.net/forum?id=wGRNAqVBQT2">Paper</a>
                <a href="https://arxiv.org/pdf/2111.00190.pdf">arXiv</a>
                <a href="https://dragonlong.github.io/equi-pose/">Project</a>
                <a href="https://github.com/dragonlong/equi-pose">Code</a>
                <a
                  id="esscop-a"
                  href="javascript:hideshow(document.getElementById('esscop-a'),document.getElementById('esscop'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="esscop" style="font-size:14px; display: none">
              @article{li2021leveraging,
                title={Leveraging SE (3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds},
                author={Li, Xiaolong and Weng, Yijia and Yi, Li and Guibas, Leonidas J and Abbott, A and Song, Shuran and Wang, He},
                journal={Advances in Neural Information Processing Systems},
                volume={34},
                year={2021}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/33.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated
                Objects from Point Clouds
              </div>
              <p>
                Yijia Weng*,
                <b>He Wang<span>*&#8224;</span></b
                >, Qiang Zhou, Yuzhe Qin, Yueqi Duan, Qingnan Fan, Baoquan Chen,
                Hao Su, Leonidas J. Guibas
              </p>
              <p class="decs">ICCV 2021 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2104.03437">arXiv</a>
                <a href="https://yijiaweng.github.io/CAPTRA/">Project</a>
                <a href="https://github.com/halfsummer11/CAPTRA">Code</a>
                <a href="https://youtu.be/JFPcOHCH2O0">Video</a>
                <a
                  id="captra-a"
                  href="javascript:hideshow(document.getElementById('captra-a'),document.getElementById('captra'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="captra" style="font-size:14px; display: none">
              @article{weng2021captra,
                title={CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds},
                author={Weng, Yijia and Wang, He and Zhou, Qiang and Qin, Yuzhe and Duan, Yueqi and Fan, Qingnan and 
                        Chen, Baoquan and Su, Hao and Guibas, Leonidas J},
                journal={arXiv preprint arXiv:2104.03437},
                year={2021}
              }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/34.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Single Image 3D Shape Retrieval via Cross-Modal Instance and
                Category Contrastive Learning
              </div>
              <p>
                Mingxian Lin, Jie Yang,
                <b>He Wang</b>, Yu-Kun Lai, Rongfei Jia, Binqiang Zhao, Lin Gao
              </p>
              <p class="decs">ICCV 2021</p>
              <div class="button">
                <a
                  href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Single_Image_3D_Shape_Retrieval_via_Cross-Modal_Instance_and_Category_ICCV_2021_paper.pdf"
                  >Paper</a
                >
                <a
                  href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Lin_Single_Image_3D_ICCV_2021_supplemental.pdf"
                  >Supp.</a
                >
                <a
                  id="retrievel-a"
                  href="javascript:hideshow(document.getElementById('retrievel-a'),document.getElementById('retrievel'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="retrievel" style="font-size:14px; display: none">
              @inproceedings{lin2021single,
                title={Single Image 3D Shape Retrieval via Cross-Modal Instance and Category Contrastive Learning},
                author={Lin, Ming-Xian and Yang, Jie and Wang, He and Lai, Yu-Kun and Jia, Rongfei and Zhao, Binqiang and Gao, Lin},
                booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
                pages={11405--11415},
                year={2021}
              }
            </p>
          </pre> -->
          <div class="selected-item">
            <img class="lazy-load" src="./assets/35.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D
                Object Detection
              </div>
              <p>
                <b>He Wang<span>*</span></b
                >, Yezhen Cong*, Or Litany, Yue Gao and Leonidas J. Guibas
              </p>
              <p class="decs">ICCV 2021</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2012.04355.pdf">Paper</a>
                <a href="https://thu17cyz.github.io/3DIoUMatch/">Project</a>
                <a href="https://github.com/THU17cyz/3DIoUMatch">Code</a>
                <a href="https://youtu.be/nuARjhkQN2U">Video</a>
                <a
                  id="3dioumatch-a"
                  href="javascript:hideshow(document.getElementById('3dioumatch-a'),document.getElementById('3dioumatch'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="3dioumatch" style="font-size:14px; display: none">
              @inproceedings{wang20213dioumatch,
                title={3DIoUMatch: Leveraging iou prediction for semi-supervised 3d object detection},
                author={Wang, He and Cong, Yezhen and Litany, Or and Gao, Yue and Guibas, Leonidas J},
                booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                pages={14615--14624},
                year={2021}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/36.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                MultiBodySync: Multi-Body Segmentation and Motion Estimation via
                3D Scan Synchronization
              </div>
              <p>
                Jiahui Huang,
                <b>He Wang</b>, Tolga Birdal, Minkyuk Sung, Federica Arrigoni,
                Shi-Min Hu, and Leonidas J. Guibas
              </p>
              <p class="decs">CVPR 2021 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2101.06605.pdf">Paper</a>
                <a href="https://github.com/huangjh-pub/multibody-sync">Code</a>
                <a href="https://www.youtube.com/watch?v=BuIBXL2UNvI">Video</a>
                <a
                  id="multibodysync-a"
                  href="javascript:hideshow(document.getElementById('multibodysync-a'),document.getElementById('multibodysync'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="multibodysync" style="font-size:14px; display: none">
              @inproceedings{huang2021multibodysync,
                title={Multibodysync: Multi-body segmentation and motion estimation via 3d scan synchronization},
                author={Huang, Jiahui and Wang, He and Birdal, Tolga and Sung, Minhyuk and Arrigoni, Federica and Hu, Shi-Min and Guibas, Leonidas J},
                booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                pages={7108--7118},
                year={2021}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/37.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Robust Neural Routing Through Space Partitions for Camera
                Relocalization in Dynamic Indoor Environments
              </div>
              <p>
                Siyan Dong*, Qingnan Fan*,
                <b>He Wang</b>, Ji Shi, Li Yi, Thomas Funkhouser, Baoquan Chen,
                Leonidas J. Guibas
              </p>
              <p class="decs">CVPR 2021 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2010.05272">Paper</a>
                <a href="https://github.com/siyandong/NeuralRouting">Code</a>
                <a
                  id="neuralrouting-a"
                  href="javascript:hideshow(document.getElementById('neuralrouting-a'),document.getElementById('neuralrouting'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="neuralrouting" style="font-size:14px; display: none">
              @InProceedings{Dong_2021_CVPR,
                author = {Dong, Siyan and Fan, Qingnan and Wang, He and Shi, Ji and Yi, Li and Funkhouser, Thomas and Chen, Baoquan and Guibas, Leonidas J.},
                title = {Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments},
                booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                month = {June},
                year = {2021},
                pages = {8544-8554}
            }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/38.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Rethinking Sampling in 3D Point Cloud Generative Adversarial
                Networks
              </div>
              <p>
                <b>He Wang*</b>, Zetian Jiang*, Li Yi, Kaichun Mo, Hao Su,
                Leonidas J. Guibas
              </p>
              <p class="decs">
                CVPR 2021 Workshop on
                <i>Learning to Generate 3D Shapes and Scenes</i>
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2006.07029">Paper</a>
                <a
                  href="https://drive.google.com/file/d/1RF77Zp6cQgoU0tf33Wjthx0D6tr3IYAJ/view?usp=sharing"
                  >Poster</a
                >
                <a href="https://www.youtube.com/watch?v=Ejzj0hnKW4Y">Video</a>
                <a
                  id="rethink-a"
                  href="javascript:hideshow(document.getElementById('rethink-a'),document.getElementById('rethink'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="rethink" style="font-size:14px; display: none">
              @article{wang2020rethinking,
                title={Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks},
                author={Wang, He and Jiang, Zetian and Yi, Li and Mo, Kaichun and Su, Hao and Guibas, Leonidas J},
                journal={arXiv preprint arXiv:2006.07029},
                year={2020}
              }
            </p>
          </pre> -->
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/39.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree
                Conditions
              </div>
              <p>
                Kaichun Mo,
                <b>He Wang</b>, Li Yi, Xinchen Yan and Leonidas J.Guibas
              </p>
              <p class="decs">ECCV 2020</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2003.08624">Paper</a>
                <a href="https://cs.stanford.edu/~kaichun/pt2pc/">Project</a>
                <a href="https://github.com/daerduoCarey/pt2pc">Code&Data</a>
                <a
                  id="pt2pc-a"
                  href="javascript:hideshow(document.getElementById('pt2pc-a'),document.getElementById('pt2pc'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="pt2pc" style="font-size:14px; display: none">
              @article{mo2020pt2pc,
                title={PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions},
                author={Mo, Kaichun and Wang, He and Yan, Xinchen and Guibas, Leonidas},
                journal={arXiv preprint arXiv:2003.08624},
                year={2020}
              }
            </p>
          </pre> -->
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/40.png" alt="" />
            <div class="item-right">
              <div class="item-title">Curriculum DeepSDF</div>
              <p>
                Yueqi Duan*, Haidong Zhu*,
                <b>He Wang</b>, Li Yi, Ram Nevatia, Leonidas J. Guibas
              </p>
              <p class="decs">ECCV 2020</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2003.08593">Paper</a>
                <a href="https://github.com/haidongz-usc/Curriculum-DeepSDF"
                  >Code</a
                >
                <a
                  id="csdf-a"
                  href="javascript:hideshow(document.getElementById('csdf-a'),document.getElementById('csdf'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div> -->
          <pre>
            <p id="csdf" style="font-size:14px; display: none">
              @misc{duan2020curriculum,
                title={Curriculum DeepSDF},
                author={Yueqi Duan and Haidong Zhu and He Wang and Li Yi and Ram Nevatia and Leonidas J. Guibas},
                year={2020},
                eprint={2003.08593},
                archivePrefix={arXiv},
                primaryClass={cs.CV}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/41.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Category-level Articulated Object Pose Estimation
              </div>
              <p>
                <b>He Wang*</b>, Xiaolong Li*, Li Yi, Leonidas Guibas, A. Lynn
                Abbott, Shuran Song
              </p>
              <p class="decs">CVPR 2020 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/1912.11913">Paper</a>
                <a href="https://articulated-pose.github.io/">Project</a>
                <a href="https://github.com/dragonlong/articulated-pose"
                  >Code&Data</a
                >
                <a
                  id="ancsh-a"
                  href="javascript:hideshow(document.getElementById('ancsh-a'),document.getElementById('ancsh'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="ancsh" style="font-size:14px; display: none">
              @article{li2019category,
                title={Category-Level Articulated Object Pose Estimation},
                author={Li, Xiaolong and Wang, He and Yi, Li and Guibas, Leonidas and Abbott, A Lynn and Song, Shuran},
                journal={arXiv preprint arXiv:1912.11913},
                year={2019}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/42.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                SAPIEN: A SimulAted Part-based Interactive ENvironment
              </div>
              <p>
                Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu,
                Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan,
                <b>He Wang</b>, Li Yi, Angel X.Chang, Leonidas J. Guibas and Hao
                Su
              </p>
              <p class="decs">CVPR 2020 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2003.08515">Paper</a>
                <a href="https://sapien.ucsd.edu/">Project</a>
                <a href="https://github.com/haosulab/SAPIEN-Release"
                  >Code&Data</a
                >
                <a href="https://youtu.be/K2yOeJhJXzM">Demo</a>
                <a
                  id="sapien-a"
                  href="javascript:hideshow(document.getElementById('sapien-a'),document.getElementById('sapien'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="sapien" style="font-size:14px; display: none">
              @InProceedings{Xiang_2020_SAPIEN,
                author={Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao 
                  and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu 
                  and Wang, He and Yi, Li and Chang, Angel X. and Guibas, Leonidas J. and Su, Hao},
                title={SAPIEN: A SimulAted Part-based Interactive ENvironment},
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
                month={June},
                year={2020}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img class="lazy-load" src="./assets/43.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Normalized Object Coordinate Space for Category-Level 6D Object
                Pose and Size Estimation
              </div>
              <p>
                <b>He Wang</b>, Srinath Sridhar, Jingwei Huang, Julien Valentin,
                Shuran Song, Leonidas J. Guibas
              </p>
              <p class="decs">
                CVPR 2019 (<b>Oral Presentation</b>),
                <a href="https://mp.weixin.qq.com/s/LAVMLCsi3-XHWLbklomxkg"
                  >WAICYOP Award 2022</a
                >
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/1901.02970">Paper</a>
                <a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/"
                  >Project</a
                >
                <a href="https://github.com/hughw19/NOCS_CVPR2019">Code&Data</a>
                <a
                  id="posercnn-a"
                  href="javascript:hideshow(document.getElementById('posercnn-a'),document.getElementById('posercnn'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="posercnn" style="font-size:14px; display: none">
              @inproceedings{wang2019normalized,
                title={Normalized object coordinate space for category-level 6d object pose and size estimation},
                author={Wang, He and Sridhar, Srinath and Huang, Jingwei and Valentin, Julien and Song, Shuran and Guibas, Leonidas J},
                booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                pages={2642--2651},
                year={2019}
              }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/44.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                GSPN: Generative Shape Proposal Network for 3D Instance
                Segmentation in Point Cloud
              </div>
              <p>
                Li Yi, Wang Zhao,
                <b>He Wang</b>, Minhyuk Sung, Leonidas Guibas
              </p>
              <p class="decs">CVPR 2019</p>
              <div class="button">
                <a href="https://arxiv.org/abs/1812.03320">Paper</a>
                <a href="https://github.com/ericyi/GSPN">Code</a>
                <a
                  id="gspn-a"
                  href="javascript:hideshow(document.getElementById('gspn-a'),document.getElementById('gspn'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="gspn" style="font-size:14px; display: none">
              @article{yi2018gspn,
                title={GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud},
                author={Yi, Li and Zhao, Wang and Wang, He and Sung, Minhyuk and Guibas, Leonidas},
                journal={arXiv preprint arXiv:1812.03320},
                year={2018}
              }
            </p>
          </pre> -->
          <div class="selected-item">
            <img class="lazy-load" src="./assets/45.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Learning a Generative Model for Multi-Step Human-Object
                Interactions from Videos
              </div>
              <p>
                <b>He Wang<span>*</span></b
                >, Soeren Pirk*, Ersin Yumer, Vladimir Kim, Ozan Sener, Srinath
                Sridhar, Leonidas J. Guibas
              </p>
              <p class="decs">
                Eurographics 2019 (<b>Best Paper Honorable Mention</b>)
              </p>
              <div class="button">
                <a
                  href="http://www.pirk.info/projects/learning_interactions/index.html"
                  >Project</a
                >
                <a
                  href="https://oss-cn-beijing.galbot.com/online/activity/19_EG_FnInteract.pdf"
                  >Paper</a
                >
                <a href="https://github.com/hughw19/ActionPlotGeneration.git"
                  >Code</a
                >
                <a href="http://ai.stanford.edu/blog/generate-human-object/"
                  >Blog</a
                >
                <a href="https://www.youtube.com/watch?v=WgpPalA2RzA">Video</a>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="main-content">
        <div id="awards" class="main-news">
          <div class="title">AWARDS</div>
          <ul>
            <li>2025 World Internet Conference Leading Technology Award</li>
            <li>The 9th "Haiying Talent" program</li>
            <li>
              Fortune magazine's 2025 "China's 40 Under 40 Business Leaders
            </li>
            <li>InTech Technology Award</li>
            <li>
              MIT Technology Review's "Innovators Under 35 China" (TR35 China)
            </li>
            <li>
              2024 Peking University-China Optics Valley Award for Scientific
              and Technological Achievements Transformation
            </li>
            <li>The Honorary Scholar of Intel China Academic Talent Program</li>
            <li>ICCV 2023 Best Paper Finalist</li>
            <li>ICRA 2023 Outstanding Manipulation Paper Finalist</li>
            <li>
              2022 World Artificial Intelligence Conference Youth Outstanding
              Paper Award
            </li>
            <li>Eurographics 2019 Best Paper Honorable Mention</li>
            <li>
              1st prize winner of
              <a href="https://sapien.ucsd.edu/challenges/maniskill2021/"
                >SAPIEN ManiSkill Challenge 2021</a
              >
              (no external annotation track)
            </li>
            <!-- <li></li> -->
          </ul>
        </div>
      </div>
      <div class="main-content">
        <div id="teaching" class="main-news">
          <div class="title">TEACHING</div>
          <p>Undergraduate course:</p>
          <ul>
            <li>
              <a
                href="https://dean.pku.edu.cn/service/web/courseDetail.php?flag=1&zxjhbh=BZ2526204834020_18972"
                >Introduction to Embodied AI</a
              >, Fall 2025
            </li>
            <li>
              <a
                href="https://dean.pku.edu.cn/service/web/courseDetail.php?flag=1&zxjhbh=BZ2526204834920_15659"
                >Introduction to Computer Vision</a
              >, Fall 2025
            </li>
            <!-- <li></li> -->
          </ul>
        </div>
      </div>
      <div class="main-content">
        <div id="professional" class="main-news">
          <div class="title">PROFESSIONAL SERVICE</div>
          <ul>
            <!-- <li>
              Associate Editor:
              <a
                href="https://www.sciencedirect.com/journal/image-and-vision-computing"
                >Image and Vision Computing</a
              >
            </li> -->
            <li>Area chair (AC): CVPR, ICCV, WACV, etc.</li>
            <li>
              Vice Chair of the Embodied AI Subcommittee, Artificial
              Intelligence Standardization Technical Committee, Ministry of
              Industry and Information Technology (MIIT), China
            </li>
            <li>
              Executive Committee Member, Intelligent Robotics Technical
              Committee, China Computer Federation (CCF)
            </li>
            <li>
              Committee Member, 3D Vision Technical Committee, China Society of
              Image and Graphics (CSIG)
            </li>
            <li>
              Area Chair, Vision and Learning Young Scholars Summit (VALSE)
            </li>
            <li>
              Committee Member, Embodied AI Professional Committee, Chinese
              Association for Artificial Intelligence (CAAI)
            </li>
            <li>Beijing Academy of Artificial Intelligence (BAAI) Scholar</li>
            <!-- <li>
              Program committee/reviewer:
              <br />Conferences: CVPR, ICCV, ECCV, NeurIPS, ICLR, AAAI,
              SIGGRAPH, RSS, IROS, ICRA <br />Journals: IEEE TPAMI, IEEE RAL
            </li> -->
            <!-- <li></li> -->
          </ul>
        </div>
      </div>
      <div class="modal">
        <div class="modal-close">
          <img src="./assets/close.svg" alt="" class="close" />
        </div>
        <div class="modal-content">
          <div class="modal-content-item" id="link1">
            <a href="#news">News</a>
          </div>
          <div class="modal-content-item" id="link2">
            <a href="#publications">Publications</a>
          </div>
          <div class="modal-content-item" id="link3">
            <a href="#awards">Awards</a>
          </div>
          <div class="modal-content-item" id="link4">
            <a href="#teaching">Teaching</a>
          </div>
          <div class="modal-content-item" id="link5">
            <a href="#professional">Professional Service</a>
          </div>
          <div class="modal-content-item" id="link6">
            <a href="#opportunities">Opportunities</a>
          </div>
        </div>
        <div class="modal-footer">
          <div class="modal-footer-item footer-item-active">
            <a href="https://hughw19.github.io">Home</a>
          </div>
          <span>/</span>
          <div class="modal-footer-item">
            <a href="https://PKU-EPIC.github.io">Lab</a>
          </div>
        </div>
      </div>
    </div>
    <script
      src="https://code.jquery.com/jquery-3.7.1.js"
      integrity="sha256-eKhayi8LEQwp4NKxN+CfCh+3qOVUtJn3QNZ0TciWLP4="
      crossorigin="anonymous"
    ></script>
    <script>
      // 当前语言
      let currentLang = localStorage.getItem('lang') || 'zh';

      // -------------------------
      // 工具方法：更新语言按钮状态
      // -------------------------
      function updateLangUI(lang) {
        if (lang === 'en') {
          $('.cn').addClass('language-active');
          $('.en').removeClass('language-active');
        } else {
          $('.en').addClass('language-active');
          $('.cn').removeClass('language-active');
        }
      }

      // -------------------------
      // 工具方法：翻译渲染
      // -------------------------
      function renderI18n(data) {
        document.querySelectorAll('[data-i18n]').forEach((el) => {
          const keys = el.getAttribute('data-i18n').split('.');
          let value = data;

          for (const k of keys) {
            value = value?.[k];
          }

          if (typeof value === 'string') {
            el.innerHTML = value;
          } else {
            console.warn(`❗ i18n key 未找到: ${keys.join('.')}`);
          }
        });
      }

      // -------------------------
      // 工具方法：加载语言 JSON
      // -------------------------
      function loadLang(lang) {
        const script = document.createElement('script');
        script.src = `/i18n/${lang}.js?v=${Date.now()}`; // 防止缓存

        script.onload = () => {
          // 加载完成后，window.I18N_DATA 就是语言包
          renderI18n(window.I18N_DATA);
          resolve();
        };
        script.onerror = () => reject(`❌ 加载语言文件失败: ${lang}.js`);

        document.body.appendChild(script);
      }

      // -------------------------
      // 语言切换主逻辑
      // -------------------------
      function setLang(lang) {
        currentLang = lang;
        localStorage.setItem('lang', lang);
        updateLangUI(lang);
        loadLang(lang);
      }

      // -------------------------
      // 初始化
      // -------------------------
      document.addEventListener('DOMContentLoaded', () => {
        // 初始化按钮状态
        updateLangUI(currentLang);

        // 加载当前语言
        loadLang(currentLang);
        // 绑定按钮
        document
          .getElementById('lang-cn')
          .addEventListener('click', () => setLang('zh'));
        document
          .getElementById('lang-en')
          .addEventListener('click', () => setLang('en'));
      });
    </script>

    <script>
      document
        .getElementById('link1')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('news');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link2')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('publications');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link3')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('awards');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link4')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('teaching');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link5')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('professional');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link6')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('opportunities');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
    </script>
    <script type="text/javascript">
      function onresizeFun() {
        if (window.innerWidth <= 768) {
          let width = document.documentElement.clientWidth;
          // 假设设计稿宽度为750px
          // 假设已知根元素我们设置为100px（这里设置100方便后续我们好计算）
          // 动态设置根元素html的fontSize
          document.documentElement.style.fontSize = 100 * (width / 430) + 'px';
          $('.head-pc').css('display', 'none');
          $('.icon').css('display', 'none');
          $('.swiper-content').css('display', 'none');
          $('.head-phone').css('display', 'flex');
          $('.icon-phone').css('display', 'flex');
          $('.swiper-content-phone').css('display', 'block');
        } else {
          $('.head-pc').css('display', 'flex');
          $('.icon').css('display', 'flex');
          $('.swiper-content').css('display', 'block');
          $('.swiper-content-phone').css('display', 'none');
          $('.head-phone').css('display', 'none');
          $('.icon-phone').css('display', 'none');
        }
        $('.main').css('display', 'block');
      }

      // pc 视频自动播放
      function autoPlayVideo() {
        const videosContainer = document.getElementById('videos');
        const videos = document.querySelectorAll('video');

        // 检查视频是否在可见范围内
        function checkVisibility(video) {
          const rect = video.getBoundingClientRect();
          const containerRect = videosContainer.getBoundingClientRect();
          return (
            rect.top >= containerRect.top &&
            rect.left >= containerRect.left &&
            rect.bottom <= containerRect.bottom &&
            rect.right <= containerRect.right
          );
        }

        // 控制视频的播放和暂停
        function controlVideoPlayback() {
          videos.forEach((video) => {
            if (checkVisibility(video)) {
              video.play();
            } else {
              video.pause();
            }
          });
        }

        // 初始检查
        controlVideoPlayback();

        // 滚动事件监听
        videosContainer.addEventListener('scroll', controlVideoPlayback);
        window.addEventListener('resize', controlVideoPlayback);
      }

      onresizeFun();
      window.addEventListener('resize', onresizeFun);

      document.addEventListener('DOMContentLoaded', function () {
        // 图片懒加载
        // 图片懒加载
        //const observer = new IntersectionObserver((entries) => {
        //  entries.forEach((entry) => {
        //    if (entry.isIntersecting) {
        //      const img = entry.target;
        //      img.src = img.getAttribute('data-src');
        //      observer.unobserve(img);
        //     }
        //   });
        //  });

        //document.querySelectorAll('img.lazy-load').forEach((img) => {
        // observer.observe(img);
        // });

        // 菜单点击事件
        $('.menu-phone').click(() => {
          $('.modal').addClass('modal-active');
          $('body').addClass('noscroll');
        });
        $('.modal-close').click(() => {
          $('.modal').removeClass('modal-active');
          $('body').removeClass('noscroll');
        });
        $('.modal-content-item').click(function () {
          // Remove the 'item-active' class from all siblings
          $(this).siblings('.modal-content-item').removeClass('item-active');
          // Add the 'item-active' class to the clicked element
          $(this).addClass('item-active');
          $('.modal').removeClass('modal-active');
          $('body').removeClass('noscroll');
        });

        if (window.innerWidth > 768) {
          autoPlayVideo();
        }
      });

      function hideshow(a, which) {
        console.log(which);
        if (!document.getElementById) return;
        if (which.style.display == 'block') {
          which.style.display = 'none';
          a.style.background = 'none';
        } else {
          which.style.display = 'block';
          a.style.background = 'rgba(189,220,255,0.30)';
        }
      }
    </script>
  </body>
</html>
