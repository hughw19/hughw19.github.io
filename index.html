<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1, minimum-scale=1"
    />
    <title>He Wang</title>
    <link rel="stylesheet" type="text/css" href="./css/home11.css" />
  </head>
  <body>
    <div class="main">
      <div class="main-head">
        <div class="head-pc">
          <div class="name">He Wang</div>
          <div class="list">
            <div class="list-item"><a href="#news">News</a></div>
            <div class="list-item">
              <a href="#publications">Publications</a>
            </div>
            <div class="list-item"><a href="#awards">Awards</a></div>
            <div class="list-item"><a href="#teaching">Teaching</a></div>
            <div class="list-item">
              <a href="#professional">Professional Service</a>
            </div>
            <div class="list-item">
              <a href="#opportunities">Opportunities</a>
            </div>
          </div>

          <div class="menu">
            <div class="menu-item active">
              <a href="https://hughw19.github.io">Home</a>
            </div>
            <span>/</span>
            <div class="menu-item">
              <a href="https://PKU-EPIC.github.io">Lab</a>
            </div>
          </div>
        </div>
        <div class="head-phone">
          <div class="name">He Wang</div>
          <svg
            class="menu-phone"
            xmlns="http://www.w3.org/2000/svg"
            width="21"
            height="19"
            viewBox="0 0 21 19"
            fill="none"
          >
            <path
              fill-rule="evenodd"
              clip-rule="evenodd"
              d="M0 1.5C0 0.671573 0.671573 0 1.5 0H19.5C20.3284 0 21 0.671573 21 1.5C21 2.32843 20.3284 3 19.5 3H1.5C0.671573 3 0 2.32843 0 1.5ZM0 9.5C0 8.67157 0.671573 8 1.5 8H19.5C20.3284 8 21 8.67157 21 9.5C21 10.3284 20.3284 11 19.5 11H1.5C0.671573 11 0 10.3284 0 9.5ZM1.5 16C0.671573 16 0 16.6716 0 17.5C0 18.3284 0.671573 19 1.5 19H19.5C20.3284 19 21 18.3284 21 17.5C21 16.6716 20.3284 16 19.5 16H1.5Z"
              fill="#808080"
            />
          </svg>
        </div>
      </div>
      <div class="main-content">
        <div class="main-userinfo">
          <div class="info">
            <div>
              <img
                class="lazy-load"
                src="https://oss-cn-beijing.galbot.com/online/blog/photo1.jpg"
                alt=""
              />
              <div class="icon-phone">
                <a
                  href="https://scholar.google.com/citations?user=roCAWkoAAAAJ&hl=en"
                  ><img
                    style="width: 18px; height: 18px"
                    src="./assets/ic_1.svg"
                    alt=""
                /></a>
                <a href="mailto:hewang@pku.edu.cn"
                  ><img
                    style="width: 19px; height: 15px"
                    src="./assets/ic_4.svg"
                    alt=""
                /></a>
                <a href="https://twitter.com/HughWang19"
                  ><img
                    style="width: 19px; height: 16px"
                    src="./assets/ic_3.svg"
                    alt=""
                /></a>
              </div>
            </div>
            <div class="info-right">
              <div class="info-title">Prof. He Wang</div>
              <p>
                Tenure-track Assistant Professor at
                <a href="https://english.pku.edu.cn/">Peking University</a>
              </p>
              <p>
                Director of
                <i>Embodied Perception and InteraCtion (EPIC) Lab</i>
              </p>
              <p>Director of <i> PKU-Galbot Joint Lab of Embodied AI</i></p>
              <div class="icon">
                <a
                  href="https://scholar.google.com/citations?user=roCAWkoAAAAJ&hl=en"
                  ><img
                    style="width: 18px; height: 18px"
                    src="./assets/ic_1.svg"
                    alt=""
                /></a>
                <a href="mailto:hewang@pku.edu.cn"
                  ><img
                    style="width: 19px; height: 15px"
                    src="./assets/ic_4.svg"
                    alt=""
                /></a>
                <a href="https://twitter.com/HughWang19"
                  ><img
                    style="width: 19px; height: 16px"
                    src="./assets/ic_3.svg"
                    alt=""
                /></a>
              </div>
            </div>
          </div>
          <div class="decs">
            <p>
              I am a tenure-track assistant professor in the
              <a href="https://cfcs.pku.edu.cn/english"
                >Center on Frontiers of Computing Studies (CFCS)
              </a>
              at <a href="https://english.pku.edu.cn/">Peking University.</a> I
              founded and lead the
              <i>Embodied Perception and InteraCtion (EPIC) Lab</i>, with the
              mission of developing generalizable skills and embodied multimodal
              embodied multimodal large models for robots to facilitate embodied
              AGI.
            </p>
            <p>
              I am also the founder and CTO of Galbot, a world-leading Embodied
              AI company that focuses on developing humanoid generalist robots.
            </p>
            <!-- <p>
              I serve as an associate editor of Image and Vision Computing and
              serve as an area chair in CVPR 2022 and WACV 2022. Prior to
              joining Peking University, I received my Ph.D. degree from
              <a href="https://www.stanford.edu">Stanford University</a> in 2021
              under the advisory of Prof.
              <a href="http://geometry.stanford.edu/member/guibas/index.html"
                >Leonidas J.Guibas</a
              >
              and my Bachelor's degree from
              <a href="http://www.tsinghua.edu.cn/">Tsinghua University</a> in
              2014.
            </p> -->
          </div>
        </div>
      </div>
      <div class="main-content" style="padding-bottom: 38px">
        <div class="main-swiper" id="videos">
          <div class="swiper-content">
            <div class="slide-source">
              <a>
                <video
                  id="video02"
                  autoplay
                  loop
                  muted
                  playsinline
                  preload="none"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/grasp.jpg"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/grasp1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p02">
                  GraspVLA is the world's first end-to-end embodied grasping
                  foundation model. Its pre-training is entirely based on
                  billion-scale "vision-language-action" synthetic data.
                </div>
              </a>
            </div>
            <div class="slide-source">
              <a>
                <video
                  style="width: 100%"
                  id="video03"
                  autoplay
                  loop
                  muted
                  playsinline
                  preload="none"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/grocery.jpg"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/grocery1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p03">
                  GroceryVLA is the world's first end-to-end embodied VLA large
                  model designed for the retail industry.
                </div>
              </a>
            </div>
            <div class="slide-source">
              <a>
                <video
                  id="video00"
                  autoplay
                  loop
                  muted
                  poster="https://oss-cn-beijing.galbot.com/online/blog/navfom-1.jpg"
                  playsinline
                  preload="none"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/navfom.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p01">
                  NavFoM: The world's first cross-embodiment omnidirectional
                  panoramic navigation foundation model. Its goal is to enable
                  robots to autonomously perceive the world and decide where to
                  go and how to get there entirely on their own in completely
                  unknown environments.
                </div>
              </a>
            </div>
            <div class="slide-source">
              <a>
                <video
                  id="video00"
                  autoplay
                  loop
                  muted
                  poster="https://oss-cn-beijing.galbot.com/online/blog/dexndm.jpg"
                  playsinline
                  preload="none"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/dexndm.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p01">
                  DexNDM: A dexterous hand neural dynamics model that, for the
                  first time, enables a general-purpose dexterous hand to stably
                  rotate complex objects—whether elongated, miniature, or
                  irregularly shaped—in any orientation and along any axis.
                </div>
              </a>
            </div>
            <div class="slide-source">
              <a>
                <video
                  id="video00"
                  autoplay
                  loop
                  muted
                  poster="https://oss-cn-beijing.galbot.com/online/blog/any2track.jpg"
                  playsinline
                  preload="none"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/any2track.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p01">
                  Any2Track: a two-stage reinforcement learning framework that
                  enables a single policy to accurately track diverse, complex
                  motions while maintaining robust online adaptability to
                  real-world dynamic disturbances.
                </div>
              </a>
            </div>
            <!-- <div class="slide-source">
              <a>
                <video
                  id="video01"
                  autoplay
                  loop
                  muted
                  playsinline
                  preload="none"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/trackvla.jpg"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/trackvla2.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p01">
                  TrackVLA is a product-level navigation large model introduced
                  by Galbot, capable of pure visual environmental perception and
                  driven by natural language instructions.
                </div>
              </a>
            </div>
            <div class="slide-source">
              <a>
                <video
                  id="video02"
                  autoplay
                  loop
                  muted
                  playsinline
                  preload="none"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/grasp.jpg"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/grasp1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p02">
                  GraspVLA is the world's first end-to-end embodied grasping
                  foundation model. Its pre-training is entirely based on
                  billion-scale "vision-language-action" synthetic data.
                </div>
              </a>
            </div> -->

            <!-- <div class="slide-source">
              <a href="https://pku-epic.github.io/ASGrasp/">
                <video
                  id="video1"
                  autoplay
                  loop
                  muted
                  playsinline
                  preload="none"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.jpg"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p1">
                  Material-agnostic generalized grasping technology, capable of
                  handling scenes with completely transparent objects with high
                  success.
                </div>
              </a>
            </div> -->
            <!-- <div class="slide-source">
              <a href="  https://pku-epic.github.io/DexGraspNet/">
                <video id="video2" autoplay loop muted preload="none">
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech7-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p2">
                  《DexGraspNet》ICRA 2023 Best Manipulation Paper
                  Nominee，Million-level dexterous hands Dataset.
                </div>
              </a>
            </div>
            <script>
              var video = document.getElementById('video2');
              var container = document.getElementById('p2');
              container.style.width = video.clientWidth - 56 + 'px';
            </script> -->
            <!-- <div class="slide-source">
              <a href="https://pku-epic.github.io/Open6DOR/">
                <video id="video3" autoplay loop muted preload="none">
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech2-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div id="p3">
                  The world's first open command large model system for object
                  pick-and-place, capable of controlling object orientation:
                  Open6DOR.
                </div>
              </a>
            </div>
            <script>
              var video = document.getElementById('video3');
              var container = document.getElementById('p3');
              container.style.width = video.clientWidth - 56 + 'px';
            </script> -->
            <!-- <div class="slide-source">
              <a href="https://pku-epic.github.io/NaVid/">
                <video autoplay muted loop preload="none">
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/tech3-1.mp4"
                    type="video/mp4"
                  />
                </video>
                <div>
                  The world's first generalized embodied navigation large model:
                  NaVid.
                </div>
              </a>
            </div> -->
          </div>
          <div class="swiper-content-phone" id="videos01">
            <div class="slide-source">
              <a href="https://oss-cn-beijing.galbot.com/online/blog/grasp.mp4">
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  preload="none"
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video02"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/grasp.jpg"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/grasp.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p02">
                <a
                  >GraspVLA is the world's first end-to-end embodied grasping
                  foundation model. Its pre-training is entirely based on
                  billion-scale "vision-language-action" synthetic data.
                </a>
              </div>
            </div>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/grocery.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  preload="none"
                  muted
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video03"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/grocery.jpg"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/grocery.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p03">
                <a
                  >GroceryVLA is the world's first end-to-end embodied VLA large
                  model designed for the retail industry.
                </a>
              </div>
            </div>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/navfom_mobile.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  preload="none"
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video01"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/navfom-1.jpg"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/navfom_mobile.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p01">
                <a
                  >NavFoM: The world's first cross-embodiment omnidirectional
                  panoramic navigation foundation model. Its goal is to enable
                  robots to autonomously perceive the world and decide where to
                  go and how to get there entirely on their own in completely
                  unknown environments.
                </a>
              </div>
            </div>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/dexndm.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  preload="none"
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video01"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/dexndm.jpg"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/dexndm_mobile.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p01">
                <a
                  >DexNDM: A dexterous hand neural dynamics model that, for the
                  first time, enables a general-purpose dexterous hand to stably
                  rotate complex objects—whether elongated, miniature, or
                  irregularly shaped—in any orientation and along any axis.
                </a>
              </div>
            </div>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/any2track.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  preload="none"
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video01"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/any2track.jpg"
                >
                  <source
                    src="https://oss-cn-beijing.galbot.com/online/blog/any2track.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p01">
                <a
                  >Any2Track: a two-stage reinforcement learning framework that
                  enables a single policy to accurately track diverse, complex
                  motions while maintaining robust online adaptability to
                  real-world dynamic disturbances.
                </a>
              </div>
            </div>
            <!-- <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/trackvla.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video01"
                  preload="none"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/trackvla.jpg"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/trackvla.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p01">
                <a
                  >TrackVLA is a product-level navigation large model introduced
                  by Galbot, capable of pure visual environmental perception and
                  driven by natural language instructions.
                </a>
              </div>
            </div> -->

            <!-- <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  loop
                  muted
                  preload="none"
                  controlsList="nodownload noPictureInPicture"
                  x5-video-player-type="h5-page"
                  id="video1"
                  poster="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.jpg"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/tech6-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p1">
                <a href="https://pku-epic.github.io/ASGrasp/"
                  >Material-agnostic generalized grasping technology, capable of
                  handling scenes with completely transparent objects with high
                  success.
                </a>
              </div>
            </div> -->
            <!-- <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech7-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  id="video2"
                  x5-video-player-type="h5-page"
                  loop
                  muted
                  preload="none"
                  controlsList="nodownload noPictureInPicture"
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech7.png"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/tech7-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p2">
                <a href="  https://pku-epic.github.io/DexGraspNet/"
                  >《DexGraspNet》ICRA 2023 Best Manipulation Paper
                  Nominee，Million-level dexterous hands Dataset.
                </a>
              </div>
            </div>
            <script>
              var video = document.getElementById('video2');
              var container = document.getElementById('p2');
              container.style.width = video.clientWidth - 56 + 'px';
            </script>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech2-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />

                <video
                  id="video3"
                  loop
                  controlsList="nodownload noPictureInPicture"
                  muted
                  preload="none"
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech2.png"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/tech2-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div id="p3">
                <a href="https://pku-epic.github.io/Open6DOR/">
                  The world's first open command large model system for object
                  pick-and-place, capable of controlling object orientation:
                  Open6DOR.
                </a>
              </div>
            </div>
            <script>
              var video = document.getElementById('video3');
              var container = document.getElementById('p3');
              container.style.width = video.clientWidth - 56 + 'px';
            </script>
            <div class="slide-source">
              <a
                href="https://oss-cn-beijing.galbot.com/online/blog/tech3-1.mp4"
              >
                <img src="./assets/play.png" alt="" class="play" />
                <video
                  muted
                  controlsList="nodownload noPictureInPicture"
                  loop
                  preload="none"
                  poster="https://oss-cn-beijing.galbot.com/online/portal/video/tech3.png"
                >
                  <source
                    data-src="https://oss-cn-beijing.galbot.com/online/blog/tech3-1.mp4"
                    type="video/mp4"
                  />
                </video>
              </a>
              <div>
                <a href="https://pku-epic.github.io/NaVid/">
                  The world's first generalized embodied navigation large model:
                  NaVid.
                </a>
              </div>
            </div> -->
          </div>
        </div>
      </div>
      <div class="main-content">
        <div id="opportunities" class="main-news" style="padding-bottom: 50px">
          <div class="title">
            <span>OPPORTUNITIES</span>
            <img
              id="lang-cn"
              class="opportunities-language cn"
              src="./assets/cn.png"
              alt=""
            />
            <img
              id="lang-en"
              class="opportunities-language en"
              src="./assets/en.png"
              alt=""
            />
          </div>
          <div style="padding-bottom: 18px; border-bottom: 1px dashed #d8d8d8">
            <div class="sub-title" data-i18n="opportunities.subTitle1"></div>
            <p data-i18n="opportunities.tips"></p>
            <ul style="padding-bottom: 0px">
              <li>
                <b data-i18n="opportunities.li1"></b>
              </li>
              <li>
                <b data-i18n="opportunities.li2"></b>
              </li>
              <li>
                <b data-i18n="opportunities.li3"></b>
              </li>
              <li>
                <b data-i18n="opportunities.li4"></b>
              </li>
            </ul>
            <p data-i18n="opportunities.p1"></p>
            <p data-i18n="opportunities.p2"></p>
          </div>
          <div style="padding-bottom: 18px; border-bottom: 1px dashed #d8d8d8">
            <div class="sub-title" data-i18n="opportunities.subTitle2"></div>
            <p data-i18n="opportunities.p3"></p>
            <p data-i18n="opportunities.p4"></p>
            <p data-i18n="opportunities.p5"></p>
          </div>
          <div class="sub-title" data-i18n="opportunities.subTitle3"></div>
          <p data-i18n="opportunities.p6"></p>
          <ul style="padding-bottom: 0px">
            <li data-i18n="opportunities.li5"></li>
            <li data-i18n="opportunities.li6"></li>
          </ul>
          <p data-i18n="opportunities.p7"></p>
        </div>
      </div>
      <div class="main-content">
        <div id="news" class="main-news">
          <div class="title">NEWS</div>
          <ul>
            <li>Three papers get accepted to ICLR 2026.</li>
            <li>One paper gets accepted to RA-L.</li>
            <li>
              I am invited to be a speaker in
              <a href="https://humanoidssummit.com/agenda" target="_blank"
                >Humanoids Summit 2025.</a
              >
            </li>
            <li>
              Our project, the "Synthetic and real data-driven VLA embodied
              intelligence large model" was honored with the
              <a
                href="https://www.cac.gov.cn/2025-11/06/c_1764156715189004.htm"
                target="_blank"
                >2025 World Internet Conference Leading Technology Award.</a
              >
            </li>
            <li>One paper gets accepted to RA-L.</li>
            <li>
              I have been selected for Fortune magazine's 2025 "China's 40 Under
              40 Business Leaders" list.
            </li>
            <!-- <li>I have been selected for the 9th "Haiying Talent" program.</li> -->
            <li>
              Two papers get accepted to NeurIPS, with one selected as a
              spotlight.
            </li>
            <li>
              I am invited to be a speaker in
              <a
                href="https://iccv.thecvf.com/Conferences/2025/ACWorkshop"
                target="_blank"
                >Area Chair Workshop</a
              >
              at ICCV 2025.
            </li>
            <li>
              I am invited to be a speaker in the 3rd edition of the TRICKY
              workshop(Transparent & Reflective Objects in the Wild Challenges)
              at ICCV 2025.
            </li>
            <!-- <li>
              I am invited to be a speaker in
              <a href="https://wclop.github.io" target="_blank"
                >the 1st Workshop and Challenge on Category-Level Object Pose
                Estimation in the Wild</a
              >
              at ICCV 2025.
            </li> -->
            <li>
              I am invited to be a speaker in
              <a href="https://heai-iros25-workshop.github.io" target="_blank"
                >the Workshop on Human-aware Embodied AI</a
              >
              at IROS 2025.
            </li>
            <!-- <li>...</li> -->
          </ul>
        </div>
      </div>
      <div class="main-content">
        <div id="publications" class="main-selected">
          <div class="title">Preprint</div>
          <pre>
            <p id="DexNDMbibtex" style="font-size:14px; display: none">
                @article{2025DexNDM,
                  title={DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model},
                  author={ Liu, Xueyi  and  Wang, He  and  Yi, Li },
                  year={2025},
                }
            </p>
          </pre>
          <div class="selected-item">
            <img
              src="https://oss-cn-beijing.galbot.com/online/blog/74.jpg"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                StereoVLA: Enhancing Vision-Language-Action Models with Stereo
                Vision
              </div>
              <p class="decs">
                Shengliang Deng*, Mi Yan*, Yixin Zheng*, Jiayi Su, WenHao Zhang,
                Xiaoguang Zhao, Heming Cui, Zhizheng Zhang<span>&#8224;</span>,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2512.21970">arXiv</a>
                <a href="https://shengliangd.github.io/StereoVLA-Webpage/"
                  >Project</a
                >
                <a
                  id="StereoVLAID"
                  href="javascript:hideshow(document.getElementById('StereoVLAID'),document.getElementById('StereoVLAbibtex'));"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="StereoVLAbibtex" style="font-size:14px; display: none">
                @article{2025StereoVLA,
                  title={StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision},
                  author={ Deng, Shengliang  and  Yan, Mi  and  Zheng, Yixin  and  Su, Jiayi  and  Zhang, Wenhao  and  Zhao, Xiaoguang  and  Cui, Heming  and  Zhang, Zhizheng  and  Wang, He },
                  year={2025},
                }
            </p>
          </pre>
          <div class="selected-item">
            <img
              src="https://oss-cn-beijing.galbot.com/online/blog/76.jpg"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Track Any Motions under Any Disturbances
              </div>
              <p class="decs">
                Zhikai Zhang*, Jun Guo*, Chao Chen, Jilong Wang, Chenghuai Lin,
                Yunrui Lian, HanXue, Zhenrong Wang, Maoqi Liu, Jiangran Lyu,
                Huaping Liu, <b>He Wang</b>, Li Yi<span>&#8224;</span>
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2509.13833">arXiv</a>
                <a href="https://zzk273.github.io/Any2Track/">Project</a>
                <a
                  id="5trackmotionsdisturbancesID"
                  href="javascript:hideshow(document.getElementById('5trackmotionsdisturbancesID'),document.getElementById('5trackmotionsdisturbancesbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="5trackmotionsdisturbancesbibtex" style="font-size:14px; display: none;max-width: 1400px;">
            @misc{zhang2025trackmotionsdisturbances,
              title={Track Any Motions under Any Disturbances}, 
              author={Zhikai Zhang and Jun Guo and Chao Chen and Jilong Wang and Chenghuai Lin and Yunrui Lian and
              Han Xue and Zhenrong Wang and Maoqi Liu and Jiangran Lyu and Huaping Liu and He Wang and Li Yi},
              year={2025},
              eprint={2509.13833},
              archivePrefix={arXiv},
              primaryClass={cs.RO},
              url={https://arxiv.org/abs/2509.13833}, 
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              src="https://oss-cn-beijing.galbot.com/online/blog/trackvla.gif"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA
                Models for Embodied Visual Tracking
              </div>
              <p class="decs">
                Jiahang Liu*, Yunpeng Qi*, Jiazhao Zhang*, Minghan Li, Shaoan
                Wang, Kui Wu, Hanjing Ye, Hong Zhang, Zhibo Chen, Fangwei Zhong,
                Zhizheng Zhang<span>&#8224;</span>,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2510.07134">arXiv</a>
                <a href="https://pku-epic.github.io/TrackVLA-plus-plus-Web"
                  >Project</a
                >
                <a
                  id="TrackVLA++ID"
                  href="javascript:hideshow(document.getElementById('TrackVLA++ID'),document.getElementById('TrackVLA++bibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="TrackVLA++bibtex" style="font-size:14px; display: none;max-width: 1400px;">
            @article{2025TrackVLA,
              title={TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking},
              author={ Liu, Jiahang  and  Qi, Yunpeng  and  Zhang, Jiazhao  and  Li, Minghan  and  Wang, Shaoan  and  
              Wu, Kui  and  Ye, Hanjing  and  Zhang, Hong  and  Chen, Zhibo  and  Zhong, Fangwei },
              year={2025},
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              src="https://oss-cn-beijing.galbot.com/online/blog/urbanvla.gif"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                UrbanVLA: A Vision-Language-Action Model for Urban Micromobility
              </div>
              <p class="decs">
                Anqi Li*, Zhiyong Wang*, Jiazhao Zhang*, Minghan Li, Zhibo Chen,
                Zhizheng Zhang<span>&#8224;</span>,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2510.23576">arXiv</a>
                <a href="https://pku-epic.github.io/UrbanVLA-Web">Project</a>
                <a
                  id="UrbanVLAID"
                  href="javascript:hideshow(document.getElementById('UrbanVLAID'),document.getElementById('UrbanVLAIDbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="UrbanVLAIDbibtex" style="font-size:14px; display: none;max-width: 1400px;">
            @article{li2025urbanvla,
              title={UrbanVLA: A Vision-Language-Action Model for Urban Micromobility},
              author={Li, Anqi and Wang, Zhiyong and Zhang, Jiazhao and Li, Minghan and Qi, Yunpeng and Chen, Zhibo and Zhang, Zhizheng and Wang, He},
              journal={arXiv preprint arXiv:2510.23576},
              year={2025}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              src="https://oss-cn-beijing.galbot.com/online/blog/mm_nav.gif"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                MM-Nav: Multi-View VLA Model for Robust Visual Navigation via
                Multi-Expert Learning
              </div>
              <p class="decs">
                Tianyu Xu*, Jiawei Chen*, Jiazhao Zhang*, Wenyao Zhang, Zekun
                Qi, Minghan Li, Zhizheng Zhang<span>&#8224;</span>,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2510.03142">arXiv</a>
                <a href="https://pku-epic.github.io/MM-Nav-Web">Project</a>
                <a
                  id="MM-NavID"
                  href="javascript:hideshow(document.getElementById('MM-NavID'),document.getElementById('MM-NavIDbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="MM-NavIDbibtex" style="font-size:14px; display: none;max-width: 1400px;">
            @article{xu2025mm,
              title={MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning},
              author={Xu, Tianyu and Chen, Jiawei and Zhang, Jiazhao and Zhang, Wenyao and Qi, Zekun and Li, Minghan and Zhang, Zhizheng and Wang, He},
              journal={arXiv preprint arXiv:2510.03142},
              year={2025}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/73.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Robust Differentiable Collision Detection for General Objects
              </div>
              <p class="decs">
                Jiayi Chen, Wei Zhao, Liangwang Ruan, Baoquan Chen,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2511.06267">arXiv</a>
                <a href="https://github.com/JYChen18/DiffCollision">Project</a>
                <a
                  id="chen2025robustID"
                  href="javascript:hideshow(document.getElementById('chen2025robustID'),document.getElementById('chen2025robust'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="chen2025robust" style="font-size:14px; display: none">
                @article{chen2025robust,
                  title={Robust Differentiable Collision Detection for General Objects},
                  author={Chen, Jiayi and Zhao, Wei and Ruan, Liangwang and Chen, Baoquan and Wang, He},
                  journal={arXiv preprint arXiv:2511.06267},
                  year={2025}
                }
            </p>
          </pre>
        </div>
      </div>
      <div class="main-content">
        <div class="main-selected">
          <div class="title">SELECTED PUBLICATIONS</div>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/72.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                DreamVLA: A Vision-Language-Action Model Dreamed with
                Comprehensive World Knowledge
              </div>
              <p>
                Wenyao Zhang*, Hongsi Liu*, Zekun Qi*, Yunnan Wang*, XinQiang
                Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, He Wang, Zhizheng
                Zhang, Li Yi, Wenjun Zeng, <b>Xin Jin<span>&#8224;</span></b>
              </p>
              <p class="decs">NeurIPS 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2507.04447">arXiv</a>
                <a href="https://zhangwenyao1.github.io/DreamVLA/">Project</a>
              </div>
            </div>
          </div> -->
          <div class="selected-item">
            <img
              src="https://oss-cn-beijing.galbot.com/online/blog/omni_spatial.jpg"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark
                for Vision Language Models
              </div>
              <p>
                Mengdi Jia*, Zekun Qi*, Shaochen Zhang, Wenyao Zhang, Xinqiang
                Yu, Jiawei He, <b>He Wang<span>&#8224;</span></b
                >, Li Yi<span>&#8224;</span>
              </p>
              <p class="decs">ICLR 2026</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2506.03135">arXiv</a>
                <a href="https://qizekun.github.io/omnispatial/">Project</a>
                <a
                  id="OmniSpatialID"
                  href="javascript:hideshow(document.getElementById('OmniSpatialID'),document.getElementById('OmniSpatialbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="OmniSpatialbibtex" style="font-size:14px; display: none">
            @article{omnispatial25,
              title={OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models},
              author={Jia, Mengdi and Qi, Zekun and Zhang, Shaochen and Zhang, Wenyao and Yu, Xinqiang and He, Jiawei and Wang, He and Yi, Li},
              journal={arXiv preprint arXiv:2506.03135},
              year={2025}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              src="https://oss-cn-beijing.galbot.com/online/blog/navfom.gif"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">Embodied Navigation Foundation Model</div>
              <p>
                Jiazhao Zhang*, Anqi Li*, Yunpeng Qi*, Minghan Li*, Jiahang Liu,
                Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li,
                Yuxin Fan, Wenjun Li, Zhibo Chen, Fei Gao, Qi Wu, Zhizheng
                Zhang<span>&#8224;</span>, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICLR 2026</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2509.12129">arXiv</a>
                <a href="https://pku-epic.github.io/NavFoM-Web">Project</a>
                <a
                  id="EmbodiedID"
                  href="javascript:hideshow(document.getElementById('EmbodiedID'),document.getElementById('article-Embodied'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-Embodied" style="font-size:14px; display: none">
            @article{zhang2025embodied,
              title={Embodied navigation foundation model},
              author={Zhang, Jiazhao and Li, Anqi and Qi, Yunpeng and Li, Minghan and Liu, Jiahang and 
              Wang, Shaoan and Liu, Haoran and Zhou, Gengze and Wu, Yuze and Li, Xingxing and others},
              journal={arXiv preprint arXiv:2509.12129},
              year={2025}
            }
            </p>
          </pre>
          <div class="selected-item">
            <a href="https://oss-cn-beijing.galbot.com/online/blog/DexNDM.mp4">
              <img class="play" src="./assets/play1.png" alt="" />
              <img
                class="lazy-load"
                src="https://oss-cn-beijing.galbot.com/online/blog/DexNDM.jpg"
                alt=""
              />
            </a>
            <div class="item-right">
              <div class="item-title">
                DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation
                via Joint-Wise Neural Dynamics Model
              </div>
              <p>Xueyi Liu, <b>He Wang</b>, Li Yi†</p>
              <p class="decs">ICLR 2026</p>

              <div class="button">
                <a href="https://arxiv.org/pdf/2510.08556">arXiv</a>
                <a href="https://meowuu7.github.io/DexNDM/">Project</a>
                <a
                  id="2025DexNDMID"
                  href="javascript:hideshow(document.getElementById('2025DexNDMID'),document.getElementById('article-2025DexNDM'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-2025DexNDM" style="font-size:14px; display: none">
            @article{2025DexNDM,
              title={DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model},
              author={ Liu, Xueyi  and  Wang, He  and  Yi, Li },
              year={2025},
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              src="https://oss-cn-beijing.galbot.com/online/blog/75.jpg"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                FoldNet: Learning Generalizable Closed-Loop Policy for Garment
                Folding via Keypoint-Driven Asset and Demonstration Synthesis
              </div>
              <p>Yuxing Chen*, Bowen Xiao*, <b>He Wang†</b></p>
              <p class="decs">RA-L</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2505.09109">arXiv</a>
                <a href="https://pku-epic.github.io/FoldNet/">Project</a>
                <a
                  id="FoldNetID"
                  href="javascript:hideshow(document.getElementById('FoldNetID'),document.getElementById('article-FoldNet'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-FoldNet" style="font-size:14px; display: none">
            @misc{chen2025foldnetlearninggeneralizableclosedloop,
              author={Chen, Yuxing and Xiao, Bowen and Wang, He},
              journal={IEEE Robotics and Automation Letters}, 
              title={FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis}, 
              year={2026},
              volume={},
              number={},
              pages={1-8},
              keywords={Clothing;Geometry;Imitation learning;Annotations;Trajectory;Training;Synthetic data;Pipelines;Grasping;Filtering;
              Bimanual manipulation;deep learning for visual perception;deep learning in grasping and manipulation},
              doi={10.1109/LRA.2026.3656770}}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/72.jpg"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Unleashing Humanoid Reaching Potential via Real-world-Ready
                Skill Space
              </div>
              <p>
                Zhikai Zhang*, Chao Chen*, Han Xue*, Jilong Wang, Sikai Liang,
                Yun Liu, Zongzhang Zhang, <b>He Wang</b>, Li Yi<span
                  >&#8224;</span
                >
              </p>
              <p class="decs">RA-L</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2507.04447">arXiv</a>
                <a href="https://zzk273.github.io/R2S2">Project</a>
                <a
                  id="UnleashingID"
                  href="javascript:hideshow(document.getElementById('UnleashingID'),document.getElementById('article-Unleashing'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-Unleashing" style="font-size:14px; display: none">
            @article{DBLP:journals/corr/abs-2505-10918,
              author= {Zhikai Zhang and
                              Chao Chen and
                              Han Xue and
                              Jilong Wang and
                              Sikai Liang and
                              Yun Liu and
                              Zongzhang Zhang and
                              He Wang and
                              Li Yi},
              title= {Unleashing Humanoid Reaching Potential via Real-world-Ready Skill
                              Space},
              journal= {CoRR},
              volume= {abs/2505.10918},
              year= {2025},
              url= {https://doi.org/10.48550/arXiv.2505.10918},
              doi= {10.48550/ARXIV.2505.10918},
              eprinttype= {arXiv},
              eprint= {2505.10918},
              timestamp= {Sun, 29 Jun 2025 10:28:00 +0200},
              biburl= {https://dblp.org/rec/journals/corr/abs-2505-10918.bib},
              bibsource= {dblp computer science bibliography, https://dblp.org}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/71.jpg"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                SoFar: Language-Grounded Orientation Bridges Spatial Reasoning
                and Object Manipulation
              </div>
              <p>
                Zekun Qi*, Wenyao Zhang*, Yufei Ding*, Runpei Dong, XinQiang Yu,
                Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, Jiazhao
                Zhang, Jiawei He, Jiayuan Gu, Xin Jin, Kaisheng Ma, Zhizheng
                Zhang<span>&#8224;</span>, <b>He Wang<span>&#8224;</span></b
                >, Li Yi<span>&#8224;</span>
              </p>
              <p class="decs">NeurIPS 2025 (<b>spotlight</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2502.13143">arXiv</a>
                <a href="https://qizekun.github.io/sofar/">Project</a>
                <a
                  id="sofar25"
                  href="javascript:hideshow(document.getElementById('sofar25'),document.getElementById('article-sofar25'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-sofar25" style="font-size:14px; display: none">
            bibtex: @article{sofar25,
              title={Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation},
              author={Qi, Zekun and Zhang, Wenyao and Ding, Yufei and Dong, Runpei and Yu, Xinqiang and Li, Jingwen and Xu, Lingyun and Li, Baoyu and He, Xialin and Fan, Guofan and others},
              journal={arXiv preprint arXiv:2502.13143},
              year={2025}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/70.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Advancing general robotic manipulation with multimodal
                foundation models: Anembodied Al paradigm
              </div>
              <p>
                Shifeng Huang, <b>He Wang</b>, Xing Zhou, Wenkai Chen, Haibin
                Yang, Jianwei Zhang†
              </p>
              <p class="decs">SCIENCE CHINA Technological Sciences</p>
              <div class="button">
                <a
                  href="https://link.springer.com/article/10.1007/s11431-024-2910-3#citeas"
                  >arXiv</a
                >
                <a
                  id="AdvancingID"
                  href="javascript:hideshow(document.getElementById('AdvancingID'),document.getElementById('article-AdvancingID'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-AdvancingID" style="font-size:14px; display: none">
            @article{2025Advancing,
            title={Advancing general robotic manipulation with multimodal foundation models: An embodied AI paradigm},
            author={ Huang, Shifeng  and  Wang, He  and  Zhou, Xing  and  Chen, Wenkai  and  Yang, Haibin  and  Zhang, Jianwei },
            journal={Science China Technological Sciences},
            volume={68},
            number={5},
            year={2025},
          }
            </p>
          </pre>
          <div class="selected-item">
            <a
              href="https://oss-cn-beijing.galbot.com/online/blog/trackvla1.mp4"
            >
              <img class="play" src="./assets/play1.png" alt="" />
              <img
                class="lazy-load"
                src="https://oss-cn-beijing.galbot.com/online/blog/trackvla1.png"
                alt=""
              />
            </a>
            <div class="item-right">
              <div class="item-title">
                TrackVLA: Embodied Visual Tracking in the Wild
              </div>
              <p>
                Shaoan Wang*, Jiazhao Zhang*, Minghan Li, Jiahang Liu, Anqi Li,
                Kui Wu, Fangwei Zhong, Junzhi Yu, Zhizheng
                Zhang<span>&#8224;</span>,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CoRL 2025</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2505.23189">arXiv</a>
                <a href="https://pku-epic.github.io/TrackVLA-web/">Project</a>
                <a
                  id="TrackVLAID"
                  href="javascript:hideshow(document.getElementById('TrackVLAID'),document.getElementById('article-TrackVLAID'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-TrackVLAID" style="font-size:14px; display: none">
            @article{wang2025trackvla,
              author  = {Wang, Shaoan and Zhang, Jiazhao and Li, Minghan and Liu, Jiahang and Li, Anqi and Wu, Kui and Zhong, Fangwei and Yu, Junzhi and Zhang, Zhizheng and Wang, He},
              title   = {TrackVLA: Embodied Visual Tracking in the Wild},
              journal = {arXiv pre-print},
              year    = {2025},
              url     = {http://arxiv.org/abs/2505.23189}
          }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/69.gif"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                FetchBot: Learning Generalizable Object Fetching in Cluttered
                Scenes via Zero-Shot Sim2Real
              </div>
              <p>
                Weiheng Liu*, Yuxuan Wan*, Jilong Wang, Yuxuan Kuang, Wenbo Cui,
                Xuesong Shi, Haoran Li, Dongbin Zhao, Zhizheng
                Zhang<span>&#8224;</span>, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CoRL 2025（<b>Oral</b>）</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2502.17894">arXiv</a>
                <a href="https://pku-epic.github.io/FetchBot">Project</a>
                <a
                  id="fetchbot"
                  href="javascript:hideshow(document.getElementById('fetchbot'),document.getElementById('article-fetchbot'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-fetchbot" style="font-size:14px; display: none">
            @inproceedings{liufetchbot,
              title={FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real},
              author={Liu, Weiheng and Wan, Yuxuan and Wang, Jilong and Kuang, Yuxuan and Shi, Xuesong and Li, Haoran and Zhao, Dongbin and Zhang, Zhizheng and Wang, He},
              booktitle={9th Annual Conference on Robot Learning}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/68.jpg"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                GraspVLA: a Grasping Foundation Model Pre-trained on
                Billion-scale Synthetic Action Data
              </div>
              <p>
                Shengliang Deng*, Mi Yan*, Songlin Wei, Haixin Ma, Yuxin Yang,
                Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui,
                Zhizheng Zhang†, <b>He Wang†</b>
              </p>
              <p class="decs">CoRL 2025</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2505.03233">arXiv</a>
                <a href="https://pku-epic.github.io/GraspVLA-web">Project</a>
                <a
                  id="GraspVLA"
                  href="javascript:hideshow(document.getElementById('GraspVLA'),document.getElementById('article-grasp-vla'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="article-grasp-vla" style="font-size:14px; display: none">
            @article{deng2025graspvla,
                title={GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data}, 
                author={Shengliang Deng and Mi Yan and Songlin Wei and Haixin Ma and Yuxin Yang and Jiayi Chen and Zhiqi Zhang and Taoyu Yang and Xuheng Zhang and Wenhao Zhang and Heming Cui and Zhizheng Zhang and He Wang},
                year={2025},
                eprint={2505.03233},
                archivePrefix={arXiv},
                primaryClass={cs.RO},
                url={https://arxiv.org/abs/2505.03233}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/67.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                DexVLG: Dexterous Vision-Language-Grasp Model at Scale
              </div>
              <p>
                Jiawei He*, Danshi Li*, Xinqiang Yu*, Zekun Qi, Wenyao Zhang,
                Jiayi Chen, Zhaoxiang Zhang<span>&#8224;</span>, Zhizheng
                Zhang<span>&#8224;</span>, Li Yi<span>&#8224;</span>,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICCV 2025（<b>highlight</b>）</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2507.02747">arXiv</a>
                <a href="https://jiaweihe.com/dexvlg">Project</a>
                <a
                  id="misc-dexvlg"
                  href="javascript:hideshow(document.getElementById('misc-dexvlg'),document.getElementById('dexvlg'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="dexvlg" style="font-size:14px; display: none">
              @misc{he2025dexvlgdexterousvisionlanguagegraspmodel,
                title={DexVLG: Dexterous Vision-Language-Grasp Model at Scale}, 
                author={Jiawei He and Danshi Li and Xinqiang Yu and Zekun Qi and Wenyao Zhang and Jiayi Chen and Zhaoxiang Zhang and Zhizheng Zhang and Li Yi and He Wang},
                year={2025},
                eprint={2507.02747},
                archivePrefix={arXiv},
                primaryClass={cs.CV},
                url={https://arxiv.org/abs/2507.02747}, 
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/66.gif"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                DyWA: Dynamics-adaptive World Action Model for Generalizable
                Non-prehensile Manipulation
              </div>
              <p>
                Jiangran Lyu, Ziming Li, Xuesong Shi, Chaoyi Xu, Yizhou Wang†,
                <b>He Wang†</b>
              </p>
              <p class="decs">ICCV 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2503.16806">arXiv</a>
                <a href="https://pku-epic.github.io/DyWA">Project</a>
                <a
                  id="article-dywa"
                  href="javascript:hideshow(document.getElementById('article-dywa'),document.getElementById('dywa'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="dywa" style="font-size:14px; display: none">
              @article{lyu2025dywa,
                title={DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation},
                author={Lyu, Jiangran and Li, Ziming and Shi, Xuesong and Xu, Chaoyi and Wang, Yizhou and Wang, He},
                journal={arXiv preprint arXiv:2503.16806},
                year={2025}
              } 
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/65.gif"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                RoboHanger: Learning Generalizable Robotic Hanger Insertion for
                Diverse Garments
              </div>
              <p>
                Yuxing Chen, Songlin Wei, Bowen Xiao, Jiangran Lyu, Jiayi Chen,
                Feng Zhu, <b>He Wang†</b>
              </p>
              <p class="decs">RA-L</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2412.01083">arXiv</a>
                <a href=" https://pku-epic.github.io/RoboHanger/">Project</a>
                <a
                  id="misc-robohanger"
                  href="javascript:hideshow(document.getElementById('misc-robohanger'),document.getElementById('robohanger'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="robohanger" style="font-size:14px; display: none">
              @misc{chen2025robohangerlearninggeneralizablerobotic,
                title={RoboHanger: Learning Generalizable Robotic Hanger Insertion for Diverse Garments}, 
                author={Yuxing Chen and Songlin Wei and Bowen Xiao and Jiangran Lyu and Jiayi Chen and Feng Zhu and He Wang},
                year={2025},
                eprint={2412.01083},
                archivePrefix={arXiv},
                primaryClass={cs.RO},
                url={https://arxiv.org/abs/2412.01083}, 
              } 
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/64.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp
                Taxonomy
              </div>
              <p>Jiayi Chen*, Yubin Ke*, Lin Peng, <b>He Wang†</b></p>
              <p class="decs">RSS 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2504.18829">arXiv</a>
                <a href="https://pku-epic.github.io/Dexonomy/">Project</a>
                <a
                  id="DexonomyID"
                  href="javascript:hideshow(document.getElementById('DexonomyID'),document.getElementById('Dexonomybibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="Dexonomybibtex" style="font-size:14px; display: none">
              @article{chen2025dexonomy,
                title={Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy},
                author={Chen, Jiayi and Ke, Yubin and Peng, Lin and Wang, He},
                journal={Robotics: Science and Systems},
                year={2025}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/63.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Uni-NaVid: A Video-based Vision-Language-Action Model for
                Unifying Embodied Navigation Tasks
              </div>
              <p>
                Jiazhao Zhang, Kunyu Wang, Shaoan Wang, Minghan Li, Haoran Liu,
                Songlin Wei, Zhongyuan Wang, Zhizheng Zhang†, <b>He Wang†</b>
              </p>
              <p class="decs">RSS 2025</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2412.06224">arXiv</a>
                <a href="https://pku-epic.github.io/Uni-NaVid/">Project</a>
                <a
                  id="Uni-NaVidID"
                  href="javascript:hideshow(document.getElementById('Uni-NaVidID'),document.getElementById('Uni-NaVidbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="Uni-NaVidbibtex" style="font-size:14px; display: none">
              @misc{zhang2024uninavid,
                title={Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks}, 
                author={Jiazhao Zhang and Kunyu Wang and Shaoan Wang and Minghan Li and Haoran Liu and Songlin Wei and Zhongyuan Wang and Zhizheng Zhang and He Wang},
                year={2024},
                journal = {arXiv preprint arXiv:2412.06224}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/62.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot
                Deformable Manipulation with Tools
              </div>
              <p>
                Yang You, Bokui Shen, Congyue Deng, Haoran Geng, Songlin Wei,
                <b>He Wang</b>, Leonidas J. Guibas†
              </p>
              <p class="decs">RA-L 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2311.02787">arXiv</a>
                <a href="https://qq456cvb.github.io/projects/donut/">Project</a>
                <a
                  id="MakeADonutID"
                  href="javascript:hideshow(document.getElementById('MakeADonutID'),document.getElementById('MakeADonutbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="MakeADonutbibtex" style="font-size:14px; display: none">
              @article{you2023make,
                title={Make a Donut: Language-Guided Hierarchical EMD-Space Planning for Zero-shot Deformable Object Manipulation},
                author={You, Yang and Shen, Bokui and Deng, Congyue and Geng, Haoran and Wang, He and Guibas, Leonidas},
                journal={arXiv preprint arXiv:2311.02787},
                year={2023}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/61.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Code-as-Monitor: Constraint-aware Visual Programming for
                Reactive and Proactive Robotic Failure Detection
              </div>
              <p>
                Enshen Zhou*, Qi Su*, Cheng Chi*<span>&#8224;</span>, Zhizheng
                Zhang, Zhongyuan Wang, Tiejun Huang, Lu
                Sheng<span>&#8224;</span>, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2412.04455">arXiv</a>
                <a href="https://zhoues.github.io/Code-as-Monitor/">Project</a>
                <a
                  id="MonitorID"
                  href="javascript:hideshow(document.getElementById('MonitorID'),document.getElementById('MMonitorbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="MMonitorbibtex" style="font-size:14px; display: none">
              @article{zhou2024code,
                title={Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection},
                author={Zhou, Enshen and Su, Qi and Chi, Cheng and Zhang, Zhizheng and Wang, Zhongyuan and Huang, Tiejun and Sheng, Lu and Wang, He},
                journal={arXiv preprint arXiv:2412.04455},
                year={2024}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/60.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                MobileH2R: Learning Generalizable Human to Mobile Robot Handover
                Exclusively from Scalable and Diverse Synthetic Data
              </div>
              <p>
                Zifan Wang*, Ziqing Chen*, Junyu Chen*, Jilong Wang, Yuxin Yang,
                Yunze Liu, Xueyi Liu, <b>He Wang</b>, Li Yi<span>&#8224;</span>
              </p>
              <p class="decs">CVPR 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2501.04595">arXiv</a>
                <a href="https://mobile.github.io/">Project</a>
                <a
                  id="MobileH2RID"
                  href="javascript:hideshow(document.getElementById('MobileH2RID'),document.getElementById('MobileH2Rbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="MobileH2Rbibtex" style="font-size:14px; display: none">
              @misc{wang2025mobileh2rlearninggeneralizablehuman,
                  title={MobileH2R: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data}, 
                  author={Zifan Wang and Ziqing Chen and Junyu Chen and Jilong Wang and Yuxin Yang and Yunze Liu and Xueyi Liu and He Wang and Li Yi},
                  year={2025},
                  eprint={2501.04595},
                  archivePrefix={arXiv},
                  primaryClass={cs.RO},
                  url={https://arxiv.org/abs/2501.04595}, 
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/58.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                GAPartManip: A Large-scale Part-centric Dataset for
                Material-Agnostic Articulated Object Manipulation
              </div>
              <p>
                Wenbo Cui*, Chengyang Zhao* , Songlin Wei* , Jiazhao Zhang,
                Haoran Geng, Yaran Chen,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2411.18276">arXiv</a>
                <a href="https://cwb0106.github.io/GAPartManip.github.io/"
                  >Project</a
                >
                <a
                  id="GAPartManipID"
                  href="javascript:hideshow(document.getElementById('GAPartManipID'),document.getElementById('GAPartManipbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="GAPartManipbibtex" style="font-size:14px; display: none">
              @article{cui2024gapartmanip,
                title={GAPartManip: A Large-scale Part-centric Dataset for Material-Agnostic Articulated Object Manipulation},
                author={Cui, Wenbo and Zhao, Chengyang and Wei, Songlin and Zhang, Jiazhao and Geng, Haoran and Chen, Yaran and Wang, He},
                journal={arXiv preprint arXiv:2411.18276},
                year={2024}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/57.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                BODex: Scalable and Efficient Robotic Dexterous Grasp Synthesis
                Using Bilevel Optimization
              </div>
              <p>Jiayi Chen*, Yubin Ke*, <b>He Wang†</b></p>
              <p class="decs">ICRA 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2412.16490">arXiv</a>
                <a href="https://pku-epic.github.io/BODex/">Project</a>
                <a
                  id="BODexID"
                  href="javascript:hideshow(document.getElementById('BODexID'),document.getElementById('BODexbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="BODexbibtex" style="font-size:14px; display: none">
              @article{chen2024bodex,
                title={BODex: Scalable and Efficient Robotic Dexterous Grasp Synthesis Using Bilevel Optimization},
                author={Chen, Jiayi and Ke, Yubin and Wang, He},
                journal={arXiv preprint arXiv:2412.16490},
                year={2024}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/56.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                NaVid-4D: Unleashing Spatial Intelligence in Egocentric RGB-D
                Videos for Vision-and-Language Navigation
              </div>
              <p>
                Haoran Liu*, Weikang Wan*, Xiqian Yu*, Minghan Li*, Jiazhao
                Zhang, Bo Zhao, Zhibo Chen, Zhongyuan Wang, Zhizheng Zhang†,
                <b> He Wang†</b>
              </p>
              <p class="decs">ICRA 2025</p>
              <div class="button">
                <a
                  href="https://lhrrhl0419.github.io/NaVid4D/paper/NaVid_4D.pdf"
                  >arXiv</a
                >
                <a href="https://lhrrhl0419.github.io/NaVid4D/">Project</a>
                <a
                  id="NaVid-4DID"
                  href="javascript:hideshow(document.getElementById('NaVid-4DID'),document.getElementById('NaVid-4Dbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="NaVid-4Dbibtex" style="font-size:14px; display: none">
               @inproceedings{liu2025navid4d,
                  title={NaVid-4D: Unleashing Spatial Intelligence in Egocentric RGB-D Videos for Vision-and-Language Navigation},
                  author={Liu, Haoran and Wan, Weikang and Yu, Xiqian and Li, Minghan and Zhang, Jiazhao and Zhao, 
                  Bo and Chen, Zhibo and Wang, Zhongyuan and Zhang, Zhizheng and Wang, He},
                  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}
                }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/55.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                QuadWBG: Generalizable Quadrupedal Whole-Body Grasping
              </div>
              <p>
                Jilong Wang*, Javokhirbek Rajabov*, Chaoyi Xu, Yiming Zheng,
                <b>He Wang†</b>
              </p>
              <p class="decs">ICRA 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2411.06782">arXiv</a>
                <a href="https://quadwbg.github.io/">Project</a>
                <a
                  id="QuadWBGID"
                  href="javascript:hideshow(document.getElementById('QuadWBGID'),document.getElementById('QuadWBGbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="QuadWBGbibtex" style="font-size:14px; display: none">
               @inproceedings{2024QuadWBG,
                title={QuadWBG: Generalizable Quadrupedal Whole-Body Grasping},
                author={ Wang, Jilong  and  Rajabov, Javokhirbek  and  Xu, Chaoyi  and  Zheng, Yiming  and  Wang, He },
                year={2024},
              }
            </p>
          </pre>
          <div class="selected-item">
            <a
              href="https://oss-cn-beijing.galbot.com/online/video/watchLess.mp4"
              ><img
                class="lazy-load"
                src="https://oss-cn-beijing.galbot.com/online/blog/59.jpg"
                alt=""
            /></a>
            <div class="item-right">
              <div class="item-title">
                Watch Less, Feel More: Direct Sim-to-real RL for Articulated
                Object Manipulation with Motion Adaptation and Impedance Control
              </div>
              <p>
                Tan-Dzung Do, Gireesh Nandiraju, Jilong Wang, <b>He Wang†</b>
              </p>
              <p class="decs">ICRA 2025</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2502.14457">arXiv</a>
                <a href="https://watch-less-feel-more.github.io/">Project</a>
                <a
                  id="WatchLessID"
                  href="javascript:hideshow(document.getElementById('WatchLessID'),document.getElementById('WatchLessBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="WatchLessBibtex" style="font-size:14px; display: none">
               @article{do2025watch,
                title={Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control},
                author={Do, Tan-Dzung and Gireesh, Nandiraju and Wang, Jilong and Wang, He},
                journal={arXiv preprint arXiv:2502.14457},
                year={2025}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/54.jpg"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                W-ControlUDA: Weather-Controllable Diffusion-assisted
                Unsupervised Domain Adaptation for Semantic Segmentation
              </div>
              <p>
                Fengyi Shen, Li Zhou, Kagan Kucukaytekin, George Eskandar,
                Ziyuan Liu, <b>He Wang†</b>, Alois Knoll†
              </p>
              <p class="decs">RA-L</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2402.06446">arXiv</a>
                <!-- <a href="https://pku-epic.github.io/RotationLaplace">Project</a> -->
                <a
                  id="ControlUDAID"
                  href="javascript:hideshow(document.getElementById('ControlUDAID'),document.getElementById('ControlUDABibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="ControlUDABibtex" style="font-size:14px; display: none">
            @article{fengyi_2025,
              title={W-ControlUDA: Weather-Controllable Diffusion-assisted Unsupervised Domain Adaptation for Semantic Segmentation},
              author={Shen, Fengyi and Zhou, Li and Kuecuekaytekin, Kagan and Eskandar, George Basem Fouad and Liu, Ziyuan and Wang, He and Knoll, Alois},
              volume={10},
              url={http://dx.doi.org/10.1109/LRA.2025.3544925},
              doi={10.1109/lra.2025.3544925},
              number={5},
              journal={IEEE Robotics and Automation Letters},
              publisher={Institute of Electrical and Electronics Engineers (IEEE)},
              year={2025},
              month={May},
              pages={4204-4211},
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/53.jpg"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Towards Robust Probabilistic Modeling on SO(3) via Rotation
                Laplace Distribution
              </div>
              <p>
                Yingda Yin*, Jiangran Lyu*, Yang Wang,
                <b>He Wang<span>&#8224;</span></b
                >, Baoquan Chen<span>&#8224;</span>
              </p>
              <p class="decs">TPAMI</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2305.10465">arXiv</a>
                <a href="https://pku-epic.github.io/RotationLaplace">Project</a>
                <a
                  id="TowardsID"
                  href="javascript:hideshow(document.getElementById('TowardsID'),document.getElementById('TowardsBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="TowardsBibtex" style="font-size:14px; display: none">
            @article{yin2025towards,
              title={Towards robust probabilistic modeling on SO (3) via rotation laplace distribution},
              author={Yin, Yingda and Lyu, Jiangran and Wang, Yang and Liu, Haoran and Wang, He and Chen, Baoquan},
              journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
              year={2025},
              publisher={IEEE},
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/49.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                D<sup>3</sup>RoMa: Disparity Diffusion-based Depth Sensing for
                Material-Agnostic Robotic Manipulation
              </div>
              <p>
                Songlin Wei, Haoran Geng, Jiayi Chen, Congyue Deng, Wenbo Cui,
                Chengyang Zhao, Xiaomeng Fang, Leonidas J. Guibas, <b>He Wang†</b>
              </p>
              <p class="decs">CoRL 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2409.14365">arXiv</a>
                <a href="https://pku-epic.github.io/D3RoMa">Project</a>
                <a
                  id="D3RoMaID"
                  href="javascript:hideshow(document.getElementById('D3RoMaID'),document.getElementById('D3RoMaBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="D3RoMaBibtex" style="font-size:14px; display: none">
            @inproceedings{
              wei2024droma,
              title={D3RoMa: Disparity Diffusion-based Depth Sensing for Material-Agnostic Robotic Manipulation},
              author={Songlin Wei and Haoran Geng and Jiayi Chen and Congyue Deng and Cui Wenbo and Chengyang Zhao and Xiaomeng Fang and Leonidas Guibas and He Wang},
              booktitle={8th Annual Conference on Robot Learning},
              year={2024},
              url={https://openreview.net/forum?id=7E3JAys1xO}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/50.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                DexGraspNet 2.0: Learning Generative Dexterous Grasping in
                Large-scale Synthetic Cluttered Scenes
              </div>
              <p>
                Jialiang Zhang*, Haoran Liu*, Danshi Li*, Xinqiang Yu*, Haoran
                Geng, Yufei Ding, Jiayi Chen, <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CoRL 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2410.23004">arXiv</a>
                <a href="https://pku-epic.github.io/DexGraspNet2.0">Project</a>
                <a
                  id="DexGraspNetID"
                  href="javascript:hideshow(document.getElementById('DexGraspNetID'),document.getElementById('DexGraspNetBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="DexGraspNetBibtex" style="font-size:14px; display: none">
             @inproceedings{zhangdexgraspnet,
              title={DexGraspNet 2.0: Learning Generative Dexterous Grasping in Large-scale Synthetic Cluttered Scenes},
              author={Zhang, Jialiang and Liu, Haoran and Li, Danshi and Yu, XinQiang and Geng, Haoran and Ding, Yufei and Chen, Jiayi and Wang, He},
              booktitle={8th Annual Conference on Robot Learning}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/51.jpg"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                RAM: Retrieval-Based Affordance Transfer for Generalizable
                Zero-Shot Robotic Manipulation
              </div>
              <p>
                Yuxuan Kuang*, Junjie Ye*, Haoran Geng*, Jiageng Mao, Congyue
                Deng, Leonidas J. Guibas, <b>He Wang</b>, Yue Wang†
              </p>
              <p class="decs">CoRL 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2407.04689">arXiv</a>
                <a href="https://yxkryptonite.github.io/RAM/">Project</a>
                <a
                  id="RAMID"
                  href="javascript:hideshow(document.getElementById('RAMID'),document.getElementById('RAMBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="RAMBibtex" style="font-size:14px; display: none">
             @article{kuang2024ram,
              title={RAM: Retrieval-Based Affordance Transfer for Generalizable Zero-Shot Robotic Manipulation},
              author={Kuang, Yuxuan and Ye, Junjie and Geng, Haoran and Mao, Jiageng and Deng, Congyue and Guibas, Leonidas and Wang, He and Wang, Yue},
              journal={arXiv preprint arXiv:2407.04689},
              year={2024}
            }
            </p>
          </pre>
          <div class="selected-item">
            <a
              href="https://oss-cn-beijing.galbot.com/online/video/scissorbot_1.mp4"
              ><img
                class="lazy-load"
                src="https://oss-cn-beijing.galbot.com/online/blog/52.jpg"
                alt=""
            /></a>
            <div class="item-right">
              <div class="item-title">
                ScissorBot: Learning Generalizable Scissor Skill for Paper
                Cutting via Simulation, Imitation, and Sim2Real
              </div>
              <p>
                Jiangran Lyu, Yuxing Chen, Tao Du, Feng Zhu, Huiquan Liu, Yizhou
                Wang†, <b>He Wang†</b>
              </p>
              <p class="decs">CoRL 2024</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2409.13966">arXiv</a>
                <a href="https://pku-epic.github.io/ScissorBot">Project</a>
                <a
                  id="ScissorBotID"
                  href="javascript:hideshow(document.getElementById('ScissorBotID'),document.getElementById('ScissorBotBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="ScissorBotBibtex" style="font-size:14px; display: none">
             @inproceedings{lyuscissorbot, 
                title =     {ScissorBot: Learning Generalizable Scissor Skill for Paper Cutting via Simulation, Imitation, and Sim2Real}, 
                author =    {Lyu, Jiangran and Chen, Yuxing and Du, Tao and Zhu, Feng and Liu, Huiquan and Wang, Yizhou and Wang, He},
                booktitle=  {8th Annual Conference on Robot Learning}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/46.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Task-Oriented Dexterous Grasp Synthesis via Differentiable Grasp
                Wrench Boundary Estimator
              </div>
              <p>Jiayi Chen, Yuxing Chen, Jialiang Zhang, <b>He Wang†</b></p>
              <p class="decs">IROS 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2309.13586" class="arXiv"
                  >arXiv</a
                >
                <a
                  href="https://pku-epic.github.io/TaskDexGrasp/"
                  class="project"
                  >Project</a
                >
                <a
                  id="OrientedID"
                  href="javascript:hideshow(document.getElementById('OrientedID'),document.getElementById('OrientedBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="OrientedBibtex" style="font-size:14px; display: none">
            @article{chen2023task,
              title={Task-Oriented Dexterous Grasp Synthesis via Differentiable Grasp Wrench Boundary Estimator},
              author={Chen, Jiayi and Chen, Yuxing and Zhang, Jialiang and Wang, He},
              journal={arXiv preprint arXiv:2309.13586},
              year={2023}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/47.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Open6DOR: Benchmarking Open-instruction 6-DoF Object
                Rearrangement and A VLM-based Approach
              </div>
              <p>
                Yufei Ding*, Haoran Geng*, Chaoyi Xu, Xiaomeng Fang, Jiazhao
                Zhang, Songlin Wei, Qiyu Dai, Zhizheng Zhang, <b>He Wang†</b>
              </p>
              <p class="decs">IROS 2024 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://pku-epic.github.io/Open6DOR/" class="project"
                  >Project</a
                >
                <a
                  id="Open6DORID"
                  href="javascript:hideshow(document.getElementById('Open6DORID'),document.getElementById('Open6DORBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="Open6DORBibtex" style="font-size:14px; display: none">
            @INPROCEEDINGS{10802733,
              author={Ding, Yufei and Geng, Haoran and Xu, Chaoyi and Fang, Xiaomeng and Zhang, Jiazhao and Wei, Songlin and Dai, Qiyu and Zhang, Zhizheng and Wang, He},
              booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
              title={Open6DOR: Benchmarking Open-instruction 6-DoF Object Rearrangement and A VLM-based Approach}, 
              year={2024},
              volume={},
              number={},
              pages={7359-7366},
              keywords={Three-dimensional displays;Benchmark testing;Propulsion;6-DOF;Real-time systems;Artificial intelligence;Intelligent robots;Synthetic data},
              doi={10.1109/IROS58592.2024.10802733}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/1.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                NaVid: Video-based VLM Plans the Next Step for
                Vision-and-Language Navigation
              </div>
              <p>
                Jiazhao Zhang*, Kunyu Wang*, Rongtao Xu*, Gengze Zhou, Yicong
                Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang†, <b>He Wang†</b>
              </p>
              <p class="decs">RSS 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2402.15852" class="arXiv"
                  >arXiv</a
                >
                <a href="https://pku-epic.github.io/NaVid/" class="project"
                  >Project</a
                >
                <a
                  id="NaVidID"
                  href="javascript:hideshow(document.getElementById('NaVidID'),document.getElementById('NaVidBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="NaVidBibtex" style="font-size:14px; display: none">
            @article{zhang2024navid,
              title={NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation},
              author={Zhang, Jiazhao and Wang, Kunyu and Xu, Rongtao and Zhou, Gengze and Hong, Yicong and Fang, Xiaomeng and Wu, Qi and Zhang, Zhizheng and Wang, He},
              journal={Robotics: Science and Systems},
              year={2024}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/2.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                SAGE: Bridging Semantic and Actionable Parts for Generalizable
                Manipulation of Articulated Objects
              </div>
              <p>
                Haoran Geng*, Songlin Wei*, Congyue Deng, Bokui Shen,
                <b>He Wang<span>&#8224;</span></b
                >, Leonidas J. Guibas<span>&#8224;</span>
              </p>
              <p class="decs">RSS 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2312.01307">arXiv</a>
                <a href="https://geometry.stanford.edu/projects/sage/"
                  >Project</a
                >
                <a
                  id="misc-a"
                  href="javascript:hideshow(document.getElementById('misc-a'),document.getElementById('misc'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="misc" style="font-size:14px; display: none">
            @misc{geng2023sage,
              title={SAGE: Bridging Semantic and Actionable Parts for GEneralizable Articulated-Object Manipulation under Language Instructions},
              author={Haoran Geng and Songlin Wei and Congyue Deng and Bokui Shen and He Wang and Leonidas Guibas},
              year={2023},
              eprint={2312.01307},
              archivePrefix={arXiv},
              primaryClass={cs.RO} 
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/3.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Enhancing Generalizable 6D Pose Tracking of an In-Hand Object
                with Tactile Sensing
              </div>
              <p>
                Yun Liu*, Xiaomeng Xu*, Weihang Chen, Haocheng Yuan,
                <b>He Wang</b>, Jing Xu, Rui Chen, Li Yi<span>&#8224;</span>
              </p>
              <p class="decs">RA-L</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2210.04026" class="arXiv"
                  >arXiv</a
                >
                <a href="https://github.com/leolyliu/TEG-Track" class="project"
                  >Project</a
                >
                <a
                  id="EnhancingID"
                  href="javascript:hideshow(document.getElementById('EnhancingID'),document.getElementById('EnhancingBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="EnhancingBibtex" style="font-size:14px; display: none">
            @ARTICLE{10333330,
              author={Liu, Yun and Xu, Xiaomeng and Chen, Weihang and Yuan, Haocheng and Wang, He and Xu, Jing and Chen, Rui and Yi, Li},
              journal={IEEE Robotics and Automation Letters}, 
              title={Enhancing Generalizable 6D Pose Tracking of an In-Hand Object With Tactile Sensing}, 
              year={2024},
              volume={9},
              number={2},
              pages={1106-1113},
              keywords={Visualization;Kinematics;Sensor fusion;Tracking;Three-dimensional displays;Tactile sensors;Visualization;Robot kinematics;Force and tactile sensing;sensor fusion;visual tracking},
              doi={10.1109/LRA.2023.3337690}
            }
            </p>
          </pre>
          <div class="selected-item">
            <a
              href="https://oss-cn-beijing.galbot.com/online/video/maskcluster.mp4"
              ><img
                class="lazy-load"
                src="https://oss-cn-beijing.galbot.com/online/blog/4.png"
                alt=""
            /></a>
            <div class="item-right">
              <div class="item-title">
                MaskClustering: View Consensus based Mask Graph Clustering for
                Open-Vocabulary 3D Instance Segmentation
              </div>
              <p>Mi Yan, Jiazhao Zhang, Yan Zhu, <b>He Wang†</b></p>
              <p class="decs">CVPR 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2401.07745" class="arXiv"
                  >arXiv</a
                >
                <a
                  href="https://pku-epic.github.io/MaskClustering/"
                  class="project"
                  >Project</a
                >
                <a
                  id="MaskClusteringID"
                  href="javascript:hideshow(document.getElementById('MaskClusteringID'),document.getElementById('MaskClusteringBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="MaskClusteringBibtex" style="font-size:14px; display: none">
            @INPROCEEDINGS{10658542,
              author={Yan, Mi and Zhang, Jiazhao and Zhu, Yan and Wang, He},
              booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
              title={MaskClustering: View Consensus Based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation}, 
              year={2024},
              volume={},
              number={},
              pages={28274-28284},
              keywords={Measurement;Instance segmentation;Solid modeling;Computer vision;Three-dimensional displays;Shape;Navigation;3D instance segmentation;open-vocabulary},
              doi={10.1109/CVPR52733.2024.02671}
            }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/5.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Category-Level Multi-Part Multi-Joint 3D Shape Assembly
              </div>
              <p>
                Yichen Li<span>&#8224;</span> ,Kaichun Mo, Yueqi Duan ,
                <b>He Wang</b>, Jiequan Zhang, Lin Shao, Wojciech Matusik,
                
              </p>
              <p class="decs">CVPR 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2303.06163" class="arXiv"
                  >arXiv</a
                >
                <a
                  href="https://people.csail.mit.edu/yichenl/projects/joint/"
                  class="project"
                  >Project</a
                >
                <a
                  id="Category-a"
                  href="javascript:hideshow(document.getElementById('Category-a'),document.getElementById('Category'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="Category" style="font-size:14px; display: none">
            @article{li2020impartass,
              title={Learning 3D Part Assembly from a Single Image},
              author={Li, Yichen and Mo, Kaichun and Shao, Lin and Sung, Minghyuk and Guibas, Leonidas},
              journal={European conference on computer vision (ECCV 2020)},
              year={2020}
            }
            </p>
          </pre> -->
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/6.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                STOPNet: Multiview-based 6-DoF Suction Detection for Transparent
                Objects on Production Lines
              </div>
              <p>
                Yuxuan Kuang*, Qin Han*, Danshi Li, Qiyu Dai, Lian Ding, Dong
                Sun, Hanlin Zhao,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2310.05717" class="arXiv"
                  >arXiv</a
                >
                <a href="https://pku-epic.github.io/STOPNet/" class="project"
                  >Project</a
                >
                <a
                  id="STOPNetID"
                  href="javascript:hideshow(document.getElementById('STOPNetID'),document.getElementById('STOPNetBibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="STOPNetBibtex" style="font-size:14px; display: none">
            @inproceedings{kuang2023stopnet,
              title={STOPNet: Multiview-based 6-DoF Suction Detection for Transparent Objects on Production Lines},
              author={Kuang, Yuxuan and Han, Qin and Li, Danshi and Dai, Qiyu and Ding, Lian and Sun, Dong and Zhao, Hanlin and Wang, He},
              booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
              year={2024},
              organization={IEEE}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/7.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning
                based on Online Grasping Pose Fusion
              </div>
              <p>
                Jiazhao Zhang*, Nandiraju Gireesh*, Jilong Wang, Xiaomeng Fang,
                Chaoyi Xu, Weiguang Chen, Liu Dai,
                <b>He Wang†</b>
              </p>
              <p class="decs">ICRA 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2309.15459" class="arXiv"
                  >arXiv</a
                >
                <a href="https://pku-epic.github.io/GAMMA/" class="project"
                  >Project</a
                >
                <a
                  id="GAMMA-a"
                  href="javascript:hideshow(document.getElementById('GAMMA-a'),document.getElementById('GAMMA'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="GAMMA" style="font-size:14px; display: none">
            @misc{zhang2023gamma,
              title={GAMMA: Graspability-Aware Mobile MAnipulation Policy Learning based on Online Grasping Pose Fusion}, 
              author={Jiazhao Zhang and Nandiraju Gireesh and Jilong Wang and Xiaomeng Fang and Chaoyi Xu and Weiguang Chen and Liu Dai and He Wang},
              year={2023},
              eprint={2309.15459},
              archivePrefix={arXiv},
              primaryClass={cs.RO}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/8.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                ASGrasp: Generalizable Transparent Object Reconstruction and
                6-DoF Grasp Detection from RGB-D Active Stereo Camera
              </div>
              <p>
                Jun Shi, Yong A, Yixiang Jin, Dingzhe Li, Haoyu Niu, Zhezhu Jin,
                <b>He Wang†</b>
              </p>
              <p class="decs">ICRA 2024</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2405.05648" class="arXiv"
                  >arXiv</a
                >
                <a href="https://pku-epic.github.io/ASGrasp/" class="project"
                  >Project</a
                >
                <a
                  id="ASGraspID"
                  href="javascript:hideshow(document.getElementById('ASGraspID'),document.getElementById('ASGraspbibtex'))"
                  class="bibtex"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="ASGraspbibtex" style="font-size:14px; display: none">
            @article{shi2024gsnet,
              title={ASGrasp: Generalizable Transparent Object Reconstruction and 6-DoF Grasp Detection from RGB-D Active Stereo Camera},
              author={Jun Shi, Yong A, Yixiang Jin, Dingzhe Li, Haoyu Niu, Zhezhu Jin, He Wang},
              journal={arXiv preprint arXiv:2405.05648},
              year={2024}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/9.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                UniDexGrasp++: Improving Dexterous Grasping Policy Learning via
                Geometry-aware Curriculum and Iterative Generalist-Specialist
                Learning
              </div>
              <p>
                Weikang Wan*, Haoran Geng*, Yun Liu, Zikang Shan, Yaodong Yang,
                Li Yi,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">
                ICCV 2023 (<b>Oral & Best Paper Finalist,</b> final reviews of
                all strong accepts)
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2304.00464">arXiv</a>
                <a href="https://pku-epic.github.io/UniDexGrasp++/">Project</a>
                <a
                  id="UniDexGrasp1-a"
                  href="javascript:hideshow(document.getElementById('UniDexGrasp1-a'),document.getElementById('UniDexGrasp++'))"
                  >bibtex
                </a>
              </div>
            </div>
          </div>
          <pre>
            <p id="UniDexGrasp++" style="font-size:14px; display: none">
            @article{wan2023unidexgrasp++,
              title={UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning},
              author={Wan, Weikang and Geng, Haoran and Liu, Yun and Shan, Zikang and Yang, Yaodong and Yi, Li and Wang, He},
              journal={arXiv preprint arXiv:2304.00464},
              year={2023} 
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/10.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                GAPartNet: Cross-Category Domain-Generalizable Object Perception
                and Manipulation via Generalizable and Actionable Parts
              </div>
              <p>
                Haoran Geng*, Helin Xu*, Chengyang Zhao*, Chao Xu, Li Yi, Siyuan
                Huang,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">
                CVPR 2023 (Highlight, final reviews of all accepts)
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2211.05272">arXiv</a>
                <a href="https://pku-epic.github.io/GAPartNet/">Project</a>
                <a
                  id="GAPartNet-a"
                  href="javascript:hideshow(document.getElementById('GAPartNet-a'), document.getElementById('GAPartNet'))"
                  >bibtex
                </a>
              </div>
            </div>
          </div>
          <pre>
            <p id="GAPartNet" style="font-size:14px; display: none">
              @article{geng2022gapartnet,
                title={GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable and Actionable Parts},
                author={Geng, Haoran and Xu, Helin and Zhao, Chengyang and Xu, Chao and Yi, Li and Huang, Siyuan and Wang, He},
                journal={arXiv preprint arXiv:2211.05272},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/11.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                3D-Aware Object Goal Navigation via Simultaneous Exploration and
                Identification
              </div>
              <p>
                Jiazhao Zhang*, Liu Dai*, Fanpeng Meng, Qingnan Fan, Xuelin
                Chen, Kai Xu,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2212.00338.pdf">arXiv</a>
                <a href="https://pku-epic.github.io/3D-Aware-ObjectNav/"
                  >Project</a
                >
                <a
                  id="3DNav-a"
                  href="javascript:hideshow(document.getElementById('3DNav-a'),document.getElementById('3DNav'))"
                  >bibtex
                </a>
              </div>
            </div>
          </div>
          <pre>
            <p id="3DNav" style="font-size:14px; display: none">
              @article{zhang20223d,
                title={3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification},
                author={Zhang, Jiazhao and Dai, Liu and Meng, Fanpeng and Fan, Qingnan and Chen, Xuelin and Xu, Kai and Wang, He},
                journal={arXiv preprint arXiv:2212.00338},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/12.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                UniDexGrasp: Universal Robotic Dexterous Grasping via Learning
                Diverse Proposal Generation and Goal-Conditioned Policy
              </div>
              <p>
                Yinzhen Xu*, Weikang Wan*, Jialiang Zhang*, Haoran Liu*, Zikang
                Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi
                Chen, Tengyu Liu, Li Yi,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2303.00938.pdf">arXiv</a>
                <a href="https://pku-epic.github.io/UniDexGrasp/">Project</a>
                <a
                  id="UniDexGrasp-a"
                  href="javascript:hideshow(document.getElementById('UniDexGrasp-a'),document.getElementById('UniDexGrasp'))"
                  >bibtex
                </a>
              </div>
            </div>
          </div>
          <pre>
            <p id="UniDexGrasp" style="font:18px; display: none">
              @article{xu2023unidexgrasp,
                title={UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy},
                author={Xu, Yinzhen and Wan, Weikang and Zhang, Jialiang and Liu, Haoran and Shan, Zikang and Shen, Hao and Wang, Ruicheng and Geng, Haoran and Weng, Yijia and Chen, Jiayi and others},
                journal={arXiv preprint arXiv:2303.00938},
                year={2023}
              }
            </p >
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/13.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                PartManip: Learning Cross-Category Generalizable Part
                Manipulation Policy from Point Cloud Observations
              </div>
              <p>
                Haoran Geng*, Ziming Li*, Yiran Geng, Jiayi Chen, Hao Dong,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2303.16958">arXiv</a>
                <a href="https://pku-epic.github.io/PartManip/">Project</a>
                <a
                  id="PartManip-a"
                  href="javascript:hideshow(document.getElementById('PartManip-a'), document.getElementById('PartManip'))"
                  >bibtex
                </a>
              </div>
            </div>
          </div>
          <pre>
            <p id="PartManip" style="font:18px; display: none">
              @article{geng2023partmanip,
                title={PartManip: Learning Cross-Category Generalizable Part Manipulation Policy from Point Cloud Observations},
                author={Geng, Haoran and Li, Ziming and Geng, Yiran and Chen, Jiayi and Dong, Hao and Wang, He},
                journal={arXiv preprint arXiv:2303.16958},
                year={2023}
              }
            </p >
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/14.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Delving into Discrete Normalizing Flows on SO(3) Manifold for
                Probabilistic Rotation Modeling
              </div>
              <p>
                Yulin Liu*, Haoran Liu*, Yingda Yin*, Yang Wang, Baoquan
                Chen<span>&#8224;</span>,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2304.03937.pdf">arXiv</a>
                <a
                  id="SO3NF-a"
                  href="javascript:hideshow(document.getElementById('SO3NF-a'), document.getElementById('SO3NF'))"
                  >SO3NF</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="SO3NF" style="font-size:14px; display: none">
            @article{liu2023delving,
              title={Delving into Discrete Normalizing Flows on SO (3) Manifold for Probabilistic Rotation Modeling},
              author={Liu, Yulin and Liu, Haoran and Yin, Yingda and Wang, Yang and Chen, Baoquan and Wang, He},
              journal={arXiv preprint arXiv:2304.03937},
              year={2023}
            }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/15.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                DiGA: Distil to Generalize and then Adapt for Domain Adaptive
                Semantic Segmentation
              </div>
              <p>
                Fengyi Shen, Akhil Gurram, Ziyuan Liu,
                <b>He Wang<span>&#8224;</span></b
                >, Alois Knoll<span>&#8224;</span>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2304.02222.pdf">arXiv</a>
                <a
                  id="DiGA-a"
                  href="javascript:hideshow(document.getElementById('DiGA-a'), document.getElementById('DiGA'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="DiGA" style="font-size:14px; display: none">
              @article{shen2023diga,
                title={DiGA: Distil to Generalize and then Adapt for Domain Adaptive Semantic Segmentation},
                author={Shen, Fengyi and Gurram, Akhil and Liu, Ziyuan and Wang, He and Knoll, Alois},
                journal={arXiv preprint arXiv:2304.02222},
                year={2023}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/16.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Adaptive Zone-aware Hierarchical Planner for Vision-Language
                Navigation
              </div>
              <p>
                Chen Gao, Xingyu Peng, Mi Yan,
                <b>He Wang</b>, Lirong Yang, Haibing Ren, Hongsheng Li, Si Liu<span>&#8224;</span>
              </p>
              <p class="decs">CVPR 2023</p>
              <div class="button">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.pdf">paper</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/17.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for
                General Objects Based on Simulation
              </div>
              <p>
                Ruicheng Wang*, Jialiang Zhang*, Jiayi Chen, Yinzhen Xu, Puhao
                Li, Tengyu Liu,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">
                ICRA 2023 (<b>Outstanding Manipulation Paper Award Finalist</b>)
              </p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2210.02697.pdf">arXiv</a>
                <a href="https://pku-epic.github.io/DexGraspNet/">Project</a>
                <a
                  id="dexgraspnet-a"
                  href="javascript:hideshow(document.getElementById('dexgraspnet-a'),document.getElementById('dexgraspnet'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="dexgraspnet" style="font-size:14px; display: none">
              @article{2210.02697,
                title = {DexGraspNet: A Large-Scale Robotic Dexterous Grasp Dataset for General Objects Based on Simulation},
                author = {Ruicheng Wang and Jialiang Zhang and Jiayi Chen and Yinzhen Xu and Puhao Li and Tengyu Liu and He Wang},
                journal={arXiv preprint arXiv:2210.02697},
                year = {2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/18.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent
                and Specular Objects Using Generalizable NeRF
              </div>
              <p>
                Qiyu Dai*, Yan Zhu*, Yiran Geng, Ciyu Ruan, Jiazhao Zhang,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ICRA 2023</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2210.06575.pdf">arXiv</a>
                <a href="https://pku-epic.github.io/GraspNeRF/">Project</a>
                <a
                  id="GraspNeRF-a"
                  href="javascript:hideshow(document.getElementById('GraspNeRF-a'), document.getElementById('GraspNeRF'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="GraspNeRF" style="font-size:14px; display: none">
              @article{dai2022graspnerf,
                title={GraspNeRF: Multiview-based 6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF},
                author={Dai, Qiyu and Zhu, Yan and Geng, Yiran and Ruan, Ciyu and Zhang, Jiazhao and Wang, He},
                journal={arXiv preprint arXiv:2210.06575},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/19.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                A Laplace-inspired Distribution on SO(3) for Probabilistic
                Rotation Estimation
              </div>
              <p>
                Yingda Yin, Yang Wang,
                <b>He Wang<span>&#8224;</span></b
                >, Baoquan Chen<span>&#8224;</span>
              </p>
              <p class="decs">ICLR 2023 (<b>notable top 25%</b>)</p>
              <div class="button">
                <a href="https://openreview.net/pdf?id=Mvetq8DO05O">Paper</a>
                <a
                  id="RotLaplace-a"
                  href="javascript:hideshow(document.getElementById('RotLaplace-a'),document.getElementById('RotLaplace'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="RotLaplace" style="font-size:14px; display: none">
              @inproceedings{
                yin2023a,
                title={A Laplace-inspired Distribution on {SO}(3) for Probabilistic Rotation Estimation},
                author={Yingda Yin and Yang Wang and He Wang and Baoquan Chen},
                booktitle={The Eleventh International Conference on Learning Representations },
                year={2023},
                url={https://openreview.net/forum?id=Mvetq8DO05O}
              }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/20.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Self-Supervised Category-Level Articulated Object Pose
                Estimation with Part-Level SE(3) Equivariance
              </div>
              <p>
                Xueyi Liu, Ji Zhang, Ruizhen Hu, Haibin Huang,
                <b>He Wang</b>, Li Yi
              </p>
              <p class="decs">ICLR 2023</p>
              <div class="button">
                <a href="https://openreview.net/forum?id=20GtJ6hIaPA">Paper</a>
                <a
                  id="SE3Art-a"
                  href="javascript:hideshow(document.getElementById('SE3Art-a'),document.getElementById('SE3Art'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="SE3Art" style="font-size:14px; display: none">
              @inproceedings{
                liu2023selfsupervised,
                title={Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level {SE}(3) Equivariance},
                author={Xueyi Liu and Ji Zhang and Ruizhen Hu and Haibin Huang and He Wang and Li Yi},
                booktitle={The Eleventh International Conference on Learning Representations },
                year={2023},
                url={https://openreview.net/forum?id=20GtJ6hIaPA}
              }
            </p>
          </pre> -->
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/21.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Tracking and Reconstructing Hand Object Interactions from Point
                Cloud Sequences in the Wild
              </div>
              <p>
                Jiayi Chen*, Mi Yan*, Jiazhao Zhang, Yenzhen Xu, Xiaolong Li,
                Yijia Weng, Li Yi, Shuran Song,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">AAAI 2023 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2209.12009">arXiv</a>
                <a href="https://github.com/PKU-EPIC/HOTrack">Project</a>
                <a
                  id="TRHOI-a"
                  href="javascript:hideshow(document.getElementById('TRHOI-a'),document.getElementById('TRHOI'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="TRHOI" style="font-size:14px; display: none">
              @article{chen2022tracking,
                title={Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild},
                author={Chen, Jiayi and Yan, Mi and Zhang, Jiazhao and Xu, Yinzhen and Li, Xiaolong and Weng, Yijia and Yi, Li and Song, Shuran and Wang, He},
                journal={arXiv preprint arXiv:2209.12009},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/22.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                ASRO-DIO: Active Subspace Random Optimization Based Depth
                Inertial Odometry
              </div>
              <p>
                Jiazhao Zhang, Yijie Tang,
                <b>He Wang</b>, Kai Xu<span>&#8224;</span>
              </p>
              <p class="decs">IEEE Transactions on Robotics (T-RO)</p>
              <div class="button">
                <a href="https://kevinkaixu.net/papers/zhang_tro22_asro.pdf">Paper</a>
              </div>
            </div>
          </div>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/23.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Domain Randomization-Enhanced Depth Simulation and Restoration
                for Perceiving and Grasping Specular and Transparent Objects
              </div>
              <p>
                Qiyu Dai*, Jiyao Zhang*, Qiwei Li, Tianhao Wu, Hao Dong, Ziyuan
                Liu, Ping Tan,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">ECCV 2022</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2208.03792.pdf">arXiv</a>
                <a href="https://pku-epic.github.io/DREDS/">Project</a>
                <a href="https://github.com/PKU-EPIC/DREDS">Code</a>
                <a
                  id="dreds-a"
                  href="javascript:hideshow(document.getElementById('dreds-a'),document.getElementById('dreds'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="dreds" style="font-size:14px; display: none">
              @article{dai2022domain,
                title={Domain Randomization-Enhanced Depth Simulation and Restoration for Perceiving and Grasping Specular and Transparent Objects},
                author={Dai, Qiyu and Zhang, Jiyao and Li, Qiwei and Wu, Tianhao and Dong, Hao and Liu, Ziyuan and Tan, Ping and Wang, He},
                journal={arXiv preprint arXiv:2208.03792},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/24.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Learning Category-Level Generalizable Object Manipulation Policy
                via Generative Adversarial Self-Imitation Learning from
                Demonstrations
              </div>
              <p>
                Hao Shen*, Weikang Wan*,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <div>
                <p class="decs">
                  Robotics and Automation Letters (RA-L) and IROS 2022
                </p>
                <p style="opacity: 1; position: relative; top: -10px">
                  <b class="bold">1st prize winner</b> of
                  <a href="https://sapien.ucsd.edu/challenges/maniskill2021/">
                    SAPIEN ManiSkill Challenge 2021
                  </a>
                  (no external annotation track)
                </p>
              </div>

              <div class="button">
                <a href="https://arxiv.org/pdf/2203.02107.pdf">arXiv</a>
                <a
                  href="https://shen-hhao.github.io/Category_Level_Manipulation/"
                  >Project</a
                >
                <a
                  href="https://shen-hhao.github.io/Category_Level_Manipulation/"
                  >Code</a
                >
                <a
                  id="maniskill-a"
                  href="javascript:hideshow(document.getElementById('maniskill-a'),document.getElementById('maniskill'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="maniskill" style="font-size:14px; display: none">
              @article{shen2022learning,
                title={Learning Category-Level Generalizable Object Manipulation Policy via Generative Adversarial Self-Imitation Learning from Demonstrations},
                author={Shen, Hao and Wan, Weikang and Wang, He},
                journal={arXiv preprint arXiv:2203.02107},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/25.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                FisherMatch: Semi-Supervised Rotation Regression via
                Entropy-based Filtering
              </div>
              <p>
                Yingda Yin, Yingcheng Cai,
                <b>He Wang<span>&#8224;</span></b
                >, Baoquan Chen<span>&#8224;</span>
              </p>
              <p class="decs">CVPR 2022 (<b>Oral Presentation</b>)</p>

              <div class="button">
                <a href="https://arxiv.org/pdf/2203.15765.pdf">arXiv</a>
                <a href="https://yd-yin.github.io/FisherMatch/">Project</a>
                <a href="https://github.com/yd-yin/FisherMatch">Code</a>
                <a
                  id="fishermatch-a"
                  href="javascript:hideshow(document.getElementById('fishermatch-a'),document.getElementById('fishermatch'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="fishermatch" style="font-size:14px; display: none">
              @article{yin2022fishermatch,
                title={FisherMatch: Semi-Supervised Rotation Regression via Entropy-based Filtering},
                author={Yin, Yingda and Cai, Yingcheng and Wang, He and Chen, Baoquan},
                journal={arXiv preprint arXiv:2203.15765},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/26.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Projective Manifold Gradient Layer for Deep Rotation Regression
              </div>
              <p>
                Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen, Leonidas
                Guibas,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">CVPR 2022</p>

              <div class="button">
                <a href="https://arxiv.org/pdf/2110.11657.pdf">arXiv</a>
                <a href="https://jychen18.github.io/RPMG/">Project</a>
                <a href="https://github.com/jychen18/RPMG">Code</a>
                <a
                  id="rpmg-a"
                  href="javascript:hideshow(document.getElementById('rpmg-a'),document.getElementById('rpmg'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="rpmg" style="font-size:14px; display: none">
              @article{chen2021projective,
                title={Projective Manifold Gradient Layer for Deep Rotation Regression},
                author={Chen, Jiayi and Yin, Yingda and Birdal, Tolga and Chen, Baoquan and Guibas, Leonidas and Wang, He},
                journal={arXiv preprint arXiv:2110.11657},
                year={2021}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/27.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                ADeLA: Automatic Dense Labeling with Attention for Viewpoint
                Adaptation in Semantic Segmentation
              </div>
              <p>
                <!-- Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen, Leonidas
                Guibas, -->
                Yanchao Yang<span>*&#8224;</span>, Hanxiang Ren*,
                <b>He Wang</b>, Bokui Shen, Qingnan Fan, Youyi
                Zheng<span>&#8224;</span>, C Karen Liu, Leonidas J. Guibas
              </p>
              <p class="decs">CVPR 2022 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2107.14285">arXiv</a>
                <a
                  id="adela-a"
                  href="javascript:hideshow(document.getElementById('adela-a'),document.getElementById('adela'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="adela" style="font-size:14px; display: none">
              @article{yang2021adela,
                title={ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation},
                author={Yang, Yanchao and Ren, Hanxiang and Wang, He and Shen, Bokui and Fan, Qingnan and Zheng, Youyi and Liu, C Karen and Guibas, Leonidas},
                journal={arXiv preprint arXiv:2107.14285},
                year={2021}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/28.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object
                Interaction
              </div>
              <p>
                Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao
                Shen, Boqiang Liang, Zhoujie Fu,
                <b>He Wang</b>, Li Yi<span>&#8224;</span>
              </p>
              <p class="decs">CVPR 2022</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2203.01577.pdf">arXiv</a>
                <a href="https://hoi4d.github.io/">Project</a>
                <a
                  id="HOI4D-a"
                  href="javascript:hideshow(document.getElementById('HOI4D-a'),document.getElementById('HOI4D'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="HOI4D" style="font-size:14px; display: none">
              @article{liu2022hoi4d,
                title={HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction},
                author={Liu, Yunze and Liu, Yun and Jiang, Che and Fu, Zhoujie and Lyu, Kangbo and Wan, Weikang and Shen, Hao and Liang, Boqiang and Wang, He and Yi, Li},
                journal={arXiv preprint arXiv:2203.01577},
                year={2022}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/29.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Multi-Robot Active Mapping via Neural Bipartite Graph Matching
              </div>
              <p>
                Kai Ye*, Siyan Dong*, Qingnan Fan,
                <b>He Wang</b>, Li Yi, Fei Xia, Jue Wang, Baoquan Chen
              </p>
              <p class="decs">CVPR 2022</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2203.16319.pdf">arXiv</a>
                <a
                  id="MultiRobotMap-a"
                  href="javascript:hideshow(document.getElementById('MultiRobotMap-a'),document.getElementById('MultiRobotMap'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="MultiRobotMap" style="font-size:14px; display: none">
              @article{ye2022multi,
                title={Multi-Robot Active Mapping via Neural Bipartite Graph Matching},
                author={Ye, Kai and Dong, Siyan and Fan, Qingnan and Wang, He and Yi, Li and Xia, Fei and Wang, Jue and Chen, Baoquan},
                journal={arXiv preprint arXiv:2203.16319},
                year={2022}
              }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/30.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric
                Guidance
              </div>
              <p>
                Tianchen Zhao, Niansong Zhang, Xuefei Ning,
                <b>He Wang</b>, Li Yi, Yu Wang
              </p>
              <p class="decs">CVPR 2022</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2203.09887.pdf">arXiv</a>
                <a
                  id="CodedVTR-a"
                  href="javascript:hideshow(document.getElementById('CodedVTR-a'),document.getElementById('CodedVTR'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="CodedVTR" style="font-size:14px; display: none">
              @article{zhao2022codedvtr,
                title={CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance},
                author={Zhao, Tianchen and Zhang, Niansong and Ning, Xuefei and Wang, He and Yi, Li and Wang, Yu},
                journal={arXiv preprint arXiv:2203.09887},
                year={2022}
              }
            </p>
          </pre> -->
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/31.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Domain Adaptation on Point Clouds via Geometry-Aware Implicits
              </div>
              <p>
                Yuefan Shen*, Yanchao Yang*, Mi Yan,
                <b>He Wang</b>, Youyi Zheng, Leonidas J. Guibas
              </p>
              <p class="decs">CVPR 2022</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2112.09343">arXiv</a>
                <a
                  id="DAGAI-a"
                  href="javascript:hideshow(document.getElementById('DAGAI-a'),document.getElementById('DAGAI'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="DAGAI" style="font-size:14px; display: none">
              @article{shen2021domain,
                title={Domain Adaptation on Point Clouds via Geometry-Aware Implicits},
                author={Shen, Yuefan and Yang, Yanchao and Yan, Mi and Wang, He and Zheng, Youyi and Guibas, Leonidas},
                journal={arXiv preprint arXiv:2112.09343},
                year={2021}
              }
            </p>
          </pre> -->
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/32.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Leveraging SE(3) Equivariance for Self-supervised Category-Level
                Object Pose Estimation from Point Clouds
              </div>
              <p>
                Xiaolong Li, Yijia Weng, Li Yi, Leonidas J. Guibas, A. Lynn
                Abbott, Shuran Song,
                <b>He Wang<span>&#8224;</span></b>
              </p>
              <p class="decs">NeurIPS 21</p>
              <div class="button">
                <a href="https://openreview.net/forum?id=wGRNAqVBQT2">Paper</a>
                <a href="https://arxiv.org/pdf/2111.00190.pdf">arXiv</a>
                <a href="https://dragonlong.github.io/equi-pose/">Project</a>
                <a href="https://github.com/dragonlong/equi-pose">Code</a>
                <a
                  id="esscop-a"
                  href="javascript:hideshow(document.getElementById('esscop-a'),document.getElementById('esscop'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="esscop" style="font-size:14px; display: none">
              @article{li2021leveraging,
                title={Leveraging SE (3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds},
                author={Li, Xiaolong and Weng, Yijia and Yi, Li and Guibas, Leonidas J and Abbott, A and Song, Shuran and Wang, He},
                journal={Advances in Neural Information Processing Systems},
                volume={34},
                year={2021}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/33.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated
                Objects from Point Clouds
              </div>
              <p>
                Yijia Weng*,
                <b>He Wang*<span>&#8224;</span></b
                >, Qiang Zhou, Yuzhe Qin, Yueqi Duan, Qingnan Fan, Baoquan Chen,
                Hao Su, Leonidas J. Guibas
              </p>
              <p class="decs">ICCV 2021 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2104.03437">arXiv</a>
                <a href="https://yijiaweng.github.io/CAPTRA/">Project</a>
                <a href="https://github.com/halfsummer11/CAPTRA">Code</a>
                <a href="https://youtu.be/JFPcOHCH2O0">Video</a>
                <a
                  id="captra-a"
                  href="javascript:hideshow(document.getElementById('captra-a'),document.getElementById('captra'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="captra" style="font-size:14px; display: none">
              @article{weng2021captra,
                title={CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds},
                author={Weng, Yijia and Wang, He and Zhou, Qiang and Qin, Yuzhe and Duan, Yueqi and Fan, Qingnan and 
                        Chen, Baoquan and Su, Hao and Guibas, Leonidas J},
                journal={arXiv preprint arXiv:2104.03437},
                year={2021}
              }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="https://oss-cn-beijing.galbot.com/online/blog/34.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Single Image 3D Shape Retrieval via Cross-Modal Instance and
                Category Contrastive Learning
              </div>
              <p>
                Mingxian Lin, Jie Yang,
                <b>He Wang</b>, Yu-Kun Lai, Rongfei Jia, Binqiang Zhao, Lin Gao
              </p>
              <p class="decs">ICCV 2021</p>
              <div class="button">
                <a
                  href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Single_Image_3D_Shape_Retrieval_via_Cross-Modal_Instance_and_Category_ICCV_2021_paper.pdf"
                  >Paper</a
                >
                <a
                  href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Lin_Single_Image_3D_ICCV_2021_supplemental.pdf"
                  >Supp.</a
                >
                <a
                  id="retrievel-a"
                  href="javascript:hideshow(document.getElementById('retrievel-a'),document.getElementById('retrievel'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="retrievel" style="font-size:14px; display: none">
              @inproceedings{lin2021single,
                title={Single Image 3D Shape Retrieval via Cross-Modal Instance and Category Contrastive Learning},
                author={Lin, Ming-Xian and Yang, Jie and Wang, He and Lai, Yu-Kun and Jia, Rongfei and Zhao, Binqiang and Gao, Lin},
                booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
                pages={11405--11415},
                year={2021}
              }
            </p>
          </pre> -->
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/35.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D
                Object Detection
              </div>
              <p>
                <b>He Wang*</b
                >, Yezhen Cong*, Or Litany, Yue Gao, Leonidas J. Guibas
              </p>
              <p class="decs">ICCV 2021</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2012.04355.pdf">Paper</a>
                <a href="https://thu17cyz.github.io/3DIoUMatch/">Project</a>
                <a href="https://github.com/THU17cyz/3DIoUMatch">Code</a>
                <a href="https://youtu.be/nuARjhkQN2U">Video</a>
                <a
                  id="3dioumatch-a"
                  href="javascript:hideshow(document.getElementById('3dioumatch-a'),document.getElementById('3dioumatch'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="3dioumatch" style="font-size:14px; display: none">
              @inproceedings{wang20213dioumatch,
                title={3DIoUMatch: Leveraging iou prediction for semi-supervised 3d object detection},
                author={Wang, He and Cong, Yezhen and Litany, Or and Gao, Yue and Guibas, Leonidas J},
                booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                pages={14615--14624},
                year={2021}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/36.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                MultiBodySync: Multi-Body Segmentation and Motion Estimation via
                3D Scan Synchronization
              </div>
              <p>
                Jiahui Huang,
                <b>He Wang</b>, Tolga Birdal, Minkyuk Sung, Federica Arrigoni,
                Shi-Min Hu, Leonidas J. Guibas
              </p>
              <p class="decs">CVPR 2021 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/pdf/2101.06605.pdf">Paper</a>
                <a href="https://github.com/huangjh-pub/multibody-sync">Code</a>
                <a href="https://www.youtube.com/watch?v=BuIBXL2UNvI">Video</a>
                <a
                  id="multibodysync-a"
                  href="javascript:hideshow(document.getElementById('multibodysync-a'),document.getElementById('multibodysync'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="multibodysync" style="font-size:14px; display: none">
              @inproceedings{huang2021multibodysync,
                title={Multibodysync: Multi-body segmentation and motion estimation via 3d scan synchronization},
                author={Huang, Jiahui and Wang, He and Birdal, Tolga and Sung, Minhyuk and Arrigoni, Federica and Hu, Shi-Min and Guibas, Leonidas J},
                booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                pages={7108--7118},
                year={2021}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/37.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Robust Neural Routing Through Space Partitions for Camera
                Relocalization in Dynamic Indoor Environments
              </div>
              <p>
                Siyan Dong*, Qingnan Fan*,
                <b>He Wang</b>, Ji Shi, Li Yi, Thomas Funkhouser, Baoquan Chen,
                Leonidas J. Guibas
              </p>
              <p class="decs">CVPR 2021 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2010.05272">Paper</a>
                <a href="https://github.com/siyandong/NeuralRouting">Code</a>
                <a
                  id="neuralrouting-a"
                  href="javascript:hideshow(document.getElementById('neuralrouting-a'),document.getElementById('neuralrouting'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="neuralrouting" style="font-size:14px; display: none">
              @InProceedings{Dong_2021_CVPR,
                author = {Dong, Siyan and Fan, Qingnan and Wang, He and Shi, Ji and Yi, Li and Funkhouser, Thomas and Chen, Baoquan and Guibas, Leonidas J.},
                title = {Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments},
                booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                month = {June},
                year = {2021},
                pages = {8544-8554}
            }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/38.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                Rethinking Sampling in 3D Point Cloud Generative Adversarial
                Networks
              </div>
              <p>
                <b>He Wang*</b>, Zetian Jiang*, Li Yi, Kaichun Mo, Hao Su,
                Leonidas J. Guibas
              </p>
              <p class="decs">
                CVPR 2021 Workshop on
                <i>Learning to Generate 3D Shapes and Scenes</i>
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/2006.07029">Paper</a>
                <a
                  href="https://drive.google.com/file/d/1RF77Zp6cQgoU0tf33Wjthx0D6tr3IYAJ/view?usp=sharing"
                  >Poster</a
                >
                <a href="https://www.youtube.com/watch?v=Ejzj0hnKW4Y">Video</a>
                <a
                  id="rethink-a"
                  href="javascript:hideshow(document.getElementById('rethink-a'),document.getElementById('rethink'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="rethink" style="font-size:14px; display: none">
              @article{wang2020rethinking,
                title={Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks},
                author={Wang, He and Jiang, Zetian and Yi, Li and Mo, Kaichun and Su, Hao and Guibas, Leonidas J},
                journal={arXiv preprint arXiv:2006.07029},
                year={2020}
              }
            </p>
          </pre> -->
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/39.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree
                Conditions
              </div>
              <p>
                Kaichun Mo,
                <b>He Wang</b>, Li Yi, Xinchen Yan and Leonidas J.Guibas
              </p>
              <p class="decs">ECCV 2020</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2003.08624">Paper</a>
                <a href="https://cs.stanford.edu/~kaichun/pt2pc/">Project</a>
                <a href="https://github.com/daerduoCarey/pt2pc">Code&Data</a>
                <a
                  id="pt2pc-a"
                  href="javascript:hideshow(document.getElementById('pt2pc-a'),document.getElementById('pt2pc'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="pt2pc" style="font-size:14px; display: none">
              @article{mo2020pt2pc,
                title={PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions},
                author={Mo, Kaichun and Wang, He and Yan, Xinchen and Guibas, Leonidas},
                journal={arXiv preprint arXiv:2003.08624},
                year={2020}
              }
            </p>
          </pre> -->
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/40.png" alt="" />
            <div class="item-right">
              <div class="item-title">Curriculum DeepSDF</div>
              <p>
                Yueqi Duan*, Haidong Zhu*,
                <b>He Wang</b>, Li Yi, Ram Nevatia, Leonidas J. Guibas
              </p>
              <p class="decs">ECCV 2020</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2003.08593">Paper</a>
                <a href="https://github.com/haidongz-usc/Curriculum-DeepSDF"
                  >Code</a
                >
                <a
                  id="csdf-a"
                  href="javascript:hideshow(document.getElementById('csdf-a'),document.getElementById('csdf'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div> -->
          <pre>
            <p id="csdf" style="font-size:14px; display: none">
              @misc{duan2020curriculum,
                title={Curriculum DeepSDF},
                author={Yueqi Duan and Haidong Zhu and He Wang and Li Yi and Ram Nevatia and Leonidas J. Guibas},
                year={2020},
                eprint={2003.08593},
                archivePrefix={arXiv},
                primaryClass={cs.CV}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/41.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Category-Level Articulated Object Pose Estimation
              </div>
              <p>
                <b>He Wang*</b>, Xiaolong Li*, Li Yi, Leonidas J. Guibas, A. Lynn
                Abbott, Shuran Song<span>&#8224;</span>
              </p>
              <p class="decs">CVPR 2020 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/1912.11913">Paper</a>
                <a href="https://articulated-pose.github.io/">Project</a>
                <a href="https://github.com/dragonlong/articulated-pose"
                  >Code&Data</a
                >
                <a
                  id="ancsh-a"
                  href="javascript:hideshow(document.getElementById('ancsh-a'),document.getElementById('ancsh'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="ancsh" style="font-size:14px; display: none">
              @article{li2019category,
                title={Category-Level Articulated Object Pose Estimation},
                author={Li, Xiaolong and Wang, He and Yi, Li and Guibas, Leonidas and Abbott, A Lynn and Song, Shuran},
                journal={arXiv preprint arXiv:1912.11913},
                year={2019}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/42.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                SAPIEN: A SimulAted Part-based Interactive ENvironment
              </div>
              <p>
                Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu,
                Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan,
                <b>He Wang</b>, Li Yi, Angel X. Chang, Leonidas J. Guibas, Hao
                Su<span>&#8224;</span>
              </p>
              <p class="decs">CVPR 2020 (<b>Oral Presentation</b>)</p>
              <div class="button">
                <a href="https://arxiv.org/abs/2003.08515">Paper</a>
                <a href="https://sapien.ucsd.edu/">Project</a>
                <a href="https://github.com/haosulab/SAPIEN-Release"
                  >Code&Data</a
                >
                <a href="https://youtu.be/K2yOeJhJXzM">Demo</a>
                <a
                  id="sapien-a"
                  href="javascript:hideshow(document.getElementById('sapien-a'),document.getElementById('sapien'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="sapien" style="font-size:14px; display: none">
              @InProceedings{Xiang_2020_SAPIEN,
                author={Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao 
                  and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu 
                  and Wang, He and Yi, Li and Chang, Angel X. and Guibas, Leonidas J. and Su, Hao},
                title={SAPIEN: A SimulAted Part-based Interactive ENvironment},
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
                month={June},
                year={2020}
              }
            </p>
          </pre>
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/43.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Normalized Object Coordinate Space for Category-Level 6D Object
                Pose and Size Estimation
              </div>
              <p>
                <b>He Wang</b>, Srinath Sridhar, Jingwei Huang, Julien Valentin,
                Shuran Song, Leonidas J. Guibas
              </p>
              <p class="decs">
                CVPR 2019 (<b>Oral Presentation</b>),
                <a href="https://mp.weixin.qq.com/s/LAVMLCsi3-XHWLbklomxkg"
                  >WAICYOP Award 2022</a
                >
              </p>
              <div class="button">
                <a href="https://arxiv.org/abs/1901.02970">Paper</a>
                <a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/"
                  >Project</a
                >
                <a href="https://github.com/hughw19/NOCS_CVPR2019">Code&Data</a>
                <a
                  id="posercnn-a"
                  href="javascript:hideshow(document.getElementById('posercnn-a'),document.getElementById('posercnn'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="posercnn" style="font-size:14px; display: none">
              @inproceedings{wang2019normalized,
                title={Normalized object coordinate space for category-level 6d object pose and size estimation},
                author={Wang, He and Sridhar, Srinath and Huang, Jingwei and Valentin, Julien and Song, Shuran and Guibas, Leonidas J},
                booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
                pages={2642--2651},
                year={2019}
              }
            </p>
          </pre>
          <!-- <div class="selected-item">
            <img class="lazy-load" src="./assets/44.png" alt="" />
            <div class="item-right">
              <div class="item-title">
                GSPN: Generative Shape Proposal Network for 3D Instance
                Segmentation in Point Cloud
              </div>
              <p>
                Li Yi, Wang Zhao,
                <b>He Wang</b>, Minhyuk Sung, Leonidas J. Guibas
              </p>
              <p class="decs">CVPR 2019</p>
              <div class="button">
                <a href="https://arxiv.org/abs/1812.03320">Paper</a>
                <a href="https://github.com/ericyi/GSPN">Code</a>
                <a
                  id="gspn-a"
                  href="javascript:hideshow(document.getElementById('gspn-a'),document.getElementById('gspn'))"
                  >bibtex</a
                >
              </div>
            </div>
          </div>
          <pre>
            <p id="gspn" style="font-size:14px; display: none">
              @article{yi2018gspn,
                title={GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud},
                author={Yi, Li and Zhao, Wang and Wang, He and Sung, Minhyuk and Guibas, Leonidas},
                journal={arXiv preprint arXiv:1812.03320},
                year={2018}
              }
            </p>
          </pre> -->
          <div class="selected-item">
            <img
              class="lazy-load"
              src="https://oss-cn-beijing.galbot.com/online/blog/45.png"
              alt=""
            />
            <div class="item-right">
              <div class="item-title">
                Learning a Generative Model for Multi-Step Human-Object
                Interactions from Videos
              </div>
              <p>
                <b>He Wang<span>*</span></b
                >, Soeren Pirk*, Ersin Yumer, Vladimir Kim, Ozan Sener, Srinath
                Sridhar, Leonidas J. Guibas
              </p>
              <p class="decs">
                Eurographics 2019 (<b>Best Paper Honorable Mention</b>)
              </p>
              <div class="button">
                <a
                  href="http://www.pirk.info/projects/learning_interactions/index.html"
                  >Project</a
                >
                <a
                  href="https://oss-cn-beijing.galbot.com/online/activity/19_EG_FnInteract.pdf"
                  >Paper</a
                >
                <a href="https://github.com/hughw19/ActionPlotGeneration.git"
                  >Code</a
                >
                <a href="http://ai.stanford.edu/blog/generate-human-object/"
                  >Blog</a
                >
                <a href="https://www.youtube.com/watch?v=WgpPalA2RzA">Video</a>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="main-content">
        <div id="awards" class="main-news">
          <div class="title">AWARDS</div>
          <ul>
            <li>2025 World Internet Conference Leading Technology Award.</li>
            <!-- <li>The 9th "Haiying Talent" program</li> -->
            <li>
              Fortune magazine's 2025 "China's 40 Under 40 Business Leaders".
            </li>
            <li>InTech Technology Award.</li>
            <li>
              MIT Technology Review's "Innovators Under 35 China" (TR35 China).
            </li>
            <li>
              2024 Peking University-China Optics Valley Award for Scientific
              and Technological Achievements Transformation.
            </li>
            <li>The Honorary Scholar of Intel China Academic Talent Program.</li>
            <li>ICCV 2023 Best Paper Finalist.</li>
            <li>ICRA 2023 Outstanding Manipulation Paper Finalist.</li>
            <li>
              2022 World Artificial Intelligence Conference Youth Outstanding
              Paper Award.
            </li>
            <li>Eurographics 2019 Best Paper Honorable Mention.</li>
            <li>
              1st prize winner of
              <a href="https://sapien.ucsd.edu/challenges/maniskill2021/"
                >SAPIEN ManiSkill Challenge 2021</a
              >
              (no external annotation track).
            </li>
            <!-- <li></li> -->
          </ul>
        </div>
      </div>
      <div class="main-content">
        <div id="teaching" class="main-news">
          <div class="title">TEACHING</div>
          <p>Undergraduate course:</p>
          <ul>
            <li>
              <a
                href="https://dean.pku.edu.cn/service/web/courseDetail.php?flag=1&zxjhbh=BZ2526204834020_18972"
                >Introduction to Embodied AI</a
              >, Spring 2026
            </li>
            <li>
              <a
                href="https://dean.pku.edu.cn/service/web/courseDetail.php?flag=1&zxjhbh=BZ2526204834920_15659"
                >Introduction to Computer Vision</a
              >, Spring 2026
            </li>
            <!-- <li></li> -->
          </ul>
        </div>
      </div>
      <div class="main-content">
        <div id="professional" class="main-news">
          <div class="title">PROFESSIONAL SERVICE</div>
          <ul>
            <!-- <li>
              Associate Editor:
              <a
                href="https://www.sciencedirect.com/journal/image-and-vision-computing"
                >Image and Vision Computing</a
              >
            </li> -->
            <li>
              Committee Member, Third Session of the Science and Technology
              Innovation Advisory Committee, Shanghai Stock Exchange (SSE).
            </li>
            <li>Senior Advisor, Artificial Intelligence 100 Council (AI100).</li>
            <li>
              Expert Committee Member, Guangdong Artificial Intelligence and
              Robotics Industry Alliance.
            </li>
            <li>Area chair (AC): CVPR, ICCV, WACV, etc.</li>
            <li>
              Vice Chair of the Embodied AI Subcommittee, Artificial
              Intelligence Standardization Technical Committee, Ministry of
              Industry and Information Technology (MIIT), China.
            </li>
            <li>
              Executive Committee Member, Intelligent Robotics Technical
              Committee, China Computer Federation (CCF).
            </li>
            <li>
              Committee Member, 3D Vision Technical Committee, China Society of
              Image and Graphics (CSIG).
            </li>
            <li>
              Area Chair, Vision and Learning Young Scholars Summit (VALSE).
            </li>
            <li>
              Committee Member, Embodied AI Professional Committee, Chinese
              Association for Artificial Intelligence (CAAI).
            </li>
            <li>Beijing Academy of Artificial Intelligence (BAAI) Scholar.</li>
            <!-- <li>
              Program committee/reviewer:
              <br />Conferences: CVPR, ICCV, ECCV, NeurIPS, ICLR, AAAI,
              SIGGRAPH, RSS, IROS, ICRA <br />Journals: IEEE TPAMI, IEEE RAL
            </li> -->
            <!-- <li></li> -->
          </ul>
        </div>
      </div>
      <div class="modal">
        <div class="modal-close">
          <img src="./assets/close.svg" alt="" class="close" />
        </div>
        <div class="modal-content">
          <div class="modal-content-item" id="link1">
            <a href="#news">News</a>
          </div>
          <div class="modal-content-item" id="link2">
            <a href="#publications">Publications</a>
          </div>
          <div class="modal-content-item" id="link3">
            <a href="#awards">Awards</a>
          </div>
          <div class="modal-content-item" id="link4">
            <a href="#teaching">Teaching</a>
          </div>
          <div class="modal-content-item" id="link5">
            <a href="#professional">Professional Service</a>
          </div>
          <div class="modal-content-item" id="link6">
            <a href="#opportunities">Opportunities</a>
          </div>
        </div>
        <div class="modal-footer">
          <div class="modal-footer-item footer-item-active">
            <a href="https://hughw19.github.io">Home</a>
          </div>
          <span>/</span>
          <div class="modal-footer-item">
            <a href="https://PKU-EPIC.github.io">Lab</a>
          </div>
        </div>
      </div>
    </div>
    <script src="./js/jquery-3.7.1.js" crossorigin="anonymous"></script>
    <script>
      // -------------------------
      // 语言数据定义
      // -------------------------
      var I18N_LANG_DATA = {
        zh: {
          opportunities: {
            subTitle1: '博士招生机会',
            tips: '我们团队每年提供以下博士招生名额：',
            li1: '北京大学计算机前沿中心（CFCS）博士名额：2名；',
            li2: '北京智源研究院 — 中科院自动化所联合培养博士名额：2名及以上；',
            // li3: '北京中关村学院博士名额：若干；',
            li4: '面向香港地区学生及国际学生的博士名额：1名。',
            p1: '此外，我们与<b>清华大学交叉信息研究院、上海期智研究院</b>保持紧密合作，每年多名学生经推荐进入<b>清华大学、上海交通大学等</b>高校攻读<b>学术型或工程型博士。</b>',
            p2: '欢迎具备具身智能研究经验和论文发表经历的本科生、硕士生在对应申请截止日期前<b>至少提前一年</b>联系我。<b>优先录取有本团队实习经历的学生。</b>',
            subTitle2: '访问学生与科研实习生',
            p3: '我们欢迎来自全球顶尖高校的优秀<b>本科生和研究生</b>申请<b>6个月以上的线下科研实习。',
            p4: '实习地点位于<b>北京大学–银河通用具身智能联合实验室（中关村鼎好大厦）</b>, 并提供一流的科研环境及具有竞争力的生活与住宿津贴。',
            p5: '过去几年，我们已合作实习生上百名，累计在国际顶级会议/期刊上发表论文上百篇。团队实习生在博士申请中具备显著优势，许多同学被录取到<b>北大、清华、智源–自动化所联合培养博士项目</b>, 以及<b>Stanford、MIT、Berkeley、CMU、UCLA、UCSD</b> 等世界一流大学。',
            subTitle3: '全职岗位（银河通用 & 智源）',
            p6: '在<b>银河通用（Galbot）</b>与<b>北京智源研究院具身智能中心</b>, 我们长期招聘：',
            li5: '全职研究科学家',
            li6: '全职工程师',
            p7: '如有兴趣，欢迎邮件联系。',
          },
        },
        en: {
          opportunities: {
            subTitle1: 'Doctoral Recruitment Opportunities',
            tips: 'Our team offers the following doctoral positions each year:',
            li1: 'Doctoral positions at the Center for Frontiers in Computing (CFCS), Peking University: 2;',
            li2: 'Joint doctoral program positions between Beijing Academy of Artificial Intelligence and Institute of Automation, Chinese Academy of Sciences: 2 or more;',
            // li3: 'Doctoral positions at Beijing Zhongguancun College: a number of openings;',
            li4: 'Doctoral positions for students from Hong Kong and international students: 1.',
            p1: 'In addition, we maintain clos collaborations with the <b>Institute for Interdisciplinary Information Sciences at Tsinghua University </b>and <b>the Shanghai Qizhi Institute.</b> Each year, a number of students are recommended to pursue <b>academic or engineering doctoral degrees</b> at <b>Tsinghua University, Shanghai Jiao Tong University</b>, and other institutions.',
            p2: "Undergraduate and master's students with research experience in embodied intelligence and a record of publications are welcome to contact me <b>at least one year</b> before the corresponding application deadlines. <b>Priority will be given to applicants with internship experience in our team.</b>",
            subTitle2: 'Visiting Students and Research Interns',
            p3: 'We welcome outstanding <b>undergraduate and graduate students</b> from top universities worldwide to apply for <b>on-site research internships lasting six months or more.</b>',
            p4: 'The internship takes place at the <b>Peking University–Galbot Embodied Intelligence Joint Laboratory(located in Zhongguancun Dinghao Building)</b>, offering a first-class research environment and competitive living and accommodation allowances.',
            p5: 'Over the past few years, we have collaborated with hundreds of interns, resulting in the publication of over a hundred papers in top-tier international conferences and journals. Interns from our team have a significant advantage in doctoral applications, with many admitted to <b>Peking University, Tsinghua University, the BAAI–IA Joint Doctoral Program</b>, and world-leading institutions such as <b>Stanford, MIT, Berkeley, CMU, UCLA, and UCSD.</b>',
            subTitle3: 'Full-time Positions (Galbot & BAAI)',
            p6: 'At <b>Galbot</b> and <b>the Embodied Intelligence Center of Beijing Academy of Artificial Intelligence</b>, we are continuously recruiting:',
            li5: 'Full-time Research Scientists',
            li6: 'Full-time Engineers',
            p7: 'If interested, please feel free to contact us via email.',
          },
        },
      };

      // 当前语言
      // var currentLang = localStorage.getItem('lang') || 'en';
      var currentLang = 'zh';

      // -------------------------
      // 工具方法：更新语言按钮状态
      // -------------------------
      function updateLangUI(lang) {
        if (lang === 'en') {
          $('.cn').addClass('language-active');
          $('.en').removeClass('language-active');
        } else {
          $('.en').addClass('language-active');
          $('.cn').removeClass('language-active');
        }
      }

      // -------------------------
      // 工具方法：翻译渲染
      // -------------------------
      function renderI18n(data) {
        document.querySelectorAll('[data-i18n]').forEach((el) => {
          const keys = el.getAttribute('data-i18n').split('.');
          let value = data;

          for (const k of keys) {
            value = value?.[k];
          }

          if (typeof value === 'string') {
            el.innerHTML = value;
          } else {
            console.warn(`❗ i18n key 未找到: ${keys.join('.')}`);
          }
        });
      }

      // -------------------------
      // 工具方法：获取语言数据
      // -------------------------
      function getLangData(lang) {
        return I18N_LANG_DATA[lang] || I18N_LANG_DATA['zh'];
      }

      // -------------------------
      // 语言切换主逻辑
      // -------------------------
      function setLang(lang) {
        currentLang = lang;
        localStorage.setItem('lang', lang);
        updateLangUI(lang);
        const langData = getLangData(lang);
        renderI18n(langData);
      }

      // -------------------------
      // 初始化
      // -------------------------
      document.addEventListener('DOMContentLoaded', () => {
        // 初始化按钮状态
        updateLangUI(currentLang);

        // 加载当前语言
        const langData = getLangData(currentLang);
        renderI18n(langData);

        // 视频懒加载 - 加载视频的通用函数
        window.loadVideo = function (video) {
          if (video.dataset.loaded === 'true') return; // 已加载则跳过
          const source = video.querySelector('source[data-src]');
          if (source && source.dataset.src) {
            source.src = source.dataset.src;
            video.load();
            video.dataset.loaded = 'true'; // 标记已加载
          }
        };

        // PC 版视频由 autoPlayVideo 函数控制，这里不处理

        // 手机版视频懒加载（进入视口时加载，但不自动播放）
        const phoneVideos = document.querySelectorAll(
          '.swiper-content-phone video',
        );
        const phoneObserver = new IntersectionObserver(
          (entries) => {
            entries.forEach((entry) => {
              if (entry.isIntersecting) {
                window.loadVideo(entry.target);
                phoneObserver.unobserve(entry.target);
              }
            });
          },
          { rootMargin: '200px' },
        );
        phoneVideos.forEach((video) => phoneObserver.observe(video));

        // 绑定按钮
        document
          .getElementById('lang-cn')
          .addEventListener('click', () => setLang('zh'));
        document
          .getElementById('lang-en')
          .addEventListener('click', () => setLang('en'));
      });
    </script>

    <script>
      document
        .getElementById('link1')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('news');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link2')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('publications');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link3')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('awards');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link4')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('teaching');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link5')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('professional');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
      document
        .getElementById('link6')
        .addEventListener('click', function (event) {
          event.preventDefault(); // 阻止默认锚点跳转行为
          const targetElement = document.getElementById('opportunities');
          const offset = 70; // 位移量
          const targetPosition =
            targetElement.getBoundingClientRect().top + window.pageYOffset;
          const offsetPosition = targetPosition - offset;

          window.scrollTo({
            top: offsetPosition,
            behavior: 'smooth',
          });
        });
    </script>
    <script type="text/javascript">
      function onresizeFun() {
        if (window.innerWidth <= 768) {
          let width = document.documentElement.clientWidth;
          // 假设设计稿宽度为750px
          // 假设已知根元素我们设置为100px（这里设置100方便后续我们好计算）
          // 动态设置根元素html的fontSize
          document.documentElement.style.fontSize = 100 * (width / 430) + 'px';
          $('.head-pc').css('display', 'none');
          $('.icon').css('display', 'none');
          $('.swiper-content').css('display', 'none');
          $('.head-phone').css('display', 'flex');
          $('.icon-phone').css('display', 'flex');
          $('.swiper-content-phone').css('display', 'block');
        } else {
          $('.head-pc').css('display', 'flex');
          $('.icon').css('display', 'flex');
          $('.swiper-content').css('display', 'block');
          $('.swiper-content-phone').css('display', 'none');
          $('.head-phone').css('display', 'none');
          $('.icon-phone').css('display', 'none');
        }
        $('.main').css('display', 'block');
      }

      // pc 视频自动播放
      function autoPlayVideo() {
        const videosContainer = document.getElementById('videos');
        if (!videosContainer) return;
        // 只选择 PC 版视频
        const videos = videosContainer.querySelectorAll(
          '.swiper-content video',
        );

        // 检查视频是否在可见范围内
        function checkVisibility(video) {
          const rect = video.getBoundingClientRect();
          const containerRect = videosContainer.getBoundingClientRect();
          return (
            rect.top >= containerRect.top &&
            rect.left >= containerRect.left &&
            rect.bottom <= containerRect.bottom &&
            rect.right <= containerRect.right
          );
        }

        // 控制视频的播放和暂停
        function controlVideoPlayback() {
          videos.forEach((video) => {
            if (checkVisibility(video)) {
              // 先加载视频再播放
              if (window.loadVideo) window.loadVideo(video);
              video.play().catch(() => {});
            } else {
              video.pause();
            }
          });
        }

        // 初始检查
        controlVideoPlayback();

        // 滚动事件监听
        videosContainer.addEventListener('scroll', controlVideoPlayback);
        window.addEventListener('resize', controlVideoPlayback);
      }

      onresizeFun();
      window.addEventListener('resize', onresizeFun);

      document.addEventListener('DOMContentLoaded', function () {
        // 图片懒加载
        // 图片懒加载
        //const observer = new IntersectionObserver((entries) => {
        //  entries.forEach((entry) => {
        //    if (entry.isIntersecting) {
        //      const img = entry.target;
        //      img.src = img.getAttribute('data-src');
        //      observer.unobserve(img);
        //     }
        //   });
        //  });

        //document.querySelectorAll('img.lazy-load').forEach((img) => {
        // observer.observe(img);
        // });

        // 菜单点击事件
        $('.menu-phone').click(() => {
          $('.modal').addClass('modal-active');
          $('body').addClass('noscroll');
        });
        $('.modal-close').click(() => {
          $('.modal').removeClass('modal-active');
          $('body').removeClass('noscroll');
        });
        $('.modal-content-item').click(function () {
          // Remove the 'item-active' class from all siblings
          $(this).siblings('.modal-content-item').removeClass('item-active');
          // Add the 'item-active' class to the clicked element
          $(this).addClass('item-active');
          $('.modal').removeClass('modal-active');
          $('body').removeClass('noscroll');
        });

        if (window.innerWidth > 768) {
          autoPlayVideo();
        }
      });

      function hideshow(a, which) {
        console.log(which);
        if (!document.getElementById) return;
        if (which.style.display == 'block') {
          which.style.display = 'none';
          a.style.background = 'none';
        } else {
          which.style.display = 'block';
          a.style.background = 'rgba(189,220,255,0.30)';
        }
      }
    </script>
  </body>
</html>
