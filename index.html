<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<title>He Wang</title>
<link rel="stylesheet" type="text/css" href="./He_Wang_files/main.css">
</head>

<body>

<table>
<tbody><tr>
<td><img src="./He_Wang_files/he_portrait.JPG" width="160"></td>
<td>
<div style="font-size:24; font-weight:bold">He Wang (王鹤)</div>
<div>
Tenure-track Assistant Professor at Peking University<br>
助理教授 博士生导师 北京大学前沿计算研究中心（CFCS）<br>
<!-- 研究科学家 北京通用人工智能研究院（BIGAI）<br>-->
<b>Email:</b> <tt>hewang at pku dot edu dot cn</tt><br>

<br>
<a href="https://scholar.google.com/citations?user=roCAWkoAAAAJ&hl=en">[Google Scholar]</a><a href="https://github.com/hughw19">[Github]</a><a href="https://twitter.com/HughWang19">[Twitter]</a>

<br>

</div>
</td>
</tr>
</tbody></table>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>




<div class="section">
<h3>About Me</h3>
<ul>
I am a tenure-track Assistant Professor in the <a href="https://cfcs.pku.edu.cn/english">Center on Frontiers of Computing Studies(CFCS)</a> at <a href="https://english.pku.edu.cn/">Peking University</a>
<!--and a research scientist at <a href="https://www.bigai.ai/">Beijing Institute for General Artificial Intelligence</a> -->. 
My research interests span across 3D vision, robotics, and machine learning, with a special focus on embodied AI. My research objective is to endow robots working in complex real-world scenes with generalizable 3D vision and interaction policies.
Prior to joining Peking University, I received my PhD degree from <a href="https://www.stanford.edu">Stanford University</a> advised by Prof. <a href="http://geometry.stanford.edu/member/guibas/index.html">Leonidas J. Guibas</a> in 2021 and my bachelor degree from <a href="http://www.tsinghua.edu.cn/">Tsinghua University</a> in 2014.<br> 
<br>
<br>
We are actively looking for visiting students, interns, PhDs, and postdocs. Feel free to contact me if you are interested in my research or potential collaborations. For PhD applicants, we do have two PhD student openings each year and please contact me at least one year prior to the applicaiton deadline; for research interns, we welcome undergradute and graduate students from top univerisities to apply for >6 months research internship and can recommend for oversea graduate school applications.
</div>
<br>


<div class="section">
<h3>News</h3>
<ul>

<li>  <b style="color: green; background-color: #ffff42">NEW</b> Two papers are selected to <b><font color='red'>oral presentations</font></b> in CVPR 2022. </b>
<li>  <b style="color: green; background-color: #ffff42">NEW</b> Seven papers get accepted to CVPR 2022. </b>
<li>  One paper accepted to NeurIPS 2021. </b>
<li>  My co-first author paper, <i>CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds</i>, receives ICCV <b><font color='red'>oral presentation</font></b> (acceptance rate: 3%) </b>!
<li>  Two papers accepted to ICCV 2021. </b>
<li>  I will serve as an area chair (AC) of <a href="http://cvpr2022.thecvf.com">CVPR 2022</a>.</b>
<li>  I am serving as an area chair (AC) of <a href="http://wacv2022.thecvf.com">WACV 2022</a>.</b>
<li>  I am serving as an executive area chair (EAC) of <a href="http://valser.org">VALSE</a>.</b>

</ul>
</div>
<br>

<div class="mainsection">

<a name="publications"></a>
<div class="mainsection">
<h3>Selected Publications</h3>
<ul>
*: equivalent contribution, <span>&#8224;</span>: corresponding author<br><br>
<table width="100%">
	


<!-- FisherMatch -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/fishermatch.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">FisherMatch: Semi-Supervised Rotation Regression via Entropy-based Filtering</a></b><br><br>Yingda Yin, Yingcheng Cai, <b>He Wang<span>&#8224;</span></b>, Baoquan Chen<span>&#8224;</span> <br><br> CVPR 2022 (<font color="red">Oral Presentation</font>) <br><br>
<a href="https://arxiv.org/pdf/2203.15765.pdf">arXiv</a>/<a href="https://yd-yin.github.io/FisherMatch/">Project</a>/<a href="javascript:hideshow(document.getElementById('fishermatch'))">bibtex</a>
</p><pre><p id="fishermatch" style="font:18px; display: none">
@article{yin2022fishermatch,
  title={FisherMatch: Semi-Supervised Rotation Regression via Entropy-based Filtering},
  author={Yin, Yingda and Cai, Yingcheng and Wang, He and Chen, Baoquan},
  journal={arXiv preprint arXiv:2203.15765},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
	
<!-- RPMG -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/rpmg.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Projective Manifold Gradient Layer for Deep Rotation Regression</a></b><br><br>Jiayi Chen, Yingda Yin, Tolga Birdal, Baoquan Chen, Leonidas Guibas, <b>He Wang<span>&#8224;</span></b> <br><br> CVPR 2022 <br><br>
<a href="https://arxiv.org/pdf/2110.11657.pdf">arXiv</a>/<a href="https://jychen18.github.io/RPMG/">Project</a>/<a href="https://github.com/jychen18/RPMG">Code</a>/<a href="javascript:hideshow(document.getElementById('rpmg'))">bibtex</a>
</p><pre><p id="rpmg" style="font:18px; display: none">
@article{chen2021projective,
  title={Projective Manifold Gradient Layer for Deep Rotation Regression},
  author={Chen, Jiayi and Yin, Yingda and Birdal, Tolga and Chen, Baoquan and Guibas, Leonidas and Wang, He},
  journal={arXiv preprint arXiv:2110.11657},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	

<!-- ADELA -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/adela.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
	<b><a href="">ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation</a></b><br><br>Yanchao Yang*, Hanxiang Ren*, <b>He Wang</b>, Bokui Shen, Qingnan Fan, Youyi Zheng, C. Karen Liu, Leonidas J. Guibas<br><br> CVPR 2022 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/2107.14285">arXiv</a>/<a href="javascript:hideshow(document.getElementById('adela'))">bibtex</a>
</p><pre><p id="adela" style="font:18px; display: none">
@article{yang2021adela,
  title={ADeLA: Automatic Dense Labeling with Attention for Viewpoint Adaptation in Semantic Segmentation},
  author={Yang, Yanchao and Ren, Hanxiang and Wang, He and Shen, Bokui and Fan, Qingnan and Zheng, Youyi and Liu, C Karen and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2107.14285},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
<!-- HOI4D -->
 <tbody><tr>
 <td width="25%" valign="top"><p><img src="./He_Wang_files/hoi4d.png" width="250" alt="" style="border-style: none" align="top"></p></td>
 <td width="75%" valign="top"><p>
 	<b><a href="">HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction</a></b><br><br>Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, <b>He Wang</b>, Li Yi <br><br> CVPR 2022 <br><br>
 <a href="https://arxiv.org/pdf/2203.01577.pdf">arXiv</a>/<a href="javascript:hideshow(document.getElementById('HOI4D'))">bibtex</a>
 </p><pre><p id="HOI4D" style="font:18px; display: none">
 @article{liu2022hoi4d,
   title={HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction},
   author={Liu, Yunze and Liu, Yun and Jiang, Che and Fu, Zhoujie and Lyu, Kangbo and Wan, Weikang and Shen, Hao and Liang, Boqiang and Wang, He and Yi, Li},
   journal={arXiv preprint arXiv:2203.01577},
   year={2022}
 }
 </p><p></p></pre>
 <p></p></td>
 </tr>
 <tr><td><br></td></tr>


<!-- MultiRobotMap -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/multirobotmap.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
	<b><a href="">Multi-Robot Active Mapping via Neural Bipartite Graph Matching</a></b><br><br>Kai Ye*, Siyan Dong*, Qingnan Fan, <b>He Wang</b>, Li Yi, Fei Xia, Jue Wang, Baoquan Chen <br><br> CVPR 2022 <br><br>
<a href="https://arxiv.org/pdf/2203.16319.pdf">arXiv</a>/<a href="javascript:hideshow(document.getElementById('MultiRobotMap'))">bibtex</a>
</p><pre><p id="MultiRobotMap" style="font:18px; display: none">
@article{ye2022multi,
  title={Multi-Robot Active Mapping via Neural Bipartite Graph Matching},
  author={Ye, Kai and Dong, Siyan and Fan, Qingnan and Wang, He and Yi, Li and Xia, Fei and Wang, Jue and Chen, Baoquan},
  journal={arXiv preprint arXiv:2203.16319},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>

	
<!-- CodedVTR -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/codedvtr.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
	<b><a href="">CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance</a></b><br><br>Tianchen Zhao, Niansong Zhang, Xuefei Ning, <b>He Wang</b>, Li Yi, Yu Wang <br><br> CVPR 2022 <br><br>
<a href="https://arxiv.org/pdf/2203.09887.pdf">arXiv</a>/<a href="javascript:hideshow(document.getElementById('CodedVTR'))">bibtex</a>
</p><pre><p id="CodedVTR" style="font:18px; display: none">
@article{zhao2022codedvtr,
  title={CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance},
  author={Zhao, Tianchen and Zhang, Niansong and Ning, Xuefei and Wang, He and Yi, Li and Wang, Yu},
  journal={arXiv preprint arXiv:2203.09887},
  year={2022}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>

<!-- DAGAI -->
<td width="25%" valign="top"><p><img src="./He_Wang_files/dagai.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
	 <b><a href="">Domain Adaptation on Point Clouds via Geometry-Aware Implicits</a></b><br><br>Yuefan Shen*, Yanchao Yang*, Mi Yan, <b>He Wang</b>, Youyi Zheng, Leonidas J. Guibas<br><br> CVPR 2022 <br><br>
<a href="https://arxiv.org/abs/2112.09343">arXiv</a>/<a href="javascript:hideshow(document.getElementById('DAGAI'))">bibtex</a>
</p><pre><p id="DAGAI" style="font:18px; display: none">
@article{shen2021domain,
  title={Domain Adaptation on Point Clouds via Geometry-Aware Implicits},
  author={Shen, Yuefan and Yang, Yanchao and Yan, Mi and Wang, He and Zheng, Youyi and Guibas, Leonidas},
  journal={arXiv preprint arXiv:2112.09343},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
	
	
<!-- ESSCOP -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/esscop.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="https://arxiv.org/pdf/2111.00190.pdf">Leveraging SE(3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds</a></b><br><br>Xiaolong Li, Yijia Weng, Li Yi, Leonidas J. Guibas, A. Lynn Abbott, Shuran Song, <b>He Wang<span>&#8224;</span></b> <br><br> NeurIPS 21 <br><br>
<a href="https://openreview.net/forum?id=wGRNAqVBQT2">Paper</a>/<a href="https://arxiv.org/pdf/2111.00190.pdf">arXiv</a>/<a href="https://dragonlong.github.io/equi-pose/">Project</a>/<a href="https://github.com/dragonlong/equi-pose">Code</a>/<a href="javascript:hideshow(document.getElementById('esscop'))">bibtex</a>
</p><pre><p id="esscop" style="font:18px; display: none">
@article{li2021leveraging,
  title={Leveraging SE (3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds},
  author={Li, Xiaolong and Weng, Yijia and Yi, Li and Guibas, Leonidas J and Abbott, A and Song, Shuran and Wang, He},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	
	
<!-- CAPTRA -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/captra.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds</a></b><br><br>Yijia Weng*, <b>He Wang*<span>&#8224;</span></b>, Qiang Zhou, Yuzhe Qin, Yueqi Duan, Qingnan Fan, Baoquan Chen, Hao Su, Leonidas J. Guibas<br><br> ICCV 2021 (<font color="red">Oral Presentation</font>) <br><br>
<a href="https://arxiv.org/abs/2104.03437">arXiv</a>/<a href="https://yijiaweng.github.io/CAPTRA/">Project</a>/<a href="https://github.com/halfsummer11/CAPTRA">Code</a>/<a href="https://youtu.be/JFPcOHCH2O0">Video</a>/<a href="javascript:hideshow(document.getElementById('captra'))">bibtex</a>
</p><pre><p id="captra" style="font:18px; display: none">
@article{weng2021captra,
  title={CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds},
  author={Weng, Yijia and Wang, He and Zhou, Qiang and Qin, Yuzhe and Duan, Yueqi and Fan, Qingnan and 
          Chen, Baoquan and Su, Hao and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:2104.03437},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	

<!-- Retrieval -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/retrieval.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Single Image 3D Shape Retrieval via Cross-Modal Instance and Category Contrastive Learning</a></b><br><br> Mingxian Lin, Jie Yang, <b>He Wang</b>, Yu-Kun Lai, Rongfei Jia, Binqiang Zhao, Lin Gao <br><br> ICCV 2021 <br><br>
<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Single_Image_3D_Shape_Retrieval_via_Cross-Modal_Instance_and_Category_ICCV_2021_paper.pdf">Paper</a>/<a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Lin_Single_Image_3D_ICCV_2021_supplemental.pdf">Supp.</a>/<a href="javascript:hideshow(document.getElementById('retrievel'))">bibtex</a>
</p><pre><p id="retrievel" style="font:18px; display: none">
@inproceedings{lin2021single,
  title={Single Image 3D Shape Retrieval via Cross-Modal Instance and Category Contrastive Learning},
  author={Lin, Ming-Xian and Yang, Jie and Wang, He and Lai, Yu-Kun and Jia, Rongfei and Zhao, Binqiang and Gao, Lin},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11405--11415},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
	
</tr>
<tr><td><br></td></tr>
	
	


<!-- 3DIoUMatch -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/3dioumatch.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">3DIoUMatch: Leveraging IoU Prediction for Semi-Supervised 3D Object Detection</a></b><br><br> <b>He Wang*</b>, Yezhen Cong*, Or Litany, Yue Gao and Leonidas J. Guibas<br><br>CVPR 2021<br><br>
<a href="https://arxiv.org/pdf/2012.04355.pdf">Paper</a>/<a href="https://thu17cyz.github.io/3DIoUMatch/">Project</a>/<a href="https://github.com/THU17cyz/3DIoUMatch">Code</a>/<a href="https://youtu.be/nuARjhkQN2U">Video</a>/<a href="javascript:hideshow(document.getElementById('3dioumatch'))">bibtex</a>
</p><pre><p id="3dioumatch" style="font:18px; display: none">
@inproceedings{wang20213dioumatch,
  title={3DIoUMatch: Leveraging iou prediction for semi-supervised 3d object detection},
  author={Wang, He and Cong, Yezhen and Litany, Or and Gao, Yue and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14615--14624},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- MultiBodySync -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/multibodysync.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan Synchronization</a></b><br><br> Jiahui Huang, <b>He Wang</b>, Tolga Birdal, Minkyuk Sung, Federica Arrigoni, Shi-Min Hu, and Leonidas J. Guibas<br><br>CVPR 2021 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/pdf/2101.06605.pdf">Paper</a>/<a href="https://github.com/huangjh-pub/multibody-sync">Code</a>/<a href="https://www.youtube.com/watch?v=BuIBXL2UNvI">Video</a>/<a href="javascript:hideshow(document.getElementById('multibodysync'))">bibtex</a>
</p><pre><p id="multibodysync" style="font:18px; display: none">
@inproceedings{huang2021multibodysync,
  title={Multibodysync: Multi-body segmentation and motion estimation via 3d scan synchronization},
  author={Huang, Jiahui and Wang, He and Birdal, Tolga and Sung, Minhyuk and Arrigoni, Federica and Hu, Shi-Min and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7108--7118},
  year={2021}
}
</p><p></p></pre>
<p></p></td>
</tr>

<!-- NeuralRouting -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/neuralrouting.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments</a></b><br><br>Siyan Dong*, Qingnan Fan*, <b>He Wang</b>,  Ji Shi, Li Yi, Thomas Funkhouser, Baoquan Chen, Leonidas J. Guibas<br><br>CVPR 2021 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/2010.05272">Paper</a>/<a href="https://github.com/siyandong/NeuralRouting">Code</a>/<a href="javascript:hideshow(document.getElementById('neuralrouting'))">bibtex</a>
</p><pre><p id="neuralrouting" style="font:18px; display: none">
@InProceedings{Dong_2021_CVPR,
    author    = {Dong, Siyan and Fan, Qingnan and Wang, He and Shi, Ji and Yi, Li and Funkhouser, Thomas and Chen, Baoquan and Guibas, Leonidas J.},
    title     = {Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {8544-8554}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>

	
<!-- Rethink -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/rethink.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks</a></b><br><br><b>He Wang*</b>, Zetian Jiang*, Li Yi, Kaichun Mo, Hao Su, Leonidas J. Guibas<br><br>CVPR 2021 Workshop on <i>Learning to Generate 3D Shapes and Scenes</i> <br><br>
<a href="https://arxiv.org/abs/2006.07029">Paper</a>/<a href="https://drive.google.com/file/d/1RF77Zp6cQgoU0tf33Wjthx0D6tr3IYAJ/view?usp=sharing">Poster</a>/<a href="https://www.youtube.com/watch?v=Ejzj0hnKW4Y">Video</a>/<a href="javascript:hideshow(document.getElementById('rethink'))">bibtex</a>
</p><pre><p id="rethink" style="font:18px; display: none">
@article{wang2020rethinking,
  title={Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks},
  author={Wang, He and Jiang, Zetian and Yi, Li and Mo, Kaichun and Su, Hao and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:2006.07029},
  year={2020}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>

<!-- PT2PC -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/pt2pc.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions</a></b><br><br>Kaichun Mo, <b>He Wang</b>, Li Yi, Xinchen Yan and Leonidas J. Guibas<br><br>ECCV 2020<br><br>
<a href="https://arxiv.org/abs/2003.08624">Paper</a>/<a href="https://cs.stanford.edu/~kaichun/pt2pc/">Project</a>/<a href="https://github.com/daerduoCarey/pt2pc">Code&Data</a>/<a href="javascript:hideshow(document.getElementById('pt2pc'))">bibtex</a>
</p><pre><p id="pt2pc" style="font:18px; display: none">
@article{mo2020pt2pc,
    title={{PT2PC}: Learning to Generate 3D Point Cloud Shapes from Part Tree Conditions},
    author={Mo, Kaichun and Wang, He and Yan, Xinchen and Guibas, Leonidas},
    journal={arXiv preprint arXiv:2003.08624},
    year={2020}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- Curriculm -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/csdf.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Curriculum DeepSDF</a></b><br><br>Yueqi Duan*, Haidong Zhu*, <b>He Wang</b>, Li Yi, Ram Nevatia, Leonidas J. Guibas<br><br>ECCV 2020<br><br>
<a href="https://arxiv.org/abs/2003.08593">Paper</a>/<a href="https://github.com/haidongz-usc/Curriculum-DeepSDF">Code</a>/<a href="javascript:hideshow(document.getElementById('csdf'))">bibtex</a>
</p><pre><p id="csdf" style="font:18px; display: none">
@misc{duan2020curriculum,
    title={Curriculum DeepSDF},
    author={Yueqi Duan and Haidong Zhu and He Wang and Li Yi and Ram Nevatia and Leonidas J. Guibas},
    year={2020},
    eprint={2003.08593},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- ANCSH -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/ancsh.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Category-level Articulated Object Pose Estimation</a></b><br><br><b>He Wang*</b>, Xiaolong Li*, Li Yi, Leonidas Guibas, A. Lynn Abbott, Shuran Song<br><br>CVPR 2020 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/1912.11913">Paper</a>/<a href="https://articulated-pose.github.io/">Project</a>/<a href="https://github.com/dragonlong/articulated-pose">Code&Data</a>/<a href="javascript:hideshow(document.getElementById('ancsh'))">bibtex</a>
</p><pre><p id="ancsh" style="font:18px; display: none">
@article{li2019category,
  title={Category-Level Articulated Object Pose Estimation},
  author={Li, Xiaolong and Wang, He and Yi, Li and Guibas, Leonidas and Abbott, A Lynn and Song, Shuran},
  journal={arXiv preprint arXiv:1912.11913},
  year={2019}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SAPIEN -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/sapien.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">SAPIEN: A SimulAted Part-based Interactive ENvironment</a></b><br><br>Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan,<br><b>He Wang</b>, Li Yi, Angel X.Chang, Leonidas J. Guibas and Hao Su<br><br>CVPR 2020 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/2003.08515">Paper</a>/<a href="https://sapien.ucsd.edu/">Project</a>/<a href="https://github.com/haosulab/SAPIEN-Release">Code&Data</a>/<a href="https://youtu.be/K2yOeJhJXzM">Demo</a>/<a href="javascript:hideshow(document.getElementById('sapien'))">bibtex</a>
</p><pre><p id="sapien" style="font:18px; display: none">
@InProceedings{Xiang_2020_SAPIEN,
    author = {Xiang, Fanbo and Qin, Yuzhe and Mo, Kaichun and Xia, Yikuan and Zhu, Hao 
                  and Liu, Fangchen and Liu, Minghua and Jiang, Hanxiao and Yuan, Yifu 
                  and Wang, He and Yi, Li and Chang, Angel X. and Guibas, Leonidas J. and Su, Hao},
    title = {{SAPIEN}: A SimulAted Part-based Interactive ENvironment},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- Pose RCNN -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/posercnn.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation</a></b><br><br><b>He Wang</b>, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, Leonidas J. Guibas<br><br>CVPR 2019 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/1901.02970">Paper</a>/<a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/">Project</a>/<a href="https://github.com/hughw19/NOCS_CVPR2019">Code&Data</a>/<a href="javascript:hideshow(document.getElementById('posercnn'))">bibtex</a>
</p><pre><p id="posercnn" style="font:18px; display: none">
@article{wang2019normalized,
  title={Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation},
  author={Wang, He and Sridhar, Srinath and Huang, Jingwei and Valentin, Julien and Song, Shuran 
	  and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:1901.02970},
  year={2019}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>
    
<!-- GSPN -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/gspn.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud</a></b><br><br>Li Yi, Wang Zhao, <b>He Wang</b>, Minhyuk Sung, Leonidas Guibas<br><br>CVPR 2019<br><br>
<a href="https://arxiv.org/abs/1812.03320">Paper</a>/<a href="https://github.com/ericyi/GSPN">Code</a>/<a href="javascript:hideshow(document.getElementById('gspn'))">bibtex</a>
</p><pre><p id="gspn" style="font:18px; display: none">
@article{yi2018gspn,
  title={GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud},
  author={Yi, Li and Zhao, Wang and Wang, He and Sung, Minhyuk and Guibas, Leonidas},
  journal={arXiv preprint arXiv:1812.03320},
  year={2018}
}
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Genertive Model for Interaction -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/interaction.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">Learning a Generative Model for Multi-Step Human-Object Interactions from Videos</a></b><br><br><b>He Wang*</b>, Soeren Pirk*, Ersin Yumer, Vladimir Kim, Ozan Sener, Srinath Sridhar, Leonidas J. Guibas<br><br>Eurographics 2019 (<font color="red">Best Paper Honorable Mention</font>)<br><br>
<a href="http://www.pirk.info/projects/learning_interactions/index.html">Project</a>/<a href="./He_Wang_files/19_EG_FnInteract.pdf">Paper</a>/<a href="https://github.com/hughw19/ActionPlotGeneration.git">Code</a>/<a href="http://ai.stanford.edu/blog/generate-human-object/">Blog</a>/<a href="https://www.youtube.com/watch?v=WgpPalA2RzA">Video</a>
</p><pre><p id="interaction" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

</tbody></table>

</ul>
</div>
<br>

<h3>Preprints</h3>
<ul>


<table width="100%">

	
	
<!-- IFDefense -->
<tbody><tr>
<td width="25%" valign="top"><p><img src="./He_Wang_files/ifdefense.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="75%" valign="top"><p>
    <b><a href="">IF-Defense: 3D Adversarial Point Cloud Defense via Implicit Function based Restoration</a></b><br><br>Ziyi Wu*, Yueqi Duan*, <b>He Wang</b>, Qingnan Fan, Leonidas J. Guibas<br><br>arXiv:2010.05272 (10/11/2020)<br><br>
<a href="https://arxiv.org/abs/2010.05272">arXiv</a>/<a href="javascript:hideshow(document.getElementById('ifdefense'))">bibtex</a>
</p><pre><p id="ifdefense" style="font:18px; display: none">
@misc{wu2020ifdefense,
      title={IF-Defense: 3D Adversarial Point Cloud Defense via Implicit Function based Restoration}, 
      author={Ziyi Wu and Yueqi Duan and He Wang and Qingnan Fan and Leonidas J. Guibas},
      year={2020},
      eprint={2010.05272},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
</p><p></p></pre>
<p></p></td>
</tr>
<tr><td><br></td></tr>
	





</tbody></table>


</ul>
</div>
<br>



<a name="education"></a>
<div class="section">
<h3>Education</h3>
<ul>
<li>2015.9 - 2021.6: Ph.D. in Electrical Engineering, Stanford University</li>
<li>2014.9 - 2017.6: M.S. in Electrical Engineering, Stanford University</li>
<li>2010.9 - 2014.7: B.Eng. in Microelectronics and Nanoelectronics, Tsinghua University</li>
</ul>
</div>
<br>

<!-- <a name="experiences"></a>
<div class="section">
<h3>Experiences</h3>
<ul>
<li>2020.7 - 2020.8: Visiting student at Prof. Hao Su's lab at UCSD. </li>
<li>2019.6 - 2019.9: Research intern at the robotics team of Facebook AI Research (FAIR).</li>
<li>2019.2 - 2019.5: Student researcher at Google Daydream team.</li>
<li>2018.6 - 2018.9: Software engineer intern at Google Daydream team.</li>
</ul>
</div>
<br> -->

<a name="service"></a>
<div class="section">
<h3>Professional service</h3>
<ul>
    <li> Area chair (AC):
        <ul>
            <li> Conferences: CVPR, WACV</li>
	    <li> Seminars: <a href="http://valser.org">VALSE</a></li>
        </ul>
    
    <li> Program committee/reviewer:
    <ul>
        <li> Conferences: ICCV, ECCV, ICLR, AAAI, SIGGRAPH, 3DV</li>
        <li> Journals: IEEE TPAMI, IEEE CG&A, IEEE TVCG</li>
    </ul>
</div>



<hr>
<div id="footer" style="font-size:10">He Wang <i>Last updated: April 20, 2022 </i></div>

</body></html>
